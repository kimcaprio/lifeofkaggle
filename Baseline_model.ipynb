{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMqBSZ2WrypfmEFwAlfif9p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimcaprio/lifeofkaggle/blob/master/Baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wHJ6sFD2mls",
        "colab_type": "text"
      },
      "source": [
        "# Baseline 모델\n",
        "## tabular 데이터를 다루는 캐클의 파이프라인\n",
        "### 1. 데이터 전처리 -> 2. 피처엔지니어링 -> 3. 머신러닝 모델학습 -> 4. 테스트 데이터 예측 및 캐글 업로드\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwVGkqiC3Gb5",
        "colab_type": "text"
      },
      "source": [
        "1.   데이터 전처리\n",
        "\n",
        "*   제품 변수의 결측값을 0으로 대체. 제품 보유 여부에 대한 정보가 없으면, 해당 제품을 보유하지 않고 있지 않다고 가정.\n",
        "*   훈련 데이터와 테스트 데이터를 통합. 훈련 데이터와 테스트 데이터는 날짜변수(fecha_dato)로 쉽게 구분 가능. 동일한 24개의 고객 변수를 공유하고 있으며, 테스트 데이터에 없는 24개의 제품 변수는 0으로 채움.\n",
        "* 번주형, 수치형 데이터를 전처리. 변수형 데이터는 .factorize()통해 label encoding을 수행. 데이터 타입이 object로 표현되는 수치형 데이터에서는 .unique()를 통해 특이값들을 대체하거나 제거하고, 정수형 데이터로 변환.\n",
        "*   추후 모델 학습에 사용할 변수 이름을 features 리스트에 미리 담는다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuO1MEyA69ok",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84c95c83-fd24-4541-d034-244467147bd7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lVUCwgC7TtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/My Drive/CoLab/explorer_of_machine_learning/for_kaggle/.kaggle/\" # put path for wherever you put it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrhgpxqK7U5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkCA9mkb2l_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "2a1c98cf-94bd-41bf-97bb-6848761c8972"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "\n",
        "np.random.seed(2018)\n",
        "\n",
        "# 데이터를 불러온다\n",
        "trn = pd.read_csv('/content/drive/My Drive/CoLab/explorer_of_machine_learning/ch02_santander/data/train_ver2.csv')\n",
        "tst = pd.read_csv('/content/drive/My Drive/CoLab/explorer_of_machine_learning/ch02_santander/data/test_ver2.csv')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76du4a0Aqohx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4ea4467a-c2d1-495d-8dca-1e4ee8141a62"
      },
      "source": [
        "## 데이터 전처리\n",
        "# 제품 변수를 별도로 저장\n",
        "prods = trn.columns[24:].tolist()\n",
        "\n",
        "# 제품 변수 결측값을 미리 0으로 대체\n",
        "trn[prods] = trn[prods].fillna(0.0).astype(np.int8)\n",
        "\n",
        "# 24개 제품 중 하나도 보유하지 않는 고객 데이터를 제거\n",
        "no_product = trn[prods].sum(axis=1) == 0\n",
        "trn = trn[~no_product]\n",
        "\n",
        "# 훈련 데이터와 테스트 데이터를 통합. 테스트 데이터에 없는 제품 변수는 0으로 대체\n",
        "for col in trn.columns[24:]:\n",
        "  tst[col] = 0\n",
        "df = pd.concat([trn, tst], axis=0)\n",
        "\n",
        "# 학습에 사용할 변수를 담는 list\n",
        "features = []\n",
        "\n",
        "# 범주형 변수를 .factorize() 함수를 통해 label encoding 함(one-hot) -- 날짜 데이터는 제외 시켜야...\n",
        "# 그래서 모든 object type 변수를 fatorize 해서는 안됨... categorical_cols = [col for col in trn.columns[:24] if trn[col].dtype in ['O']]\n",
        "categorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp','canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\n",
        "print(categorical_cols)\n",
        "for col in categorical_cols:\n",
        "  df[col], _ = df[col].factorize(na_sentinel=-99)\n",
        "features += categorical_cols\n",
        "\n",
        "# 수치형 변수의 특이값과 결측값을 -99로 대체하고, 정수형으로 변환\n",
        "df['age'].replace(' NA', -99, inplace=True)\n",
        "df['age'] = df['age'].astype(np.int8)\n",
        "\n",
        "df['antiguedad'].replace('     NA', -99, inplace=True)\n",
        "df['antiguedad'] = df['antiguedad'].astype(np.int8)\n",
        "\n",
        "df['renta'].replace('         NA', -99, inplace=True)\n",
        "df['renta'].fillna(-99, inplace=True)\n",
        "df['renta'] = df['renta'].astype(float).astype(np.int8)\n",
        "\n",
        "df['indrel_1mes'].replace('P', 5, inplace=True)\n",
        "df['indrel_1mes'].fillna(-99, inplace=True)\n",
        "df['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)\n",
        "\n",
        "#학습에 사용할 수치형 변수를 features에 추가\n",
        "#features +=  [col for col in trn.columns[:24] if trn[col].dtype in ['int64', 'float64']]\n",
        "features += ['age', 'antiguedad', 'renta', 'ind_nuevo', 'indrel', 'indrel_1mes', 'ind_actividad_cliente']\n",
        "print(features)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\n",
            "['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento', 'age', 'antiguedad', 'renta', 'ind_nuevo', 'indrel', 'indrel_1mes', 'ind_actividad_cliente']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MeYTErP888",
        "colab_type": "text"
      },
      "source": [
        "2.   피처 엔지니어링\n",
        "*   모델 학습에 사용할 파생변수를 생성."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruovoQQ1eU5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c33cc652-9622-4def-e0b5-61a534cfb205"
      },
      "source": [
        "df['fecha_alta'].head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2015-01-12\n",
              "1    2012-08-10\n",
              "2    2012-08-10\n",
              "3    2012-08-10\n",
              "4    2012-08-10\n",
              "Name: fecha_alta, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjZdfeboP8UX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (피처 엔지니어링) 두 날짜 변수에서 연도와 월 정보를 추출\n",
        "df['fecha_alta_month'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
        "df['fecha_alta_year'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
        "features += ['fecha_alta_month', 'fecha_alta_year']\n",
        "\n",
        "df['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
        "df['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
        "features += ['ult_fec_cli_1t_month', 'ult_cli_1t_year']\n",
        "\n",
        "# 그 외 변수의 결측값은 모두 -99로 변환\n",
        "df.fillna(-99, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QR5zFM_XjM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (피처 엔지니어링) lag-1 데이터를 생성\n",
        "# # 날짜를 숫자로 변환하는 함수. 2015-01-28 은 1, 2016-06-28은 18로 변환\n",
        "def date_to_int(str_date):\n",
        "  Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]\n",
        "  int_date = (int(Y) - 2015) * 12 + int(M)\n",
        "  return int_date\n",
        "\n",
        "# 날짜를 숫자로 변환하여 int_date에 저장\n",
        "df['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)\n",
        "\n",
        "# 데이터를 복사하고, int_date 날짜에 1을 더하여 lag를 생성. 변수명에 _prev를 추가\n",
        "df_lag = df.copy()\n",
        "df_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns]\n",
        "df_lag['int_date'] += 1\n",
        "\n",
        "# 원본 데이터와 lag 데이터를 ncodper와 int_date 기준으로 합침. lag 데이터의 int_date는 1이 밀려 있기 때문에, 저번 달의 제품 정보가 삽입.\n",
        "df_trn = df.merge(df_lag, on=['ncodpers', 'int_date'], how='left')\n",
        "\n",
        "# 메모리 효율을 위해 불필요한 변수르 메모리에서 제거.\n",
        "#del df, df_lag\n",
        "\n",
        "# 저번달의 제품 정보가 존재하지 않을 경우를 대비하여 0으로 대체\n",
        "for prod in prods:\n",
        "  prev = prod + '_prev'\n",
        "  df_trn[prev].fillna(0, inplace=True)\n",
        "df_trn.fillna(-99, inplace=True)\n",
        "\n",
        "# lag-1 변수를 추가\n",
        "features += [feature + '_prev' for feature in features]\n",
        "features += [prod + '_prev' for prod in prods]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttt1nxKUZXlT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "3.   교차검증\n",
        "\n",
        "\n",
        "*   테스트 데이터 제외하고 주어진 train 데이터를 활용하여 교차검증 데이터로 활용\n",
        "*   시계열 데이터의 경우, 테스트 데이터 중 마지막 날짜 또는 년, 월 을 교차검증 데이터로 분리하여 활용하는 것이 일반적. <- 미래를 예측하는 모델이기 때문\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQKAez1jaApF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "1164506e-9a1f-4905-b9a7-b2ea69704d9e"
      },
      "source": [
        "## 모델 학습\n",
        "# 학습을 위하여 데이터를 훈련, 테스트용으로 분리\n",
        "# 학습에는 2016-01-28 ~ 2016-04-28 epdlxjaks tkdyd, 검증에는 2016-05-28 epdlxjfmf tkdyd.\n",
        "use_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
        "trn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\n",
        "tst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\n",
        "#del df_trn\n",
        "\n",
        "print(\"tst\", tst)\n",
        "\n",
        "# 훈련 데이터에서 신규 구매 건수만 추출\n",
        "X = []\n",
        "Y = []\n",
        "for i, prod in enumerate(prods):\n",
        "  prev = prod + '_prev'\n",
        "  prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n",
        "  prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
        "  X.append(prX)\n",
        "  Y.append(prY)\n",
        "XY = pd.concat(X)\n",
        "Y = np.hstack(Y)\n",
        "XY['y'] = Y\n",
        "\n",
        "#훈련, 검증 데이터로 분리\n",
        "vld_date = '2016-05-28'\n",
        "XY_trn = XY[XY['fecha_dato'] != vld_date]\n",
        "XY_vld = XY[XY['fecha_dato'] == vld_date]\n",
        "\n",
        "print(\"XY_trn\", XY_trn)\n",
        "print(\"XY_vld\", XY_vld)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tst           fecha_dato  ...  ult_fec_cli_1t_year_prev\n",
            "11091070  2016-06-28  ...                       0.0\n",
            "11091071  2016-06-28  ...                       0.0\n",
            "11091072  2016-06-28  ...                       0.0\n",
            "11091073  2016-06-28  ...                       0.0\n",
            "11091074  2016-06-28  ...                       0.0\n",
            "...              ...  ...                       ...\n",
            "12020680  2016-06-28  ...                       0.0\n",
            "12020681  2016-06-28  ...                     -99.0\n",
            "12020682  2016-06-28  ...                       0.0\n",
            "12020683  2016-06-28  ...                       0.0\n",
            "12020684  2016-06-28  ...                       0.0\n",
            "\n",
            "[929615 rows x 104 columns]\n",
            "XY_trn           fecha_dato  ncodpers  ...  ult_fec_cli_1t_year_prev   y\n",
            "7658069   2016-01-28   1474324  ...                       0.0   1\n",
            "7628180   2016-01-28   1432311  ...                     -99.0   2\n",
            "7628198   2016-01-28   1432232  ...                     -99.0   2\n",
            "7628482   2016-01-28   1432080  ...                     -99.0   2\n",
            "7628692   2016-01-28   1432952  ...                       0.0   2\n",
            "...              ...       ...  ...                       ...  ..\n",
            "10394466  2016-04-28   1297367  ...                       0.0  23\n",
            "10394481  2016-04-28   1297315  ...                       0.0  23\n",
            "10394487  2016-04-28   1297332  ...                       0.0  23\n",
            "10394502  2016-04-28   1297428  ...                       0.0  23\n",
            "10394529  2016-04-28   1297412  ...                       0.0  23\n",
            "\n",
            "[161137 rows x 105 columns]\n",
            "XY_vld           fecha_dato  ncodpers  ...  ult_fec_cli_1t_year_prev   y\n",
            "10597872  2016-05-28    194160  ...                       0.0   0\n",
            "10394747  2016-05-28    658081  ...                       0.0   2\n",
            "10395030  2016-05-28    658132  ...                     -99.0   2\n",
            "10395104  2016-05-28    658521  ...                     -99.0   2\n",
            "10395155  2016-05-28    655909  ...                     -99.0   2\n",
            "...              ...       ...  ...                       ...  ..\n",
            "11090455  2016-05-28   1166385  ...                       0.0  23\n",
            "11090468  2016-05-28   1166355  ...                       0.0  23\n",
            "11090667  2016-05-28   1166343  ...                       0.0  23\n",
            "11090696  2016-05-28   1166232  ...                       0.0  23\n",
            "11090782  2016-05-28   1166874  ...                       0.0  23\n",
            "\n",
            "[37897 rows x 105 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUToWvAJwBni",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   xgboost 모델을 사용 하여 학습 모델 만들기\n",
        "2.   주요 파라미터\n",
        "*   max_depth : 트리 모델의 최대 깊이를 의미. 값이 높을 수록 더 복잡한 트리를 만들지만, 과적합의 원인이 될수 있음.\n",
        "*   eta : 딥러닝에서의 learning rate 의미. 0과 1사이의 값을 가지며, 값이 너무 높으면 학습이 잘 안되고, 낮으면 학습에 시간이 오래 걸림.\n",
        "*   colsample_bytree : 트리를 생성할 때 룬련 데이터에서 변수를 샘플링해주는 비율. 마찬가지로 과적합을 막아주는 방안. (딥러닝의 drop out 개념?) 보통 0.6 ~ 0.9\n",
        "*   colsample_bylevel : 트리의 레벨 별로 훈련 데이터의 변수를 샘플링해주는 비율. 보통 0.6 ~ 0.9\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XWYHmwEwBBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f64f4a2-c1bd-424e-b7df-b77f0bf39064"
      },
      "source": [
        "## XGBoost 모델 parameter를 설정\n",
        "param = {\n",
        "  'booster': 'gbtree',\n",
        "  'max_depth' : 8,\n",
        "  'nthread' : 4,\n",
        "  'num_class' : len(prods),\n",
        "  'objective' : 'multi:softprob',\n",
        "  'silent' : 1,\n",
        "  'eval_metric' : 'mlogloss',\n",
        "  'eta' : 0.1,\n",
        "  'min_child_weight' : 10,\n",
        "  'colsample_bytree' : 0.8,\n",
        "  'colsample_bylevel' : 0.9,\n",
        "  'seed' : 2018,\n",
        "}\n",
        "\n",
        "# 훈련, 검증 데이터를 XGBoost 형태로 변환\n",
        "X_trn = XY_trn.as_matrix(columns=features)\n",
        "Y_trn = XY_trn.as_matrix(columns=['y'])\n",
        "dtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n",
        "\n",
        "X_vld = XY_vld.as_matrix(columns=features)\n",
        "Y_vld = XY_vld.as_matrix(columns=['y'])\n",
        "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
        "\n",
        "print(Y_vld)\n",
        "\n",
        "# XBBoost 모델을 훈련 데이터로 학습\n",
        "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
        "model = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)\n",
        "\n",
        "import pickle\n",
        "pickle.dump(model, open('/content/drive/My Drive/CoLab/explorer_of_machine_learning/ch02_santander/modle/xgb.baseline.pkl', 'wb'))\n",
        "best_ntree_limit = model.best_ntree_limit"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 0]\n",
            " [ 2]\n",
            " [ 2]\n",
            " ...\n",
            " [23]\n",
            " [23]\n",
            " [23]]\n",
            "[0]\ttrain-mlogloss:2.6752\teval-mlogloss:2.68357\n",
            "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until eval-mlogloss hasn't improved in 20 rounds.\n",
            "[1]\ttrain-mlogloss:2.43959\teval-mlogloss:2.4521\n",
            "[2]\ttrain-mlogloss:2.26071\teval-mlogloss:2.27549\n",
            "[3]\ttrain-mlogloss:2.12697\teval-mlogloss:2.14326\n",
            "[4]\ttrain-mlogloss:2.01445\teval-mlogloss:2.03139\n",
            "[5]\ttrain-mlogloss:1.92002\teval-mlogloss:1.93765\n",
            "[6]\ttrain-mlogloss:1.84373\teval-mlogloss:1.86197\n",
            "[7]\ttrain-mlogloss:1.77481\teval-mlogloss:1.79374\n",
            "[8]\ttrain-mlogloss:1.71598\teval-mlogloss:1.73584\n",
            "[9]\ttrain-mlogloss:1.66264\teval-mlogloss:1.68283\n",
            "[10]\ttrain-mlogloss:1.61532\teval-mlogloss:1.63602\n",
            "[11]\ttrain-mlogloss:1.57299\teval-mlogloss:1.59415\n",
            "[12]\ttrain-mlogloss:1.53602\teval-mlogloss:1.55785\n",
            "[13]\ttrain-mlogloss:1.50103\teval-mlogloss:1.52306\n",
            "[14]\ttrain-mlogloss:1.46888\teval-mlogloss:1.49102\n",
            "[15]\ttrain-mlogloss:1.44007\teval-mlogloss:1.46248\n",
            "[16]\ttrain-mlogloss:1.41394\teval-mlogloss:1.43632\n",
            "[17]\ttrain-mlogloss:1.39043\teval-mlogloss:1.41319\n",
            "[18]\ttrain-mlogloss:1.36815\teval-mlogloss:1.39104\n",
            "[19]\ttrain-mlogloss:1.34773\teval-mlogloss:1.37044\n",
            "[20]\ttrain-mlogloss:1.32872\teval-mlogloss:1.35157\n",
            "[21]\ttrain-mlogloss:1.31158\teval-mlogloss:1.33487\n",
            "[22]\ttrain-mlogloss:1.29593\teval-mlogloss:1.31953\n",
            "[23]\ttrain-mlogloss:1.28171\teval-mlogloss:1.30565\n",
            "[24]\ttrain-mlogloss:1.26847\teval-mlogloss:1.29285\n",
            "[25]\ttrain-mlogloss:1.25602\teval-mlogloss:1.28095\n",
            "[26]\ttrain-mlogloss:1.24436\teval-mlogloss:1.26952\n",
            "[27]\ttrain-mlogloss:1.2333\teval-mlogloss:1.25857\n",
            "[28]\ttrain-mlogloss:1.223\teval-mlogloss:1.24837\n",
            "[29]\ttrain-mlogloss:1.21386\teval-mlogloss:1.23967\n",
            "[30]\ttrain-mlogloss:1.205\teval-mlogloss:1.23131\n",
            "[31]\ttrain-mlogloss:1.19658\teval-mlogloss:1.22304\n",
            "[32]\ttrain-mlogloss:1.18865\teval-mlogloss:1.21531\n",
            "[33]\ttrain-mlogloss:1.18135\teval-mlogloss:1.20845\n",
            "[34]\ttrain-mlogloss:1.17431\teval-mlogloss:1.20194\n",
            "[35]\ttrain-mlogloss:1.16782\teval-mlogloss:1.19579\n",
            "[36]\ttrain-mlogloss:1.16179\teval-mlogloss:1.19015\n",
            "[37]\ttrain-mlogloss:1.15591\teval-mlogloss:1.18468\n",
            "[38]\ttrain-mlogloss:1.15047\teval-mlogloss:1.17958\n",
            "[39]\ttrain-mlogloss:1.14531\teval-mlogloss:1.17482\n",
            "[40]\ttrain-mlogloss:1.14045\teval-mlogloss:1.17039\n",
            "[41]\ttrain-mlogloss:1.13581\teval-mlogloss:1.16609\n",
            "[42]\ttrain-mlogloss:1.13144\teval-mlogloss:1.1621\n",
            "[43]\ttrain-mlogloss:1.12728\teval-mlogloss:1.15831\n",
            "[44]\ttrain-mlogloss:1.12327\teval-mlogloss:1.15472\n",
            "[45]\ttrain-mlogloss:1.11965\teval-mlogloss:1.15134\n",
            "[46]\ttrain-mlogloss:1.11597\teval-mlogloss:1.14812\n",
            "[47]\ttrain-mlogloss:1.11283\teval-mlogloss:1.14555\n",
            "[48]\ttrain-mlogloss:1.10974\teval-mlogloss:1.14285\n",
            "[49]\ttrain-mlogloss:1.10683\teval-mlogloss:1.14032\n",
            "[50]\ttrain-mlogloss:1.10393\teval-mlogloss:1.13775\n",
            "[51]\ttrain-mlogloss:1.10106\teval-mlogloss:1.13536\n",
            "[52]\ttrain-mlogloss:1.0985\teval-mlogloss:1.13326\n",
            "[53]\ttrain-mlogloss:1.09593\teval-mlogloss:1.13107\n",
            "[54]\ttrain-mlogloss:1.09349\teval-mlogloss:1.12895\n",
            "[55]\ttrain-mlogloss:1.09107\teval-mlogloss:1.12708\n",
            "[56]\ttrain-mlogloss:1.08876\teval-mlogloss:1.12535\n",
            "[57]\ttrain-mlogloss:1.08668\teval-mlogloss:1.12363\n",
            "[58]\ttrain-mlogloss:1.08461\teval-mlogloss:1.12191\n",
            "[59]\ttrain-mlogloss:1.08263\teval-mlogloss:1.12035\n",
            "[60]\ttrain-mlogloss:1.08083\teval-mlogloss:1.11895\n",
            "[61]\ttrain-mlogloss:1.0789\teval-mlogloss:1.11757\n",
            "[62]\ttrain-mlogloss:1.07709\teval-mlogloss:1.11626\n",
            "[63]\ttrain-mlogloss:1.0754\teval-mlogloss:1.11497\n",
            "[64]\ttrain-mlogloss:1.07388\teval-mlogloss:1.11385\n",
            "[65]\ttrain-mlogloss:1.07231\teval-mlogloss:1.11269\n",
            "[66]\ttrain-mlogloss:1.07088\teval-mlogloss:1.11171\n",
            "[67]\ttrain-mlogloss:1.06947\teval-mlogloss:1.11073\n",
            "[68]\ttrain-mlogloss:1.06806\teval-mlogloss:1.10987\n",
            "[69]\ttrain-mlogloss:1.06677\teval-mlogloss:1.10891\n",
            "[70]\ttrain-mlogloss:1.06545\teval-mlogloss:1.10805\n",
            "[71]\ttrain-mlogloss:1.06423\teval-mlogloss:1.10719\n",
            "[72]\ttrain-mlogloss:1.06291\teval-mlogloss:1.10641\n",
            "[73]\ttrain-mlogloss:1.0617\teval-mlogloss:1.10565\n",
            "[74]\ttrain-mlogloss:1.06053\teval-mlogloss:1.10482\n",
            "[75]\ttrain-mlogloss:1.05931\teval-mlogloss:1.10413\n",
            "[76]\ttrain-mlogloss:1.05826\teval-mlogloss:1.1035\n",
            "[77]\ttrain-mlogloss:1.05711\teval-mlogloss:1.10282\n",
            "[78]\ttrain-mlogloss:1.05611\teval-mlogloss:1.10224\n",
            "[79]\ttrain-mlogloss:1.05512\teval-mlogloss:1.10164\n",
            "[80]\ttrain-mlogloss:1.05413\teval-mlogloss:1.10104\n",
            "[81]\ttrain-mlogloss:1.05325\teval-mlogloss:1.10051\n",
            "[82]\ttrain-mlogloss:1.05227\teval-mlogloss:1.09994\n",
            "[83]\ttrain-mlogloss:1.05135\teval-mlogloss:1.09945\n",
            "[84]\ttrain-mlogloss:1.05038\teval-mlogloss:1.09901\n",
            "[85]\ttrain-mlogloss:1.04941\teval-mlogloss:1.09856\n",
            "[86]\ttrain-mlogloss:1.04852\teval-mlogloss:1.09813\n",
            "[87]\ttrain-mlogloss:1.04774\teval-mlogloss:1.09772\n",
            "[88]\ttrain-mlogloss:1.04686\teval-mlogloss:1.09737\n",
            "[89]\ttrain-mlogloss:1.04613\teval-mlogloss:1.09701\n",
            "[90]\ttrain-mlogloss:1.04543\teval-mlogloss:1.09664\n",
            "[91]\ttrain-mlogloss:1.04468\teval-mlogloss:1.09635\n",
            "[92]\ttrain-mlogloss:1.04378\teval-mlogloss:1.096\n",
            "[93]\ttrain-mlogloss:1.04306\teval-mlogloss:1.09568\n",
            "[94]\ttrain-mlogloss:1.0423\teval-mlogloss:1.09537\n",
            "[95]\ttrain-mlogloss:1.04158\teval-mlogloss:1.09506\n",
            "[96]\ttrain-mlogloss:1.04091\teval-mlogloss:1.09482\n",
            "[97]\ttrain-mlogloss:1.04016\teval-mlogloss:1.09451\n",
            "[98]\ttrain-mlogloss:1.03946\teval-mlogloss:1.09426\n",
            "[99]\ttrain-mlogloss:1.03861\teval-mlogloss:1.09398\n",
            "[100]\ttrain-mlogloss:1.03791\teval-mlogloss:1.09376\n",
            "[101]\ttrain-mlogloss:1.03733\teval-mlogloss:1.09353\n",
            "[102]\ttrain-mlogloss:1.03664\teval-mlogloss:1.09331\n",
            "[103]\ttrain-mlogloss:1.03598\teval-mlogloss:1.09307\n",
            "[104]\ttrain-mlogloss:1.03519\teval-mlogloss:1.09288\n",
            "[105]\ttrain-mlogloss:1.0346\teval-mlogloss:1.09264\n",
            "[106]\ttrain-mlogloss:1.03385\teval-mlogloss:1.0925\n",
            "[107]\ttrain-mlogloss:1.0332\teval-mlogloss:1.09233\n",
            "[108]\ttrain-mlogloss:1.03258\teval-mlogloss:1.09218\n",
            "[109]\ttrain-mlogloss:1.03199\teval-mlogloss:1.09198\n",
            "[110]\ttrain-mlogloss:1.0315\teval-mlogloss:1.09183\n",
            "[111]\ttrain-mlogloss:1.03089\teval-mlogloss:1.09168\n",
            "[112]\ttrain-mlogloss:1.0303\teval-mlogloss:1.09154\n",
            "[113]\ttrain-mlogloss:1.0297\teval-mlogloss:1.0914\n",
            "[114]\ttrain-mlogloss:1.02905\teval-mlogloss:1.09122\n",
            "[115]\ttrain-mlogloss:1.02837\teval-mlogloss:1.09108\n",
            "[116]\ttrain-mlogloss:1.02776\teval-mlogloss:1.0909\n",
            "[117]\ttrain-mlogloss:1.02723\teval-mlogloss:1.0908\n",
            "[118]\ttrain-mlogloss:1.02665\teval-mlogloss:1.09063\n",
            "[119]\ttrain-mlogloss:1.02611\teval-mlogloss:1.09052\n",
            "[120]\ttrain-mlogloss:1.02558\teval-mlogloss:1.09045\n",
            "[121]\ttrain-mlogloss:1.02493\teval-mlogloss:1.09028\n",
            "[122]\ttrain-mlogloss:1.02412\teval-mlogloss:1.09021\n",
            "[123]\ttrain-mlogloss:1.02349\teval-mlogloss:1.09009\n",
            "[124]\ttrain-mlogloss:1.02282\teval-mlogloss:1.08999\n",
            "[125]\ttrain-mlogloss:1.02221\teval-mlogloss:1.08989\n",
            "[126]\ttrain-mlogloss:1.02165\teval-mlogloss:1.08984\n",
            "[127]\ttrain-mlogloss:1.02094\teval-mlogloss:1.08978\n",
            "[128]\ttrain-mlogloss:1.0205\teval-mlogloss:1.0897\n",
            "[129]\ttrain-mlogloss:1.01978\teval-mlogloss:1.08959\n",
            "[130]\ttrain-mlogloss:1.01913\teval-mlogloss:1.08951\n",
            "[131]\ttrain-mlogloss:1.01849\teval-mlogloss:1.08939\n",
            "[132]\ttrain-mlogloss:1.01797\teval-mlogloss:1.08937\n",
            "[133]\ttrain-mlogloss:1.01719\teval-mlogloss:1.08925\n",
            "[134]\ttrain-mlogloss:1.01655\teval-mlogloss:1.08918\n",
            "[135]\ttrain-mlogloss:1.01599\teval-mlogloss:1.08911\n",
            "[136]\ttrain-mlogloss:1.0153\teval-mlogloss:1.08905\n",
            "[137]\ttrain-mlogloss:1.01457\teval-mlogloss:1.08894\n",
            "[138]\ttrain-mlogloss:1.01402\teval-mlogloss:1.08883\n",
            "[139]\ttrain-mlogloss:1.01329\teval-mlogloss:1.08871\n",
            "[140]\ttrain-mlogloss:1.01261\teval-mlogloss:1.08867\n",
            "[141]\ttrain-mlogloss:1.01208\teval-mlogloss:1.08862\n",
            "[142]\ttrain-mlogloss:1.0115\teval-mlogloss:1.08857\n",
            "[143]\ttrain-mlogloss:1.01088\teval-mlogloss:1.08846\n",
            "[144]\ttrain-mlogloss:1.01029\teval-mlogloss:1.08843\n",
            "[145]\ttrain-mlogloss:1.00952\teval-mlogloss:1.08834\n",
            "[146]\ttrain-mlogloss:1.00893\teval-mlogloss:1.08828\n",
            "[147]\ttrain-mlogloss:1.00836\teval-mlogloss:1.08826\n",
            "[148]\ttrain-mlogloss:1.00794\teval-mlogloss:1.08821\n",
            "[149]\ttrain-mlogloss:1.00741\teval-mlogloss:1.08817\n",
            "[150]\ttrain-mlogloss:1.00661\teval-mlogloss:1.08806\n",
            "[151]\ttrain-mlogloss:1.00595\teval-mlogloss:1.08802\n",
            "[152]\ttrain-mlogloss:1.00535\teval-mlogloss:1.08803\n",
            "[153]\ttrain-mlogloss:1.00478\teval-mlogloss:1.08798\n",
            "[154]\ttrain-mlogloss:1.00421\teval-mlogloss:1.08797\n",
            "[155]\ttrain-mlogloss:1.00376\teval-mlogloss:1.08794\n",
            "[156]\ttrain-mlogloss:1.00317\teval-mlogloss:1.08794\n",
            "[157]\ttrain-mlogloss:1.00263\teval-mlogloss:1.08792\n",
            "[158]\ttrain-mlogloss:1.00198\teval-mlogloss:1.08785\n",
            "[159]\ttrain-mlogloss:1.00134\teval-mlogloss:1.08783\n",
            "[160]\ttrain-mlogloss:1.0007\teval-mlogloss:1.08777\n",
            "[161]\ttrain-mlogloss:1.00005\teval-mlogloss:1.0877\n",
            "[162]\ttrain-mlogloss:0.999449\teval-mlogloss:1.08764\n",
            "[163]\ttrain-mlogloss:0.998865\teval-mlogloss:1.08762\n",
            "[164]\ttrain-mlogloss:0.998347\teval-mlogloss:1.08759\n",
            "[165]\ttrain-mlogloss:0.997677\teval-mlogloss:1.08746\n",
            "[166]\ttrain-mlogloss:0.997089\teval-mlogloss:1.08735\n",
            "[167]\ttrain-mlogloss:0.99645\teval-mlogloss:1.08734\n",
            "[168]\ttrain-mlogloss:0.995882\teval-mlogloss:1.08727\n",
            "[169]\ttrain-mlogloss:0.995226\teval-mlogloss:1.08722\n",
            "[170]\ttrain-mlogloss:0.994838\teval-mlogloss:1.08721\n",
            "[171]\ttrain-mlogloss:0.994259\teval-mlogloss:1.08716\n",
            "[172]\ttrain-mlogloss:0.993769\teval-mlogloss:1.08715\n",
            "[173]\ttrain-mlogloss:0.99322\teval-mlogloss:1.08711\n",
            "[174]\ttrain-mlogloss:0.992602\teval-mlogloss:1.08707\n",
            "[175]\ttrain-mlogloss:0.992103\teval-mlogloss:1.08707\n",
            "[176]\ttrain-mlogloss:0.991425\teval-mlogloss:1.08699\n",
            "[177]\ttrain-mlogloss:0.990733\teval-mlogloss:1.08701\n",
            "[178]\ttrain-mlogloss:0.990186\teval-mlogloss:1.08697\n",
            "[179]\ttrain-mlogloss:0.989489\teval-mlogloss:1.08693\n",
            "[180]\ttrain-mlogloss:0.988925\teval-mlogloss:1.08695\n",
            "[181]\ttrain-mlogloss:0.988333\teval-mlogloss:1.08693\n",
            "[182]\ttrain-mlogloss:0.987874\teval-mlogloss:1.08688\n",
            "[183]\ttrain-mlogloss:0.987383\teval-mlogloss:1.08685\n",
            "[184]\ttrain-mlogloss:0.986964\teval-mlogloss:1.0869\n",
            "[185]\ttrain-mlogloss:0.986323\teval-mlogloss:1.08686\n",
            "[186]\ttrain-mlogloss:0.985762\teval-mlogloss:1.08685\n",
            "[187]\ttrain-mlogloss:0.98529\teval-mlogloss:1.08682\n",
            "[188]\ttrain-mlogloss:0.984683\teval-mlogloss:1.08684\n",
            "[189]\ttrain-mlogloss:0.984116\teval-mlogloss:1.08686\n",
            "[190]\ttrain-mlogloss:0.983554\teval-mlogloss:1.08683\n",
            "[191]\ttrain-mlogloss:0.983034\teval-mlogloss:1.08681\n",
            "[192]\ttrain-mlogloss:0.982566\teval-mlogloss:1.08682\n",
            "[193]\ttrain-mlogloss:0.982116\teval-mlogloss:1.08682\n",
            "[194]\ttrain-mlogloss:0.981599\teval-mlogloss:1.08682\n",
            "[195]\ttrain-mlogloss:0.981079\teval-mlogloss:1.08678\n",
            "[196]\ttrain-mlogloss:0.980575\teval-mlogloss:1.0868\n",
            "[197]\ttrain-mlogloss:0.980127\teval-mlogloss:1.08677\n",
            "[198]\ttrain-mlogloss:0.979603\teval-mlogloss:1.08677\n",
            "[199]\ttrain-mlogloss:0.979131\teval-mlogloss:1.08679\n",
            "[200]\ttrain-mlogloss:0.97847\teval-mlogloss:1.08673\n",
            "[201]\ttrain-mlogloss:0.977985\teval-mlogloss:1.08676\n",
            "[202]\ttrain-mlogloss:0.977542\teval-mlogloss:1.08675\n",
            "[203]\ttrain-mlogloss:0.97693\teval-mlogloss:1.08672\n",
            "[204]\ttrain-mlogloss:0.976447\teval-mlogloss:1.08671\n",
            "[205]\ttrain-mlogloss:0.97596\teval-mlogloss:1.08671\n",
            "[206]\ttrain-mlogloss:0.975475\teval-mlogloss:1.08668\n",
            "[207]\ttrain-mlogloss:0.974989\teval-mlogloss:1.08666\n",
            "[208]\ttrain-mlogloss:0.974428\teval-mlogloss:1.0867\n",
            "[209]\ttrain-mlogloss:0.973848\teval-mlogloss:1.08675\n",
            "[210]\ttrain-mlogloss:0.973334\teval-mlogloss:1.08672\n",
            "[211]\ttrain-mlogloss:0.972784\teval-mlogloss:1.08677\n",
            "[212]\ttrain-mlogloss:0.972305\teval-mlogloss:1.0868\n",
            "[213]\ttrain-mlogloss:0.971889\teval-mlogloss:1.08679\n",
            "[214]\ttrain-mlogloss:0.971462\teval-mlogloss:1.08681\n",
            "[215]\ttrain-mlogloss:0.971002\teval-mlogloss:1.08682\n",
            "[216]\ttrain-mlogloss:0.970444\teval-mlogloss:1.08678\n",
            "[217]\ttrain-mlogloss:0.969902\teval-mlogloss:1.08676\n",
            "[218]\ttrain-mlogloss:0.969364\teval-mlogloss:1.08677\n",
            "[219]\ttrain-mlogloss:0.96885\teval-mlogloss:1.08674\n",
            "[220]\ttrain-mlogloss:0.968355\teval-mlogloss:1.08677\n",
            "[221]\ttrain-mlogloss:0.967739\teval-mlogloss:1.08676\n",
            "[222]\ttrain-mlogloss:0.967129\teval-mlogloss:1.08679\n",
            "[223]\ttrain-mlogloss:0.966644\teval-mlogloss:1.08682\n",
            "[224]\ttrain-mlogloss:0.966233\teval-mlogloss:1.08686\n",
            "[225]\ttrain-mlogloss:0.965701\teval-mlogloss:1.08683\n",
            "[226]\ttrain-mlogloss:0.965185\teval-mlogloss:1.0868\n",
            "[227]\ttrain-mlogloss:0.964619\teval-mlogloss:1.08683\n",
            "Stopping. Best iteration:\n",
            "[207]\ttrain-mlogloss:0.974989\teval-mlogloss:1.08666\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG4v1Wv9AdDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apk(actual, predicted, k=7, default=0.0):\n",
        "    if len(predicted) > k:\n",
        "        predicted = predicted[:k]\n",
        "    #MAP@7 이므로, 최대 7개만 사용\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i , p in enumerate(predicted):\n",
        "        #점수를 부여하는 조건은 다음과 같음 :\n",
        "        # 예측 값이 정답에 있고 ('p in actual')\n",
        "        # 예측 값이 중복이 아니면('p not in predicted[:i]')\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i+1.0)\n",
        "\n",
        "        #정답 값이 공백일 경우, 무조건 0.0을 반환\n",
        "        if not actual:\n",
        "            return default\n",
        "\n",
        "        #정답의 개수(len(actual))로 averate precision을 구한다\n",
        "        return score / min(len(actual), k)\n",
        "\n",
        "#list of list인 정답 값(actual)과 예측값(predicted)에서 고객별 Average Precision을 구하고, np.mean()을 통해 평균을 계산\n",
        "def mapk(actual, predicted, k=7, default=0.0):\n",
        "    return np.mean([apk(a,p,k,default) for a,p in zip(actual, predicted)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd57-Yl_7Ae2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "5.   평가\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35tunzI47AA5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebaed8ac-8e21-46fd-f452-52b2a244cb50"
      },
      "source": [
        "#MAP@7 평가 척도를 위한 준비작업.\n",
        "# 고객 식별 버놓를 추출한다.\n",
        "vld = trn[trn['fecha_dato'] == vld_date]\n",
        "ncodpers_vld = vld.as_matrix(columns=['ncodpers'])\n",
        "\n",
        "print(\"ncodpers_vld : \", ncodpers_vld)\n",
        "print(\"vld : \", vld)\n",
        "\n",
        "# 검증 데이터에서 신규 구매를 구한다\n",
        "for prod in prods:\n",
        "  prev = prod + '_prev'\n",
        "  padd = prod + '_add'\n",
        "  vld[padd] = vld[prod] - vld[prev]\n",
        "add_vld = vld.as_matrix(columns=[prod + '_add' for prod in prods])\n",
        "\n",
        "print(\"add_vld : \",add_vld)\n",
        "\n",
        "print(\"ncodpers_vld.len : \", len(ncodpers_vld))\n",
        "\n",
        "add_vld_list = [list() for i in range(len(ncodpers_vld))]\n",
        "\n",
        "print(\"add_vld_list.len : \", add_vld_list[0:10])\n",
        "\n",
        "#import map7 as mapk\n",
        "# 고객 데이터에서 얻을 수 있는 MAP@7 최고점을 미리 구함.(0.042663)\n",
        "print(mapk(add_vld_list, add_vld_list, 7, 0.0))\n",
        "\n",
        "# 검증 데이터에 대한 예측값을 구한다\n",
        "X_vld = vld.as_matrix(columns=features)\n",
        "Y_vld = vld.as_matrix(columns=['y'])\n",
        "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
        "preds_vld = model.predict(dvld, ntree_limit=best_ntree_limit)\n",
        "\n",
        "# 저번 달에 보유한 제품은 신규 구매가 불가하기 때문에, 확률값에서 미리 1을 빼줌\n",
        "preds_vld = preds_vld - vld.as_matrix(columns=[prod + '_prev' for prod in prods])\n",
        "\n",
        "# 검증 데이터 예측 상위 7개를 추출\n",
        "result_vld = []\n",
        "for ncodper, pred in zip(ncodpers_vld, preds_vld):\n",
        "  y_prods = [(y,p,ip) for y, p, ip, in zip(pred, prods, range(len(prods)))]\n",
        "  y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
        "  result_vld.append([ip for y,p,ip in y_prods])\n",
        "\n",
        "# 검증 데이터에서의 MAP@7 점수를 구한다.\n",
        "print(mapk(add_vld_list, result_vld, 7, 0.0))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ncodpers_vld :  [[ 657788]\n",
            " [ 657795]\n",
            " [ 657790]\n",
            " ...\n",
            " [1166763]\n",
            " [1166789]\n",
            " [1550586]]\n",
            "vld :            fecha_dato  ...  ult_fec_cli_1t_year_prev\n",
            "10394531  2016-05-28  ...                       0.0\n",
            "10394532  2016-05-28  ...                       0.0\n",
            "10394533  2016-05-28  ...                       0.0\n",
            "10394534  2016-05-28  ...                       0.0\n",
            "10394535  2016-05-28  ...                       0.0\n",
            "...              ...  ...                       ...\n",
            "11091065  2016-05-28  ...                       0.0\n",
            "11091066  2016-05-28  ...                       0.0\n",
            "11091067  2016-05-28  ...                       0.0\n",
            "11091068  2016-05-28  ...                       0.0\n",
            "11091069  2016-05-28  ...                     -99.0\n",
            "\n",
            "[696539 rows x 104 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add_vld :  [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]]\n",
            "ncodpers_vld.len :  696539\n",
            "add_vld_list.len :  [[], [], [], [], [], [], [], [], [], []]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7f1c17636dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#import map7 as mapk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# 고객 데이터에서 얻을 수 있는 MAP@7 최고점을 미리 구함.(0.042663)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_vld_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_vld_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# 검증 데이터에 대한 예측값을 구한다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-3d8041f61147>\u001b[0m in \u001b[0;36mmapk\u001b[0;34m(actual, predicted, k, default)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#list of list인 정답 값(actual)과 예측값(predicted)에서 고객별 Average Precision을 구하고, np.mean()을 통해 평균을 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmapk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mapk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3257\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         ret = um.true_divide(\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JugKcQJ7lgqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}