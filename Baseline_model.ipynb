{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN3udjuWWZPhKjan2FNKAZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimcaprio/lifeofkaggle/blob/master/Baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wHJ6sFD2mls",
        "colab_type": "text"
      },
      "source": [
        "# Baseline 모델\n",
        "## tabular 데이터를 다루는 캐클의 파이프라인\n",
        "### 1. 데이터 전처리 -> 2. 피처엔지니어링 -> 3. 머신러닝 모델학습 -> 4. 테스트 데이터 예측 및 캐글 업로드\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwVGkqiC3Gb5",
        "colab_type": "text"
      },
      "source": [
        "1.   데이터 전처리\n",
        "\n",
        "*   제품 변수의 결측값을 0으로 대체. 제품 보유 여부에 대한 정보가 없으면, 해당 제품을 보유하지 않고 있지 않다고 가정.\n",
        "*   훈련 데이터와 테스트 데이터를 통합. 훈련 데이터와 테스트 데이터는 날짜변수(fecha_dato)로 쉽게 구분 가능. 동일한 24개의 고객 변수를 공유하고 있으며, 테스트 데이터에 없는 24개의 제품 변수는 0으로 채움.\n",
        "* 번주형, 수치형 데이터를 전처리. 변수형 데이터는 .factorize()통해 label encoding을 수행. 데이터 타입이 object로 표현되는 수치형 데이터에서는 .unique()를 통해 특이값들을 대체하거나 제거하고, 정수형 데이터로 변환.\n",
        "*   추후 모델 학습에 사용할 변수 이름을 features 리스트에 미리 담는다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuO1MEyA69ok",
        "colab_type": "code",
        "outputId": "3b144c59-9291-446a-fd19-573b7c50e4c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lVUCwgC7TtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/My Drive/CoLab/explorer_of_machine_learning/for_kaggle/.kaggle/\" # put path for wherever you put it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkCA9mkb2l_7",
        "colab_type": "code",
        "outputId": "8fa2738a-bab2-4e43-8757-6849eab1d33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "\n",
        "np.random.seed(2018)\n",
        "\n",
        "# 데이터를 불러온다\n",
        "trn = pd.read_csv('/content/drive/My Drive/CoLab/explorer_of_machine_learning/ch02_santander/data/train_ver2.csv')\n",
        "tst = pd.read_csv('/content/drive/My Drive/CoLab/explorer_of_machine_learning/ch02_santander/data/test_ver2.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76du4a0Aqohx",
        "colab_type": "code",
        "outputId": "e6700b4a-ed02-4850-a801-c16cb2804777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "## 데이터 전처리\n",
        "# 제품 변수를 별도로 저장\n",
        "prods = trn.columns[24:].tolist()\n",
        "\n",
        "# 제품 변수 결측값을 미리 0으로 대체\n",
        "trn[prods] = trn[prods].fillna(0.0).astype(np.int8)\n",
        "\n",
        "# 24개 제품 중 하나도 보유하지 않는 고객 데이터를 제거\n",
        "no_product = trn[prods].sum(axis=1) == 0\n",
        "trn = trn[~no_product]\n",
        "\n",
        "# 훈련 데이터와 테스트 데이터를 통합. 테스트 데이터에 없는 제품 변수는 0으로 대체\n",
        "for col in trn.columns[24:]:\n",
        "  tst[col] = 0\n",
        "df = pd.concat([trn, tst], axis=0)\n",
        "\n",
        "# 학습에 사용할 변수를 담는 list\n",
        "features = []\n",
        "\n",
        "# 범주형 변수를 .factorize() 함수를 통해 label encoding 함(one-hot) -- 날짜 데이터는 제외 시켜야...\n",
        "# 그래서 모든 object type 변수를 fatorize 해서는 안됨... categorical_cols = [col for col in trn.columns[:24] if trn[col].dtype in ['O']]\n",
        "categorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp','canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\n",
        "\n",
        "for col in categorical_cols:\n",
        "  df[col], _ = df[col].factorize(na_sentinel=-99)\n",
        "features += categorical_cols\n",
        "\n",
        "# 수치형 변수의 특이값과 결측값을 -99로 대체하고, 정수형으로 변환\n",
        "df['age'].replace(' NA', -99, inplace=True)\n",
        "df['age'] = df['age'].astype(np.int8)\n",
        "\n",
        "df['antiguedad'].replace('     NA', -99, inplace=True)\n",
        "df['antiguedad'] = df['antiguedad'].astype(np.int8)\n",
        "\n",
        "df['renta'].replace('         NA', -99, inplace=True)\n",
        "df['renta'].fillna(-99, inplace=True)\n",
        "df['renta'] = df['renta'].astype(float).astype(np.int8)\n",
        "\n",
        "df['indrel_1mes'].replace('P', 5, inplace=True)\n",
        "df['indrel_1mes'].fillna(-99, inplace=True)\n",
        "df['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)\n",
        "\n",
        "#학습에 사용할 수치형 변수를 features에 추가\n",
        "#features +=  [col for col in trn.columns[:24] if trn[col].dtype in ['int64', 'float64']]\n",
        "features += ['age', 'antiguedad', 'renta', 'ind_nuevo', 'indrel', 'indrel_1mes', 'ind_actividad_cliente']\n",
        "print(features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento', 'age', 'antiguedad', 'renta', 'ind_nuevo', 'indrel', 'indrel_1mes', 'ind_actividad_cliente']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MeYTErP888",
        "colab_type": "text"
      },
      "source": [
        "2.   피처 엔지니어링\n",
        "*   모델 학습에 사용할 파생변수를 생성."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruovoQQ1eU5U",
        "colab_type": "code",
        "outputId": "97a021ed-282b-465d-fe2b-ed3f0c81d752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "df['fecha_alta'].head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2015-01-12\n",
              "1    2012-08-10\n",
              "2    2012-08-10\n",
              "3    2012-08-10\n",
              "4    2012-08-10\n",
              "Name: fecha_alta, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjZdfeboP8UX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (피처 엔지니어링) 두 날짜 변수에서 연도와 월 정보를 추출\n",
        "df['fecha_alta_month'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
        "df['fecha_alta_year'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
        "features += ['fecha_alta_month', 'fecha_alta_year']\n",
        "\n",
        "df['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
        "df['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
        "features += ['ult_fec_cli_1t_month', 'ult_cli_1t_year']\n",
        "\n",
        "# 그 외 변수의 결측값은 모두 -99로 변환\n",
        "df.fillna(-99, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QR5zFM_XjM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (피처 엔지니어링) lag-1 데이터를 생성\n",
        "# # 날짜를 숫자로 변환하는 함수. 2015-01-28 은 1, 2016-06-28은 18로 변환\n",
        "def date_to_int(str_date):\n",
        "  Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]\n",
        "  int_date = (int(Y) - 2015) * 12 + int(M)\n",
        "  return int_date\n",
        "\n",
        "# 날짜를 숫자로 변환하여 int_date에 저장\n",
        "df['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)\n",
        "\n",
        "# 데이터를 복사하고, int_date 날짜에 1을 더하여 lag를 생성. 변수명에 _prev를 추가\n",
        "df_lag = df.copy()\n",
        "df_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns]\n",
        "df_lag['int_date'] += 1\n",
        "\n",
        "# 원본 데이터와 lag 데이터를 ncodper와 int_date 기준으로 합침. lag 데이터의 int_date는 1이 밀려 있기 때문에, 저번 달의 제품 정보가 삽입.\n",
        "df_trn = df.merge(df_lag, on=['ncodpers', 'int_date'], how='left')\n",
        "\n",
        "# 메모리 효율을 위해 불필요한 변수르 메모리에서 제거.\n",
        "#del df, df_lag\n",
        "\n",
        "# 저번달의 제품 정보가 존재하지 않을 경우를 대비하여 0으로 대체\n",
        "for prod in prods:\n",
        "  prev = prod + '_prev'\n",
        "  df_trn[prev].fillna(0, inplace=True)\n",
        "df_trn.fillna(-99, inplace=True)\n",
        "\n",
        "# lag-1 변수를 추가\n",
        "features += [feature + '_prev' for feature in features]\n",
        "features += [prod + '_prev' for prod in prods]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttt1nxKUZXlT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "3.   교차검증\n",
        "\n",
        "\n",
        "*   테스트 데이터 제외하고 주어진 train 데이터를 활용하여 교차검증 데이터로 활용\n",
        "*   시계열 데이터의 경우, 테스트 데이터 중 마지막 날짜 또는 년, 월 을 교차검증 데이터로 분리하여 활용하는 것이 일반적. <- 미래를 예측하는 모델이기 때문\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQKAez1jaApF",
        "colab_type": "code",
        "outputId": "ac76c3ab-957f-4e37-9acd-3eeb0122f280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "source": [
        "## 모델 학습\n",
        "# 학습을 위하여 데이터를 훈련, 테스트용으로 분리\n",
        "# 학습에는 2016-01-28 ~ 2016-04-28 epdlxjaks tkdyd, 검증에는 2016-05-28 epdlxjfmf tkdyd.\n",
        "use_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
        "trn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\n",
        "tst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\n",
        "#del df_trn\n",
        "\n",
        "print(\"tst\", tst)\n",
        "\n",
        "# 훈련 데이터에서 신규 구매 건수만 추출\n",
        "X = []\n",
        "Y = []\n",
        "for i, prod in enumerate(prods):\n",
        "  prev = prod + '_prev'\n",
        "  prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n",
        "  prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
        "  X.append(prX)\n",
        "  Y.append(prY)\n",
        "XY = pd.concat(X)\n",
        "Y = np.hstack(Y)\n",
        "XY['y'] = Y\n",
        "\n",
        "#훈련, 검증 데이터로 분리\n",
        "vld_date = '2016-05-28'\n",
        "XY_trn = XY[XY['fecha_dato'] != vld_date]\n",
        "XY_vld = XY[XY['fecha_dato'] == vld_date]\n",
        "\n",
        "print(\"XY_trn\", XY_trn)\n",
        "print(\"XY_vld\", XY_vld)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tst           fecha_dato  ...  ult_fec_cli_1t_year_prev\n",
            "11091070  2016-06-28  ...                       0.0\n",
            "11091071  2016-06-28  ...                       0.0\n",
            "11091072  2016-06-28  ...                       0.0\n",
            "11091073  2016-06-28  ...                       0.0\n",
            "11091074  2016-06-28  ...                       0.0\n",
            "...              ...  ...                       ...\n",
            "12020680  2016-06-28  ...                       0.0\n",
            "12020681  2016-06-28  ...                     -99.0\n",
            "12020682  2016-06-28  ...                       0.0\n",
            "12020683  2016-06-28  ...                       0.0\n",
            "12020684  2016-06-28  ...                       0.0\n",
            "\n",
            "[929615 rows x 104 columns]\n",
            "XY_trn           fecha_dato  ncodpers  ...  ult_fec_cli_1t_year_prev   y\n",
            "7658069   2016-01-28   1474324  ...                       0.0   1\n",
            "7628180   2016-01-28   1432311  ...                     -99.0   2\n",
            "7628198   2016-01-28   1432232  ...                     -99.0   2\n",
            "7628482   2016-01-28   1432080  ...                     -99.0   2\n",
            "7628692   2016-01-28   1432952  ...                       0.0   2\n",
            "...              ...       ...  ...                       ...  ..\n",
            "10394466  2016-04-28   1297367  ...                       0.0  23\n",
            "10394481  2016-04-28   1297315  ...                       0.0  23\n",
            "10394487  2016-04-28   1297332  ...                       0.0  23\n",
            "10394502  2016-04-28   1297428  ...                       0.0  23\n",
            "10394529  2016-04-28   1297412  ...                       0.0  23\n",
            "\n",
            "[161137 rows x 105 columns]\n",
            "XY_vld           fecha_dato  ncodpers  ...  ult_fec_cli_1t_year_prev   y\n",
            "10597872  2016-05-28    194160  ...                       0.0   0\n",
            "10394747  2016-05-28    658081  ...                       0.0   2\n",
            "10395030  2016-05-28    658132  ...                     -99.0   2\n",
            "10395104  2016-05-28    658521  ...                     -99.0   2\n",
            "10395155  2016-05-28    655909  ...                     -99.0   2\n",
            "...              ...       ...  ...                       ...  ..\n",
            "11090455  2016-05-28   1166385  ...                       0.0  23\n",
            "11090468  2016-05-28   1166355  ...                       0.0  23\n",
            "11090667  2016-05-28   1166343  ...                       0.0  23\n",
            "11090696  2016-05-28   1166232  ...                       0.0  23\n",
            "11090782  2016-05-28   1166874  ...                       0.0  23\n",
            "\n",
            "[37897 rows x 105 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUToWvAJwBni",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   xgboost 모델을 사용 하여 학습 모델 만들기\n",
        "2.   주요 파라미터\n",
        "*   max_depth : 트리 모델의 최대 깊이를 의미. 값이 높을 수록 더 복잡한 트리를 만들지만, 과적합의 원인이 될수 있음.\n",
        "*   eta : 딥러닝에서의 learning rate 의미. 0과 1사이의 값을 가지며, 값이 너무 높으면 학습이 잘 안되고, 낮으면 학습에 시간이 오래 걸림.\n",
        "*   colsample_bytree : 트리를 생성할 때 룬련 데이터에서 변수를 샘플링해주는 비율. 마찬가지로 과적합을 막아주는 방안. (딥러닝의 drop out 개념?) 보통 0.6 ~ 0.9\n",
        "*   colsample_bylevel : 트리의 레벨 별로 훈련 데이터의 변수를 샘플링해주는 비율. 보통 0.6 ~ 0.9\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XWYHmwEwBBl",
        "colab_type": "code",
        "outputId": "14a5e38d-d38e-4bec-8c89-82c7de8fc871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## XGBoost 모델 parameter를 설정\n",
        "param = {\n",
        "  'booster': 'gbtree',\n",
        "  'max_depth' : 8,\n",
        "  'nthread' : 4,\n",
        "  'num_class' : len(prods),\n",
        "  'objective' : 'multi:softprob',\n",
        "  'silent' : 1,\n",
        "  'eval_metric' : 'mlogloss',\n",
        "  'eta' : 0.1,\n",
        "  'min_child_weight' : 10,\n",
        "  'colsample_bytree' : 0.8,\n",
        "  'colsample_bylevel' : 0.9,\n",
        "  'seed' : 2018,\n",
        "}\n",
        "\n",
        "# 훈련, 검증 데이터를 XGBoost 형태로 변환\n",
        "X_trn = XY_trn.as_matrix(columns=features)\n",
        "Y_trn = XY_trn.as_matrix(columns=['y'])\n",
        "dtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n",
        "\n",
        "X_vld = XY_vld.as_matrix(columns=features)\n",
        "Y_vld = XY_vld.as_matrix(columns=['y'])\n",
        "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
        "\n",
        "print(Y_vld)\n",
        "\n",
        "# XBBoost 모델을 훈련 데이터로 학습\n",
        "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
        "model = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)\n",
        "\n",
        "import pickle\n",
        "pickle.dump(model, open('/content/drive/My Drive/CoLab/explorer_of_machine_learning/ch02_santander/modle/xgb.baseline.pkl', 'wb'))\n",
        "best_ntree_limit = model.best_ntree_limit"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 0]\n",
            " [ 2]\n",
            " [ 2]\n",
            " ...\n",
            " [23]\n",
            " [23]\n",
            " [23]]\n",
            "[0]\ttrain-mlogloss:2.6752\teval-mlogloss:2.68357\n",
            "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until eval-mlogloss hasn't improved in 20 rounds.\n",
            "[1]\ttrain-mlogloss:2.43959\teval-mlogloss:2.4521\n",
            "[2]\ttrain-mlogloss:2.26071\teval-mlogloss:2.27549\n",
            "[3]\ttrain-mlogloss:2.12697\teval-mlogloss:2.14326\n",
            "[4]\ttrain-mlogloss:2.01445\teval-mlogloss:2.03139\n",
            "[5]\ttrain-mlogloss:1.92002\teval-mlogloss:1.93765\n",
            "[6]\ttrain-mlogloss:1.84373\teval-mlogloss:1.86197\n",
            "[7]\ttrain-mlogloss:1.77481\teval-mlogloss:1.79374\n",
            "[8]\ttrain-mlogloss:1.71598\teval-mlogloss:1.73584\n",
            "[9]\ttrain-mlogloss:1.66264\teval-mlogloss:1.68283\n",
            "[10]\ttrain-mlogloss:1.61532\teval-mlogloss:1.63602\n",
            "[11]\ttrain-mlogloss:1.57299\teval-mlogloss:1.59415\n",
            "[12]\ttrain-mlogloss:1.53602\teval-mlogloss:1.55785\n",
            "[13]\ttrain-mlogloss:1.50103\teval-mlogloss:1.52306\n",
            "[14]\ttrain-mlogloss:1.46888\teval-mlogloss:1.49102\n",
            "[15]\ttrain-mlogloss:1.44007\teval-mlogloss:1.46248\n",
            "[16]\ttrain-mlogloss:1.41394\teval-mlogloss:1.43632\n",
            "[17]\ttrain-mlogloss:1.39043\teval-mlogloss:1.41319\n",
            "[18]\ttrain-mlogloss:1.36815\teval-mlogloss:1.39104\n",
            "[19]\ttrain-mlogloss:1.34773\teval-mlogloss:1.37044\n",
            "[20]\ttrain-mlogloss:1.32872\teval-mlogloss:1.35157\n",
            "[21]\ttrain-mlogloss:1.31158\teval-mlogloss:1.33487\n",
            "[22]\ttrain-mlogloss:1.29593\teval-mlogloss:1.31953\n",
            "[23]\ttrain-mlogloss:1.28171\teval-mlogloss:1.30565\n",
            "[24]\ttrain-mlogloss:1.26847\teval-mlogloss:1.29285\n",
            "[25]\ttrain-mlogloss:1.25602\teval-mlogloss:1.28095\n",
            "[26]\ttrain-mlogloss:1.24436\teval-mlogloss:1.26952\n",
            "[27]\ttrain-mlogloss:1.2333\teval-mlogloss:1.25857\n",
            "[28]\ttrain-mlogloss:1.223\teval-mlogloss:1.24837\n",
            "[29]\ttrain-mlogloss:1.21386\teval-mlogloss:1.23967\n",
            "[30]\ttrain-mlogloss:1.205\teval-mlogloss:1.23131\n",
            "[31]\ttrain-mlogloss:1.19658\teval-mlogloss:1.22304\n",
            "[32]\ttrain-mlogloss:1.18865\teval-mlogloss:1.21531\n",
            "[33]\ttrain-mlogloss:1.18135\teval-mlogloss:1.20845\n",
            "[34]\ttrain-mlogloss:1.17431\teval-mlogloss:1.20194\n",
            "[35]\ttrain-mlogloss:1.16782\teval-mlogloss:1.19579\n",
            "[36]\ttrain-mlogloss:1.16179\teval-mlogloss:1.19015\n",
            "[37]\ttrain-mlogloss:1.15591\teval-mlogloss:1.18468\n",
            "[38]\ttrain-mlogloss:1.15047\teval-mlogloss:1.17958\n",
            "[39]\ttrain-mlogloss:1.14531\teval-mlogloss:1.17482\n",
            "[40]\ttrain-mlogloss:1.14045\teval-mlogloss:1.17039\n",
            "[41]\ttrain-mlogloss:1.13581\teval-mlogloss:1.16609\n",
            "[42]\ttrain-mlogloss:1.13144\teval-mlogloss:1.1621\n",
            "[43]\ttrain-mlogloss:1.12728\teval-mlogloss:1.15831\n",
            "[44]\ttrain-mlogloss:1.12327\teval-mlogloss:1.15472\n",
            "[45]\ttrain-mlogloss:1.11965\teval-mlogloss:1.15134\n",
            "[46]\ttrain-mlogloss:1.11597\teval-mlogloss:1.14812\n",
            "[47]\ttrain-mlogloss:1.11283\teval-mlogloss:1.14555\n",
            "[48]\ttrain-mlogloss:1.10974\teval-mlogloss:1.14285\n",
            "[49]\ttrain-mlogloss:1.10683\teval-mlogloss:1.14032\n",
            "[50]\ttrain-mlogloss:1.10393\teval-mlogloss:1.13775\n",
            "[51]\ttrain-mlogloss:1.10106\teval-mlogloss:1.13536\n",
            "[52]\ttrain-mlogloss:1.0985\teval-mlogloss:1.13326\n",
            "[53]\ttrain-mlogloss:1.09593\teval-mlogloss:1.13107\n",
            "[54]\ttrain-mlogloss:1.09349\teval-mlogloss:1.12895\n",
            "[55]\ttrain-mlogloss:1.09107\teval-mlogloss:1.12708\n",
            "[56]\ttrain-mlogloss:1.08876\teval-mlogloss:1.12535\n",
            "[57]\ttrain-mlogloss:1.08668\teval-mlogloss:1.12363\n",
            "[58]\ttrain-mlogloss:1.08461\teval-mlogloss:1.12191\n",
            "[59]\ttrain-mlogloss:1.08263\teval-mlogloss:1.12035\n",
            "[60]\ttrain-mlogloss:1.08083\teval-mlogloss:1.11895\n",
            "[61]\ttrain-mlogloss:1.0789\teval-mlogloss:1.11757\n",
            "[62]\ttrain-mlogloss:1.07709\teval-mlogloss:1.11626\n",
            "[63]\ttrain-mlogloss:1.0754\teval-mlogloss:1.11497\n",
            "[64]\ttrain-mlogloss:1.07388\teval-mlogloss:1.11385\n",
            "[65]\ttrain-mlogloss:1.07231\teval-mlogloss:1.11269\n",
            "[66]\ttrain-mlogloss:1.07088\teval-mlogloss:1.11171\n",
            "[67]\ttrain-mlogloss:1.06947\teval-mlogloss:1.11073\n",
            "[68]\ttrain-mlogloss:1.06806\teval-mlogloss:1.10987\n",
            "[69]\ttrain-mlogloss:1.06677\teval-mlogloss:1.10891\n",
            "[70]\ttrain-mlogloss:1.06545\teval-mlogloss:1.10805\n",
            "[71]\ttrain-mlogloss:1.06423\teval-mlogloss:1.10719\n",
            "[72]\ttrain-mlogloss:1.06291\teval-mlogloss:1.10641\n",
            "[73]\ttrain-mlogloss:1.0617\teval-mlogloss:1.10565\n",
            "[74]\ttrain-mlogloss:1.06053\teval-mlogloss:1.10482\n",
            "[75]\ttrain-mlogloss:1.05931\teval-mlogloss:1.10413\n",
            "[76]\ttrain-mlogloss:1.05826\teval-mlogloss:1.1035\n",
            "[77]\ttrain-mlogloss:1.05711\teval-mlogloss:1.10282\n",
            "[78]\ttrain-mlogloss:1.05611\teval-mlogloss:1.10224\n",
            "[79]\ttrain-mlogloss:1.05512\teval-mlogloss:1.10164\n",
            "[80]\ttrain-mlogloss:1.05413\teval-mlogloss:1.10104\n",
            "[81]\ttrain-mlogloss:1.05325\teval-mlogloss:1.10051\n",
            "[82]\ttrain-mlogloss:1.05227\teval-mlogloss:1.09994\n",
            "[83]\ttrain-mlogloss:1.05135\teval-mlogloss:1.09945\n",
            "[84]\ttrain-mlogloss:1.05038\teval-mlogloss:1.09901\n",
            "[85]\ttrain-mlogloss:1.04941\teval-mlogloss:1.09856\n",
            "[86]\ttrain-mlogloss:1.04852\teval-mlogloss:1.09813\n",
            "[87]\ttrain-mlogloss:1.04774\teval-mlogloss:1.09772\n",
            "[88]\ttrain-mlogloss:1.04686\teval-mlogloss:1.09737\n",
            "[89]\ttrain-mlogloss:1.04613\teval-mlogloss:1.09701\n",
            "[90]\ttrain-mlogloss:1.04543\teval-mlogloss:1.09664\n",
            "[91]\ttrain-mlogloss:1.04468\teval-mlogloss:1.09635\n",
            "[92]\ttrain-mlogloss:1.04378\teval-mlogloss:1.096\n",
            "[93]\ttrain-mlogloss:1.04306\teval-mlogloss:1.09568\n",
            "[94]\ttrain-mlogloss:1.0423\teval-mlogloss:1.09537\n",
            "[95]\ttrain-mlogloss:1.04158\teval-mlogloss:1.09506\n",
            "[96]\ttrain-mlogloss:1.04091\teval-mlogloss:1.09482\n",
            "[97]\ttrain-mlogloss:1.04016\teval-mlogloss:1.09451\n",
            "[98]\ttrain-mlogloss:1.03946\teval-mlogloss:1.09426\n",
            "[99]\ttrain-mlogloss:1.03861\teval-mlogloss:1.09398\n",
            "[100]\ttrain-mlogloss:1.03791\teval-mlogloss:1.09376\n",
            "[101]\ttrain-mlogloss:1.03733\teval-mlogloss:1.09353\n",
            "[102]\ttrain-mlogloss:1.03664\teval-mlogloss:1.09331\n",
            "[103]\ttrain-mlogloss:1.03598\teval-mlogloss:1.09307\n",
            "[104]\ttrain-mlogloss:1.03519\teval-mlogloss:1.09288\n",
            "[105]\ttrain-mlogloss:1.0346\teval-mlogloss:1.09264\n",
            "[106]\ttrain-mlogloss:1.03385\teval-mlogloss:1.0925\n",
            "[107]\ttrain-mlogloss:1.0332\teval-mlogloss:1.09233\n",
            "[108]\ttrain-mlogloss:1.03258\teval-mlogloss:1.09218\n",
            "[109]\ttrain-mlogloss:1.03199\teval-mlogloss:1.09198\n",
            "[110]\ttrain-mlogloss:1.0315\teval-mlogloss:1.09183\n",
            "[111]\ttrain-mlogloss:1.03089\teval-mlogloss:1.09168\n",
            "[112]\ttrain-mlogloss:1.0303\teval-mlogloss:1.09154\n",
            "[113]\ttrain-mlogloss:1.0297\teval-mlogloss:1.0914\n",
            "[114]\ttrain-mlogloss:1.02905\teval-mlogloss:1.09122\n",
            "[115]\ttrain-mlogloss:1.02837\teval-mlogloss:1.09108\n",
            "[116]\ttrain-mlogloss:1.02776\teval-mlogloss:1.0909\n",
            "[117]\ttrain-mlogloss:1.02723\teval-mlogloss:1.0908\n",
            "[118]\ttrain-mlogloss:1.02665\teval-mlogloss:1.09063\n",
            "[119]\ttrain-mlogloss:1.02611\teval-mlogloss:1.09052\n",
            "[120]\ttrain-mlogloss:1.02558\teval-mlogloss:1.09045\n",
            "[121]\ttrain-mlogloss:1.02493\teval-mlogloss:1.09028\n",
            "[122]\ttrain-mlogloss:1.02412\teval-mlogloss:1.09021\n",
            "[123]\ttrain-mlogloss:1.02349\teval-mlogloss:1.09009\n",
            "[124]\ttrain-mlogloss:1.02282\teval-mlogloss:1.08999\n",
            "[125]\ttrain-mlogloss:1.02221\teval-mlogloss:1.08989\n",
            "[126]\ttrain-mlogloss:1.02165\teval-mlogloss:1.08984\n",
            "[127]\ttrain-mlogloss:1.02094\teval-mlogloss:1.08978\n",
            "[128]\ttrain-mlogloss:1.0205\teval-mlogloss:1.0897\n",
            "[129]\ttrain-mlogloss:1.01978\teval-mlogloss:1.08959\n",
            "[130]\ttrain-mlogloss:1.01913\teval-mlogloss:1.08951\n",
            "[131]\ttrain-mlogloss:1.01849\teval-mlogloss:1.08939\n",
            "[132]\ttrain-mlogloss:1.01797\teval-mlogloss:1.08937\n",
            "[133]\ttrain-mlogloss:1.01719\teval-mlogloss:1.08925\n",
            "[134]\ttrain-mlogloss:1.01655\teval-mlogloss:1.08918\n",
            "[135]\ttrain-mlogloss:1.01599\teval-mlogloss:1.08911\n",
            "[136]\ttrain-mlogloss:1.0153\teval-mlogloss:1.08905\n",
            "[137]\ttrain-mlogloss:1.01457\teval-mlogloss:1.08894\n",
            "[138]\ttrain-mlogloss:1.01402\teval-mlogloss:1.08883\n",
            "[139]\ttrain-mlogloss:1.01329\teval-mlogloss:1.08871\n",
            "[140]\ttrain-mlogloss:1.01261\teval-mlogloss:1.08867\n",
            "[141]\ttrain-mlogloss:1.01208\teval-mlogloss:1.08862\n",
            "[142]\ttrain-mlogloss:1.0115\teval-mlogloss:1.08857\n",
            "[143]\ttrain-mlogloss:1.01088\teval-mlogloss:1.08846\n",
            "[144]\ttrain-mlogloss:1.01029\teval-mlogloss:1.08843\n",
            "[145]\ttrain-mlogloss:1.00952\teval-mlogloss:1.08834\n",
            "[146]\ttrain-mlogloss:1.00893\teval-mlogloss:1.08828\n",
            "[147]\ttrain-mlogloss:1.00836\teval-mlogloss:1.08826\n",
            "[148]\ttrain-mlogloss:1.00794\teval-mlogloss:1.08821\n",
            "[149]\ttrain-mlogloss:1.00741\teval-mlogloss:1.08817\n",
            "[150]\ttrain-mlogloss:1.00661\teval-mlogloss:1.08806\n",
            "[151]\ttrain-mlogloss:1.00595\teval-mlogloss:1.08802\n",
            "[152]\ttrain-mlogloss:1.00535\teval-mlogloss:1.08803\n",
            "[153]\ttrain-mlogloss:1.00478\teval-mlogloss:1.08798\n",
            "[154]\ttrain-mlogloss:1.00421\teval-mlogloss:1.08797\n",
            "[155]\ttrain-mlogloss:1.00376\teval-mlogloss:1.08794\n",
            "[156]\ttrain-mlogloss:1.00317\teval-mlogloss:1.08794\n",
            "[157]\ttrain-mlogloss:1.00263\teval-mlogloss:1.08792\n",
            "[158]\ttrain-mlogloss:1.00198\teval-mlogloss:1.08785\n",
            "[159]\ttrain-mlogloss:1.00134\teval-mlogloss:1.08783\n",
            "[160]\ttrain-mlogloss:1.0007\teval-mlogloss:1.08777\n",
            "[161]\ttrain-mlogloss:1.00005\teval-mlogloss:1.0877\n",
            "[162]\ttrain-mlogloss:0.999449\teval-mlogloss:1.08764\n",
            "[163]\ttrain-mlogloss:0.998865\teval-mlogloss:1.08762\n",
            "[164]\ttrain-mlogloss:0.998347\teval-mlogloss:1.08759\n",
            "[165]\ttrain-mlogloss:0.997677\teval-mlogloss:1.08746\n",
            "[166]\ttrain-mlogloss:0.997089\teval-mlogloss:1.08735\n",
            "[167]\ttrain-mlogloss:0.99645\teval-mlogloss:1.08734\n",
            "[168]\ttrain-mlogloss:0.995882\teval-mlogloss:1.08727\n",
            "[169]\ttrain-mlogloss:0.995226\teval-mlogloss:1.08722\n",
            "[170]\ttrain-mlogloss:0.994838\teval-mlogloss:1.08721\n",
            "[171]\ttrain-mlogloss:0.994259\teval-mlogloss:1.08716\n",
            "[172]\ttrain-mlogloss:0.993769\teval-mlogloss:1.08715\n",
            "[173]\ttrain-mlogloss:0.99322\teval-mlogloss:1.08711\n",
            "[174]\ttrain-mlogloss:0.992602\teval-mlogloss:1.08707\n",
            "[175]\ttrain-mlogloss:0.992103\teval-mlogloss:1.08707\n",
            "[176]\ttrain-mlogloss:0.991425\teval-mlogloss:1.08699\n",
            "[177]\ttrain-mlogloss:0.990733\teval-mlogloss:1.08701\n",
            "[178]\ttrain-mlogloss:0.990186\teval-mlogloss:1.08697\n",
            "[179]\ttrain-mlogloss:0.989489\teval-mlogloss:1.08693\n",
            "[180]\ttrain-mlogloss:0.988925\teval-mlogloss:1.08695\n",
            "[181]\ttrain-mlogloss:0.988333\teval-mlogloss:1.08693\n",
            "[182]\ttrain-mlogloss:0.987874\teval-mlogloss:1.08688\n",
            "[183]\ttrain-mlogloss:0.987383\teval-mlogloss:1.08685\n",
            "[184]\ttrain-mlogloss:0.986964\teval-mlogloss:1.0869\n",
            "[185]\ttrain-mlogloss:0.986323\teval-mlogloss:1.08686\n",
            "[186]\ttrain-mlogloss:0.985762\teval-mlogloss:1.08685\n",
            "[187]\ttrain-mlogloss:0.98529\teval-mlogloss:1.08682\n",
            "[188]\ttrain-mlogloss:0.984683\teval-mlogloss:1.08684\n",
            "[189]\ttrain-mlogloss:0.984116\teval-mlogloss:1.08686\n",
            "[190]\ttrain-mlogloss:0.983554\teval-mlogloss:1.08683\n",
            "[191]\ttrain-mlogloss:0.983034\teval-mlogloss:1.08681\n",
            "[192]\ttrain-mlogloss:0.982566\teval-mlogloss:1.08682\n",
            "[193]\ttrain-mlogloss:0.982116\teval-mlogloss:1.08682\n",
            "[194]\ttrain-mlogloss:0.981599\teval-mlogloss:1.08682\n",
            "[195]\ttrain-mlogloss:0.981079\teval-mlogloss:1.08678\n",
            "[196]\ttrain-mlogloss:0.980575\teval-mlogloss:1.0868\n",
            "[197]\ttrain-mlogloss:0.980127\teval-mlogloss:1.08677\n",
            "[198]\ttrain-mlogloss:0.979603\teval-mlogloss:1.08677\n",
            "[199]\ttrain-mlogloss:0.979131\teval-mlogloss:1.08679\n",
            "[200]\ttrain-mlogloss:0.97847\teval-mlogloss:1.08673\n",
            "[201]\ttrain-mlogloss:0.977985\teval-mlogloss:1.08676\n",
            "[202]\ttrain-mlogloss:0.977542\teval-mlogloss:1.08675\n",
            "[203]\ttrain-mlogloss:0.97693\teval-mlogloss:1.08672\n",
            "[204]\ttrain-mlogloss:0.976447\teval-mlogloss:1.08671\n",
            "[205]\ttrain-mlogloss:0.97596\teval-mlogloss:1.08671\n",
            "[206]\ttrain-mlogloss:0.975475\teval-mlogloss:1.08668\n",
            "[207]\ttrain-mlogloss:0.974989\teval-mlogloss:1.08666\n",
            "[208]\ttrain-mlogloss:0.974428\teval-mlogloss:1.0867\n",
            "[209]\ttrain-mlogloss:0.973848\teval-mlogloss:1.08675\n",
            "[210]\ttrain-mlogloss:0.973334\teval-mlogloss:1.08672\n",
            "[211]\ttrain-mlogloss:0.972784\teval-mlogloss:1.08677\n",
            "[212]\ttrain-mlogloss:0.972305\teval-mlogloss:1.0868\n",
            "[213]\ttrain-mlogloss:0.971889\teval-mlogloss:1.08679\n",
            "[214]\ttrain-mlogloss:0.971462\teval-mlogloss:1.08681\n",
            "[215]\ttrain-mlogloss:0.971002\teval-mlogloss:1.08682\n",
            "[216]\ttrain-mlogloss:0.970444\teval-mlogloss:1.08678\n",
            "[217]\ttrain-mlogloss:0.969902\teval-mlogloss:1.08676\n",
            "[218]\ttrain-mlogloss:0.969364\teval-mlogloss:1.08677\n",
            "[219]\ttrain-mlogloss:0.96885\teval-mlogloss:1.08674\n",
            "[220]\ttrain-mlogloss:0.968355\teval-mlogloss:1.08677\n",
            "[221]\ttrain-mlogloss:0.967739\teval-mlogloss:1.08676\n",
            "[222]\ttrain-mlogloss:0.967129\teval-mlogloss:1.08679\n",
            "[223]\ttrain-mlogloss:0.966644\teval-mlogloss:1.08682\n",
            "[224]\ttrain-mlogloss:0.966233\teval-mlogloss:1.08686\n",
            "[225]\ttrain-mlogloss:0.965701\teval-mlogloss:1.08683\n",
            "[226]\ttrain-mlogloss:0.965185\teval-mlogloss:1.0868\n",
            "[227]\ttrain-mlogloss:0.964619\teval-mlogloss:1.08683\n",
            "Stopping. Best iteration:\n",
            "[207]\ttrain-mlogloss:0.974989\teval-mlogloss:1.08666\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG4v1Wv9AdDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apk(actual, predicted, k=7, default=0.0):\n",
        "    if len(predicted) > k:\n",
        "        predicted = predicted[:k]\n",
        "    #MAP@7 이므로, 최대 7개만 사용\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i , p in enumerate(predicted):\n",
        "        #점수를 부여하는 조건은 다음과 같음 :\n",
        "        # 예측 값이 정답에 있고 ('p in actual')\n",
        "        # 예측 값이 중복이 아니면('p not in predicted[:i]')\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i+1.0)\n",
        "\n",
        "        #정답 값이 공백일 경우, 무조건 0.0을 반환\n",
        "    if not actual:\n",
        "      return default\n",
        "    # print(\"score / min(len(actual), k) :: \", score / min(len(actual), k))\n",
        "    #정답의 개수(len(actual))로 averate precision을 구한다\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "#list of list인 정답 값(actual)과 예측값(predicted)에서 고객별 Average Precision을 구하고, np.mean()을 통해 평균을 계산\n",
        "def mapk(actual, predicted, k=7, default=0.0):\n",
        "    # rst = [apk(a,p,k,default) for a,p in zip(actual, predicted)]\n",
        "    # print(\"rst ::: \", rst[:1000])\n",
        "    return np.mean([apk(a,p,k,default) for a,p in zip(actual, predicted)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd57-Yl_7Ae2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "5.   평가\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35tunzI47AA5",
        "colab_type": "code",
        "outputId": "0e1a5615-0b36-443b-dd09-aeb69e03750a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "#MAP@7 평가 척도를 위한 준비작업.\n",
        "# 고객 식별 버놓를 추출한다.\n",
        "vld = trn[trn['fecha_dato'] == vld_date]\n",
        "ncodpers_vld = vld.as_matrix(columns=['ncodpers'])\n",
        "\n",
        "#print(\"ncodpers_vld : \", ncodpers_vld)\n",
        "#print(\"vld : \", vld)\n",
        "\n",
        "# 검증 데이터에서 신규 구매를 구한다\n",
        "for prod in prods:\n",
        "  prev = prod + '_prev'\n",
        "  padd = prod + '_add'\n",
        "\n",
        "  vld[prev] = vld[prev].astype(float).astype(np.int8)\n",
        "  vld[prod] = vld[prod].astype(float).astype(np.int8)\n",
        "\n",
        "  vld[padd] = vld[prod] - vld[prev]\n",
        "\n",
        "add_vld = vld.as_matrix(columns=[prod + '_add' for prod in prods])\n",
        "#ncodpers_vld 크기의 list를 생성\n",
        "add_vld_list = [list() for i in range(len(ncodpers_vld))]\n",
        "\n",
        "# 고객별 신규 구매 정답 값을 add_vld_list에 저장하고, 총 count를 count_vld에 저장한다.\n",
        "count_vld = 0\n",
        "for ncodper in range(len(ncodpers_vld)):\n",
        "    for prod in range(len(prods)):\n",
        "      if add_vld[ncodper, prod] > 0:\n",
        "        # print(\"ncodper : \", ncodper, \"  || prod : \", prod)\n",
        "        add_vld_list[ncodper].append(prod)\n",
        "        count_vld += 1\n",
        "        \n",
        "# print(\"count_vld : \", count_vld)\n",
        "# print(\"add_vld_list[0:10] : \", add_vld_list[0:1000])\n",
        "\n",
        "#import map7 as mapk\n",
        "# 고객 데이터에서 얻을 수 있는 MAP@7 최고점을 미리 구함.(0.042663)\n",
        "print(mapk(add_vld_list, add_vld_list, 7, 0.0))\n",
        "\n",
        "# 검증 데이터에 대한 예측값을 구한다\n",
        "X_vld = vld.as_matrix(columns=features)\n",
        "Y_vld = vld.as_matrix(columns=['y'])\n",
        "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
        "preds_vld = model.predict(dvld, ntree_limit=best_ntree_limit)\n",
        "\n",
        "# 저번 달에 보유한 제품은 신규 구매가 불가하기 때문에, 확률값에서 미리 1을 빼줌\n",
        "preds_vld = preds_vld - vld.as_matrix(columns=[prod + '_prev' for prod in prods])\n",
        "\n",
        "# 검증 데이터 예측 상위 7개를 추출\n",
        "result_vld = []\n",
        "for ncodper, pred in zip(ncodpers_vld, preds_vld):\n",
        "  y_prods = [(y,p,ip) for y, p, ip, in zip(pred, prods, range(len(prods)))]\n",
        "  y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
        "  result_vld.append([ip for y,p,ip in y_prods])\n",
        "\n",
        "# 검증 데이터에서의 MAP@7 점수를 구한다.\n",
        "print(mapk(add_vld_list, result_vld, 7, 0.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.04266379915553903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.03646478055889802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkKDPASSvHXw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "5.   테스트 데이터 예측 및 캐글 업데이트\n",
        "\n",
        "\n",
        "*   XGBoost의 get_fscore() 함수를 통해서 학습한 모델의 변수 중요도를 출력 가능\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkrtp-4XvNvp",
        "colab_type": "code",
        "outputId": "3598e779-2865-46cf-c099-23398a0980ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# XGBoost 모델을 전체 훈련 데이터로 재학습한다!\n",
        "X_all = XY.as_matrix(columns=features)\n",
        "Y_all = XY.as_matrix(columns=['y'])\n",
        "dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
        "watch_list =[(dall, 'train')]\n",
        "\n",
        "# 트리 개수를 늘어난 데이터 양만큼 비례해서 증가\n",
        "best_ntree_limit = int(round(int(best_ntree_limit * (len(XY_trn) + len(XY_vld)))/(len(XY_trn))))\n",
        "# print(type(best_ntree_limit.astype(float8).astype(int8)))\n",
        "#XGBoost 모델 재 학습\n",
        "model = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list)\n",
        "\n",
        "#변수 중요도를 출력해본다. 예상하던 변수가 상위로 올라와 있는가?\n",
        "print(\"Feature importance:\")\n",
        "for kv in sorted([(k,v) for k, v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
        "  print(kv)\n",
        "\n",
        "# 캐글 제출을 위하여 테스트 데이터에 대한 예측값을 구함\n",
        "X_tst = tst.as_matrix(columns=features)\n",
        "dtst = xgb.DMatrix(X_tst, feature_names=features)\n",
        "preds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)\n",
        "ncodpers_tst = tst.as_matrix(columns=['ncodpers'])\n",
        "pred_tst = preds_tst - tst.as_matrix(columns=[prod + '_prev' for prod in prods])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\ttrain-mlogloss:2.67555\n",
            "[1]\ttrain-mlogloss:2.43966\n",
            "[2]\ttrain-mlogloss:2.26045\n",
            "[3]\ttrain-mlogloss:2.12681\n",
            "[4]\ttrain-mlogloss:2.01412\n",
            "[5]\ttrain-mlogloss:1.91965\n",
            "[6]\ttrain-mlogloss:1.84318\n",
            "[7]\ttrain-mlogloss:1.77418\n",
            "[8]\ttrain-mlogloss:1.71523\n",
            "[9]\ttrain-mlogloss:1.66179\n",
            "[10]\ttrain-mlogloss:1.61435\n",
            "[11]\ttrain-mlogloss:1.57202\n",
            "[12]\ttrain-mlogloss:1.53506\n",
            "[13]\ttrain-mlogloss:1.50008\n",
            "[14]\ttrain-mlogloss:1.46791\n",
            "[15]\ttrain-mlogloss:1.43891\n",
            "[16]\ttrain-mlogloss:1.4129\n",
            "[17]\ttrain-mlogloss:1.38938\n",
            "[18]\ttrain-mlogloss:1.36713\n",
            "[19]\ttrain-mlogloss:1.34675\n",
            "[20]\ttrain-mlogloss:1.32772\n",
            "[21]\ttrain-mlogloss:1.3107\n",
            "[22]\ttrain-mlogloss:1.2952\n",
            "[23]\ttrain-mlogloss:1.28089\n",
            "[24]\ttrain-mlogloss:1.26778\n",
            "[25]\ttrain-mlogloss:1.25541\n",
            "[26]\ttrain-mlogloss:1.24375\n",
            "[27]\ttrain-mlogloss:1.23268\n",
            "[28]\ttrain-mlogloss:1.22236\n",
            "[29]\ttrain-mlogloss:1.21331\n",
            "[30]\ttrain-mlogloss:1.2045\n",
            "[31]\ttrain-mlogloss:1.1961\n",
            "[32]\ttrain-mlogloss:1.18831\n",
            "[33]\ttrain-mlogloss:1.18101\n",
            "[34]\ttrain-mlogloss:1.17414\n",
            "[35]\ttrain-mlogloss:1.16768\n",
            "[36]\ttrain-mlogloss:1.16175\n",
            "[37]\ttrain-mlogloss:1.15599\n",
            "[38]\ttrain-mlogloss:1.15058\n",
            "[39]\ttrain-mlogloss:1.14546\n",
            "[40]\ttrain-mlogloss:1.14061\n",
            "[41]\ttrain-mlogloss:1.13595\n",
            "[42]\ttrain-mlogloss:1.13175\n",
            "[43]\ttrain-mlogloss:1.12764\n",
            "[44]\ttrain-mlogloss:1.12362\n",
            "[45]\ttrain-mlogloss:1.12004\n",
            "[46]\ttrain-mlogloss:1.11638\n",
            "[47]\ttrain-mlogloss:1.11334\n",
            "[48]\ttrain-mlogloss:1.11022\n",
            "[49]\ttrain-mlogloss:1.10739\n",
            "[50]\ttrain-mlogloss:1.10452\n",
            "[51]\ttrain-mlogloss:1.10177\n",
            "[52]\ttrain-mlogloss:1.09932\n",
            "[53]\ttrain-mlogloss:1.09677\n",
            "[54]\ttrain-mlogloss:1.09434\n",
            "[55]\ttrain-mlogloss:1.09199\n",
            "[56]\ttrain-mlogloss:1.08988\n",
            "[57]\ttrain-mlogloss:1.08779\n",
            "[58]\ttrain-mlogloss:1.08583\n",
            "[59]\ttrain-mlogloss:1.08388\n",
            "[60]\ttrain-mlogloss:1.08204\n",
            "[61]\ttrain-mlogloss:1.08025\n",
            "[62]\ttrain-mlogloss:1.07856\n",
            "[63]\ttrain-mlogloss:1.07701\n",
            "[64]\ttrain-mlogloss:1.07551\n",
            "[65]\ttrain-mlogloss:1.07389\n",
            "[66]\ttrain-mlogloss:1.07245\n",
            "[67]\ttrain-mlogloss:1.07106\n",
            "[68]\ttrain-mlogloss:1.06976\n",
            "[69]\ttrain-mlogloss:1.06844\n",
            "[70]\ttrain-mlogloss:1.0672\n",
            "[71]\ttrain-mlogloss:1.06591\n",
            "[72]\ttrain-mlogloss:1.06467\n",
            "[73]\ttrain-mlogloss:1.06357\n",
            "[74]\ttrain-mlogloss:1.06242\n",
            "[75]\ttrain-mlogloss:1.06124\n",
            "[76]\ttrain-mlogloss:1.06019\n",
            "[77]\ttrain-mlogloss:1.05908\n",
            "[78]\ttrain-mlogloss:1.05813\n",
            "[79]\ttrain-mlogloss:1.05723\n",
            "[80]\ttrain-mlogloss:1.05624\n",
            "[81]\ttrain-mlogloss:1.05524\n",
            "[82]\ttrain-mlogloss:1.05433\n",
            "[83]\ttrain-mlogloss:1.05346\n",
            "[84]\ttrain-mlogloss:1.0526\n",
            "[85]\ttrain-mlogloss:1.05175\n",
            "[86]\ttrain-mlogloss:1.05086\n",
            "[87]\ttrain-mlogloss:1.05008\n",
            "[88]\ttrain-mlogloss:1.0494\n",
            "[89]\ttrain-mlogloss:1.04863\n",
            "[90]\ttrain-mlogloss:1.04794\n",
            "[91]\ttrain-mlogloss:1.04711\n",
            "[92]\ttrain-mlogloss:1.0464\n",
            "[93]\ttrain-mlogloss:1.0458\n",
            "[94]\ttrain-mlogloss:1.04501\n",
            "[95]\ttrain-mlogloss:1.04434\n",
            "[96]\ttrain-mlogloss:1.04377\n",
            "[97]\ttrain-mlogloss:1.04313\n",
            "[98]\ttrain-mlogloss:1.04252\n",
            "[99]\ttrain-mlogloss:1.04189\n",
            "[100]\ttrain-mlogloss:1.04132\n",
            "[101]\ttrain-mlogloss:1.04076\n",
            "[102]\ttrain-mlogloss:1.04013\n",
            "[103]\ttrain-mlogloss:1.0396\n",
            "[104]\ttrain-mlogloss:1.0388\n",
            "[105]\ttrain-mlogloss:1.03815\n",
            "[106]\ttrain-mlogloss:1.0375\n",
            "[107]\ttrain-mlogloss:1.03688\n",
            "[108]\ttrain-mlogloss:1.03638\n",
            "[109]\ttrain-mlogloss:1.03589\n",
            "[110]\ttrain-mlogloss:1.03538\n",
            "[111]\ttrain-mlogloss:1.0349\n",
            "[112]\ttrain-mlogloss:1.03434\n",
            "[113]\ttrain-mlogloss:1.03387\n",
            "[114]\ttrain-mlogloss:1.03342\n",
            "[115]\ttrain-mlogloss:1.03297\n",
            "[116]\ttrain-mlogloss:1.03234\n",
            "[117]\ttrain-mlogloss:1.03175\n",
            "[118]\ttrain-mlogloss:1.03119\n",
            "[119]\ttrain-mlogloss:1.03069\n",
            "[120]\ttrain-mlogloss:1.03011\n",
            "[121]\ttrain-mlogloss:1.02948\n",
            "[122]\ttrain-mlogloss:1.02908\n",
            "[123]\ttrain-mlogloss:1.02864\n",
            "[124]\ttrain-mlogloss:1.02812\n",
            "[125]\ttrain-mlogloss:1.02759\n",
            "[126]\ttrain-mlogloss:1.02722\n",
            "[127]\ttrain-mlogloss:1.02663\n",
            "[128]\ttrain-mlogloss:1.02617\n",
            "[129]\ttrain-mlogloss:1.02572\n",
            "[130]\ttrain-mlogloss:1.02513\n",
            "[131]\ttrain-mlogloss:1.02448\n",
            "[132]\ttrain-mlogloss:1.02388\n",
            "[133]\ttrain-mlogloss:1.02336\n",
            "[134]\ttrain-mlogloss:1.02282\n",
            "[135]\ttrain-mlogloss:1.02213\n",
            "[136]\ttrain-mlogloss:1.02161\n",
            "[137]\ttrain-mlogloss:1.02111\n",
            "[138]\ttrain-mlogloss:1.02061\n",
            "[139]\ttrain-mlogloss:1.02004\n",
            "[140]\ttrain-mlogloss:1.01944\n",
            "[141]\ttrain-mlogloss:1.01898\n",
            "[142]\ttrain-mlogloss:1.01842\n",
            "[143]\ttrain-mlogloss:1.01797\n",
            "[144]\ttrain-mlogloss:1.01748\n",
            "[145]\ttrain-mlogloss:1.01697\n",
            "[146]\ttrain-mlogloss:1.01643\n",
            "[147]\ttrain-mlogloss:1.01595\n",
            "[148]\ttrain-mlogloss:1.01549\n",
            "[149]\ttrain-mlogloss:1.0149\n",
            "[150]\ttrain-mlogloss:1.0145\n",
            "[151]\ttrain-mlogloss:1.0138\n",
            "[152]\ttrain-mlogloss:1.01328\n",
            "[153]\ttrain-mlogloss:1.0128\n",
            "[154]\ttrain-mlogloss:1.01237\n",
            "[155]\ttrain-mlogloss:1.01182\n",
            "[156]\ttrain-mlogloss:1.01134\n",
            "[157]\ttrain-mlogloss:1.01076\n",
            "[158]\ttrain-mlogloss:1.01027\n",
            "[159]\ttrain-mlogloss:1.00979\n",
            "[160]\ttrain-mlogloss:1.00927\n",
            "[161]\ttrain-mlogloss:1.00882\n",
            "[162]\ttrain-mlogloss:1.00827\n",
            "[163]\ttrain-mlogloss:1.00782\n",
            "[164]\ttrain-mlogloss:1.00734\n",
            "[165]\ttrain-mlogloss:1.00684\n",
            "[166]\ttrain-mlogloss:1.00623\n",
            "[167]\ttrain-mlogloss:1.00577\n",
            "[168]\ttrain-mlogloss:1.00526\n",
            "[169]\ttrain-mlogloss:1.00483\n",
            "[170]\ttrain-mlogloss:1.00435\n",
            "[171]\ttrain-mlogloss:1.00374\n",
            "[172]\ttrain-mlogloss:1.00322\n",
            "[173]\ttrain-mlogloss:1.00279\n",
            "[174]\ttrain-mlogloss:1.0022\n",
            "[175]\ttrain-mlogloss:1.00162\n",
            "[176]\ttrain-mlogloss:1.0011\n",
            "[177]\ttrain-mlogloss:1.00063\n",
            "[178]\ttrain-mlogloss:1.00008\n",
            "[179]\ttrain-mlogloss:0.999532\n",
            "[180]\ttrain-mlogloss:0.999093\n",
            "[181]\ttrain-mlogloss:0.998435\n",
            "[182]\ttrain-mlogloss:0.998065\n",
            "[183]\ttrain-mlogloss:0.997553\n",
            "[184]\ttrain-mlogloss:0.997049\n",
            "[185]\ttrain-mlogloss:0.996605\n",
            "[186]\ttrain-mlogloss:0.996154\n",
            "[187]\ttrain-mlogloss:0.995594\n",
            "[188]\ttrain-mlogloss:0.995097\n",
            "[189]\ttrain-mlogloss:0.994513\n",
            "[190]\ttrain-mlogloss:0.993969\n",
            "[191]\ttrain-mlogloss:0.993629\n",
            "[192]\ttrain-mlogloss:0.993092\n",
            "[193]\ttrain-mlogloss:0.992707\n",
            "[194]\ttrain-mlogloss:0.992272\n",
            "[195]\ttrain-mlogloss:0.991841\n",
            "[196]\ttrain-mlogloss:0.991334\n",
            "[197]\ttrain-mlogloss:0.990841\n",
            "[198]\ttrain-mlogloss:0.990337\n",
            "[199]\ttrain-mlogloss:0.989975\n",
            "[200]\ttrain-mlogloss:0.989351\n",
            "[201]\ttrain-mlogloss:0.988954\n",
            "[202]\ttrain-mlogloss:0.988518\n",
            "[203]\ttrain-mlogloss:0.988023\n",
            "[204]\ttrain-mlogloss:0.987521\n",
            "[205]\ttrain-mlogloss:0.987119\n",
            "[206]\ttrain-mlogloss:0.98656\n",
            "[207]\ttrain-mlogloss:0.986059\n",
            "[208]\ttrain-mlogloss:0.985741\n",
            "[209]\ttrain-mlogloss:0.985299\n",
            "[210]\ttrain-mlogloss:0.984863\n",
            "[211]\ttrain-mlogloss:0.984326\n",
            "[212]\ttrain-mlogloss:0.983747\n",
            "[213]\ttrain-mlogloss:0.983324\n",
            "[214]\ttrain-mlogloss:0.982819\n",
            "[215]\ttrain-mlogloss:0.982375\n",
            "[216]\ttrain-mlogloss:0.98194\n",
            "[217]\ttrain-mlogloss:0.981528\n",
            "[218]\ttrain-mlogloss:0.981144\n",
            "[219]\ttrain-mlogloss:0.980699\n",
            "[220]\ttrain-mlogloss:0.980183\n",
            "[221]\ttrain-mlogloss:0.979741\n",
            "[222]\ttrain-mlogloss:0.979264\n",
            "[223]\ttrain-mlogloss:0.978812\n",
            "[224]\ttrain-mlogloss:0.978434\n",
            "[225]\ttrain-mlogloss:0.978003\n",
            "[226]\ttrain-mlogloss:0.977574\n",
            "[227]\ttrain-mlogloss:0.977116\n",
            "[228]\ttrain-mlogloss:0.976697\n",
            "[229]\ttrain-mlogloss:0.976185\n",
            "[230]\ttrain-mlogloss:0.975835\n",
            "[231]\ttrain-mlogloss:0.975403\n",
            "[232]\ttrain-mlogloss:0.974976\n",
            "[233]\ttrain-mlogloss:0.974475\n",
            "[234]\ttrain-mlogloss:0.97405\n",
            "[235]\ttrain-mlogloss:0.973635\n",
            "[236]\ttrain-mlogloss:0.973061\n",
            "[237]\ttrain-mlogloss:0.972595\n",
            "[238]\ttrain-mlogloss:0.972142\n",
            "[239]\ttrain-mlogloss:0.971716\n",
            "[240]\ttrain-mlogloss:0.971272\n",
            "[241]\ttrain-mlogloss:0.970902\n",
            "[242]\ttrain-mlogloss:0.970478\n",
            "[243]\ttrain-mlogloss:0.970133\n",
            "[244]\ttrain-mlogloss:0.969644\n",
            "[245]\ttrain-mlogloss:0.969249\n",
            "[246]\ttrain-mlogloss:0.968904\n",
            "[247]\ttrain-mlogloss:0.968518\n",
            "[248]\ttrain-mlogloss:0.968046\n",
            "[249]\ttrain-mlogloss:0.967735\n",
            "[250]\ttrain-mlogloss:0.967276\n",
            "[251]\ttrain-mlogloss:0.966925\n",
            "[252]\ttrain-mlogloss:0.966459\n",
            "[253]\ttrain-mlogloss:0.966022\n",
            "[254]\ttrain-mlogloss:0.965712\n",
            "[255]\ttrain-mlogloss:0.965256\n",
            "[256]\ttrain-mlogloss:0.964742\n",
            "[257]\ttrain-mlogloss:0.964434\n",
            "[258]\ttrain-mlogloss:0.964103\n",
            "[259]\ttrain-mlogloss:0.963701\n",
            "[260]\ttrain-mlogloss:0.96323\n",
            "[261]\ttrain-mlogloss:0.962713\n",
            "[262]\ttrain-mlogloss:0.962313\n",
            "[263]\ttrain-mlogloss:0.961782\n",
            "[264]\ttrain-mlogloss:0.961414\n",
            "[265]\ttrain-mlogloss:0.960965\n",
            "[266]\ttrain-mlogloss:0.960528\n",
            "[267]\ttrain-mlogloss:0.960106\n",
            "[268]\ttrain-mlogloss:0.959648\n",
            "[269]\ttrain-mlogloss:0.959253\n",
            "[270]\ttrain-mlogloss:0.958787\n",
            "[271]\ttrain-mlogloss:0.958466\n",
            "[272]\ttrain-mlogloss:0.958093\n",
            "[273]\ttrain-mlogloss:0.957705\n",
            "[274]\ttrain-mlogloss:0.957277\n",
            "[275]\ttrain-mlogloss:0.956921\n",
            "[276]\ttrain-mlogloss:0.956559\n",
            "[277]\ttrain-mlogloss:0.956185\n",
            "[278]\ttrain-mlogloss:0.955728\n",
            "[279]\ttrain-mlogloss:0.955255\n",
            "[280]\ttrain-mlogloss:0.954752\n",
            "[281]\ttrain-mlogloss:0.954426\n",
            "[282]\ttrain-mlogloss:0.953929\n",
            "[283]\ttrain-mlogloss:0.953531\n",
            "[284]\ttrain-mlogloss:0.952971\n",
            "[285]\ttrain-mlogloss:0.952588\n",
            "[286]\ttrain-mlogloss:0.952167\n",
            "[287]\ttrain-mlogloss:0.951706\n",
            "[288]\ttrain-mlogloss:0.95125\n",
            "[289]\ttrain-mlogloss:0.950838\n",
            "[290]\ttrain-mlogloss:0.950444\n",
            "[291]\ttrain-mlogloss:0.950037\n",
            "[292]\ttrain-mlogloss:0.949641\n",
            "[293]\ttrain-mlogloss:0.949352\n",
            "[294]\ttrain-mlogloss:0.949073\n",
            "[295]\ttrain-mlogloss:0.948719\n",
            "[296]\ttrain-mlogloss:0.948269\n",
            "[297]\ttrain-mlogloss:0.947785\n",
            "[298]\ttrain-mlogloss:0.947408\n",
            "[299]\ttrain-mlogloss:0.947064\n",
            "[300]\ttrain-mlogloss:0.946672\n",
            "[301]\ttrain-mlogloss:0.946301\n",
            "[302]\ttrain-mlogloss:0.945913\n",
            "[303]\ttrain-mlogloss:0.945668\n",
            "[304]\ttrain-mlogloss:0.945159\n",
            "[305]\ttrain-mlogloss:0.944691\n",
            "[306]\ttrain-mlogloss:0.944265\n",
            "[307]\ttrain-mlogloss:0.943862\n",
            "[308]\ttrain-mlogloss:0.943488\n",
            "[309]\ttrain-mlogloss:0.943129\n",
            "[310]\ttrain-mlogloss:0.942767\n",
            "[311]\ttrain-mlogloss:0.942396\n",
            "[312]\ttrain-mlogloss:0.941919\n",
            "[313]\ttrain-mlogloss:0.941464\n",
            "[314]\ttrain-mlogloss:0.94104\n",
            "[315]\ttrain-mlogloss:0.940698\n",
            "[316]\ttrain-mlogloss:0.940329\n",
            "[317]\ttrain-mlogloss:0.939903\n",
            "[318]\ttrain-mlogloss:0.939515\n",
            "[319]\ttrain-mlogloss:0.939124\n",
            "[320]\ttrain-mlogloss:0.93877\n",
            "[321]\ttrain-mlogloss:0.938537\n",
            "[322]\ttrain-mlogloss:0.938135\n",
            "[323]\ttrain-mlogloss:0.937647\n",
            "[324]\ttrain-mlogloss:0.937335\n",
            "[325]\ttrain-mlogloss:0.937037\n",
            "[326]\ttrain-mlogloss:0.936566\n",
            "[327]\ttrain-mlogloss:0.936096\n",
            "[328]\ttrain-mlogloss:0.935726\n",
            "[329]\ttrain-mlogloss:0.935314\n",
            "[330]\ttrain-mlogloss:0.93499\n",
            "[331]\ttrain-mlogloss:0.934721\n",
            "[332]\ttrain-mlogloss:0.934381\n",
            "[333]\ttrain-mlogloss:0.934107\n",
            "[334]\ttrain-mlogloss:0.933803\n",
            "[335]\ttrain-mlogloss:0.933542\n",
            "[336]\ttrain-mlogloss:0.933261\n",
            "[337]\ttrain-mlogloss:0.93301\n",
            "[338]\ttrain-mlogloss:0.932662\n",
            "[339]\ttrain-mlogloss:0.932372\n",
            "[340]\ttrain-mlogloss:0.932092\n",
            "[341]\ttrain-mlogloss:0.931629\n",
            "[342]\ttrain-mlogloss:0.931273\n",
            "[343]\ttrain-mlogloss:0.930901\n",
            "[344]\ttrain-mlogloss:0.930649\n",
            "[345]\ttrain-mlogloss:0.930236\n",
            "[346]\ttrain-mlogloss:0.929926\n",
            "[347]\ttrain-mlogloss:0.929527\n",
            "[348]\ttrain-mlogloss:0.929143\n",
            "[349]\ttrain-mlogloss:0.928809\n",
            "[350]\ttrain-mlogloss:0.928483\n",
            "[351]\ttrain-mlogloss:0.928119\n",
            "[352]\ttrain-mlogloss:0.927698\n",
            "[353]\ttrain-mlogloss:0.927344\n",
            "[354]\ttrain-mlogloss:0.927018\n",
            "[355]\ttrain-mlogloss:0.926737\n",
            "[356]\ttrain-mlogloss:0.926466\n",
            "[357]\ttrain-mlogloss:0.926067\n",
            "[358]\ttrain-mlogloss:0.92571\n",
            "[359]\ttrain-mlogloss:0.92537\n",
            "[360]\ttrain-mlogloss:0.925008\n",
            "[361]\ttrain-mlogloss:0.924721\n",
            "[362]\ttrain-mlogloss:0.924311\n",
            "[363]\ttrain-mlogloss:0.924051\n",
            "[364]\ttrain-mlogloss:0.923683\n",
            "[365]\ttrain-mlogloss:0.923394\n",
            "[366]\ttrain-mlogloss:0.923054\n",
            "[367]\ttrain-mlogloss:0.922835\n",
            "[368]\ttrain-mlogloss:0.922529\n",
            "[369]\ttrain-mlogloss:0.922143\n",
            "[370]\ttrain-mlogloss:0.921812\n",
            "[371]\ttrain-mlogloss:0.921432\n",
            "[372]\ttrain-mlogloss:0.921126\n",
            "[373]\ttrain-mlogloss:0.920707\n",
            "[374]\ttrain-mlogloss:0.920403\n",
            "[375]\ttrain-mlogloss:0.920051\n",
            "[376]\ttrain-mlogloss:0.919738\n",
            "[377]\ttrain-mlogloss:0.919484\n",
            "[378]\ttrain-mlogloss:0.919186\n",
            "[379]\ttrain-mlogloss:0.918829\n",
            "[380]\ttrain-mlogloss:0.918413\n",
            "[381]\ttrain-mlogloss:0.918036\n",
            "[382]\ttrain-mlogloss:0.917704\n",
            "[383]\ttrain-mlogloss:0.917315\n",
            "[384]\ttrain-mlogloss:0.917059\n",
            "[385]\ttrain-mlogloss:0.916691\n",
            "[386]\ttrain-mlogloss:0.916292\n",
            "[387]\ttrain-mlogloss:0.916005\n",
            "[388]\ttrain-mlogloss:0.915704\n",
            "[389]\ttrain-mlogloss:0.915253\n",
            "[390]\ttrain-mlogloss:0.914923\n",
            "[391]\ttrain-mlogloss:0.914563\n",
            "[392]\ttrain-mlogloss:0.914175\n",
            "[393]\ttrain-mlogloss:0.913977\n",
            "[394]\ttrain-mlogloss:0.913722\n",
            "[395]\ttrain-mlogloss:0.913382\n",
            "[396]\ttrain-mlogloss:0.91305\n",
            "[397]\ttrain-mlogloss:0.912671\n",
            "[398]\ttrain-mlogloss:0.912407\n",
            "[399]\ttrain-mlogloss:0.912094\n",
            "[400]\ttrain-mlogloss:0.911778\n",
            "[401]\ttrain-mlogloss:0.91146\n",
            "[402]\ttrain-mlogloss:0.911125\n",
            "[403]\ttrain-mlogloss:0.910919\n",
            "[404]\ttrain-mlogloss:0.910604\n",
            "[405]\ttrain-mlogloss:0.910326\n",
            "[406]\ttrain-mlogloss:0.910014\n",
            "[407]\ttrain-mlogloss:0.909685\n",
            "[408]\ttrain-mlogloss:0.909323\n",
            "[409]\ttrain-mlogloss:0.908996\n",
            "[410]\ttrain-mlogloss:0.908729\n",
            "[411]\ttrain-mlogloss:0.908431\n",
            "[412]\ttrain-mlogloss:0.908157\n",
            "[413]\ttrain-mlogloss:0.907878\n",
            "[414]\ttrain-mlogloss:0.907604\n",
            "[415]\ttrain-mlogloss:0.907189\n",
            "[416]\ttrain-mlogloss:0.906952\n",
            "[417]\ttrain-mlogloss:0.906634\n",
            "[418]\ttrain-mlogloss:0.906274\n",
            "[419]\ttrain-mlogloss:0.90591\n",
            "[420]\ttrain-mlogloss:0.905649\n",
            "[421]\ttrain-mlogloss:0.90533\n",
            "[422]\ttrain-mlogloss:0.905015\n",
            "[423]\ttrain-mlogloss:0.904727\n",
            "[424]\ttrain-mlogloss:0.904378\n",
            "[425]\ttrain-mlogloss:0.904132\n",
            "[426]\ttrain-mlogloss:0.903822\n",
            "[427]\ttrain-mlogloss:0.903417\n",
            "[428]\ttrain-mlogloss:0.903134\n",
            "[429]\ttrain-mlogloss:0.902916\n",
            "[430]\ttrain-mlogloss:0.902675\n",
            "[431]\ttrain-mlogloss:0.902333\n",
            "[432]\ttrain-mlogloss:0.901945\n",
            "[433]\ttrain-mlogloss:0.901606\n",
            "[434]\ttrain-mlogloss:0.901351\n",
            "[435]\ttrain-mlogloss:0.901156\n",
            "[436]\ttrain-mlogloss:0.900852\n",
            "[437]\ttrain-mlogloss:0.900548\n",
            "[438]\ttrain-mlogloss:0.900214\n",
            "[439]\ttrain-mlogloss:0.899904\n",
            "[440]\ttrain-mlogloss:0.899656\n",
            "[441]\ttrain-mlogloss:0.899439\n",
            "[442]\ttrain-mlogloss:0.899166\n",
            "[443]\ttrain-mlogloss:0.89896\n",
            "[444]\ttrain-mlogloss:0.898668\n",
            "[445]\ttrain-mlogloss:0.898428\n",
            "[446]\ttrain-mlogloss:0.898107\n",
            "[447]\ttrain-mlogloss:0.897816\n",
            "[448]\ttrain-mlogloss:0.897537\n",
            "[449]\ttrain-mlogloss:0.897263\n",
            "[450]\ttrain-mlogloss:0.896955\n",
            "[451]\ttrain-mlogloss:0.89658\n",
            "[452]\ttrain-mlogloss:0.896289\n",
            "[453]\ttrain-mlogloss:0.895987\n",
            "[454]\ttrain-mlogloss:0.895647\n",
            "[455]\ttrain-mlogloss:0.895446\n",
            "[456]\ttrain-mlogloss:0.895143\n",
            "[457]\ttrain-mlogloss:0.894929\n",
            "[458]\ttrain-mlogloss:0.894612\n",
            "[459]\ttrain-mlogloss:0.89429\n",
            "[460]\ttrain-mlogloss:0.89402\n",
            "[461]\ttrain-mlogloss:0.893676\n",
            "[462]\ttrain-mlogloss:0.893413\n",
            "[463]\ttrain-mlogloss:0.893137\n",
            "[464]\ttrain-mlogloss:0.892798\n",
            "[465]\ttrain-mlogloss:0.892527\n",
            "[466]\ttrain-mlogloss:0.892286\n",
            "[467]\ttrain-mlogloss:0.891997\n",
            "[468]\ttrain-mlogloss:0.891728\n",
            "[469]\ttrain-mlogloss:0.891504\n",
            "[470]\ttrain-mlogloss:0.891208\n",
            "[471]\ttrain-mlogloss:0.891003\n",
            "[472]\ttrain-mlogloss:0.890743\n",
            "[473]\ttrain-mlogloss:0.890563\n",
            "[474]\ttrain-mlogloss:0.890364\n",
            "[475]\ttrain-mlogloss:0.890111\n",
            "[476]\ttrain-mlogloss:0.889903\n",
            "[477]\ttrain-mlogloss:0.88965\n",
            "[478]\ttrain-mlogloss:0.889456\n",
            "[479]\ttrain-mlogloss:0.889172\n",
            "[480]\ttrain-mlogloss:0.888886\n",
            "[481]\ttrain-mlogloss:0.888624\n",
            "[482]\ttrain-mlogloss:0.888373\n",
            "[483]\ttrain-mlogloss:0.888119\n",
            "[484]\ttrain-mlogloss:0.887799\n",
            "[485]\ttrain-mlogloss:0.887558\n",
            "[486]\ttrain-mlogloss:0.88731\n",
            "[487]\ttrain-mlogloss:0.887074\n",
            "[488]\ttrain-mlogloss:0.886809\n",
            "[489]\ttrain-mlogloss:0.886442\n",
            "[490]\ttrain-mlogloss:0.886148\n",
            "[491]\ttrain-mlogloss:0.885842\n",
            "[492]\ttrain-mlogloss:0.885555\n",
            "[493]\ttrain-mlogloss:0.88531\n",
            "[494]\ttrain-mlogloss:0.885061\n",
            "[495]\ttrain-mlogloss:0.884759\n",
            "[496]\ttrain-mlogloss:0.884525\n",
            "[497]\ttrain-mlogloss:0.884292\n",
            "[498]\ttrain-mlogloss:0.884066\n",
            "[499]\ttrain-mlogloss:0.883752\n",
            "[500]\ttrain-mlogloss:0.883306\n",
            "[501]\ttrain-mlogloss:0.883031\n",
            "[502]\ttrain-mlogloss:0.882769\n",
            "[503]\ttrain-mlogloss:0.88254\n",
            "[504]\ttrain-mlogloss:0.882296\n",
            "[505]\ttrain-mlogloss:0.882027\n",
            "[506]\ttrain-mlogloss:0.881776\n",
            "[507]\ttrain-mlogloss:0.881447\n",
            "[508]\ttrain-mlogloss:0.881099\n",
            "[509]\ttrain-mlogloss:0.880859\n",
            "[510]\ttrain-mlogloss:0.880579\n",
            "[511]\ttrain-mlogloss:0.880284\n",
            "[512]\ttrain-mlogloss:0.879965\n",
            "[513]\ttrain-mlogloss:0.879596\n",
            "[514]\ttrain-mlogloss:0.879299\n",
            "[515]\ttrain-mlogloss:0.879053\n",
            "[516]\ttrain-mlogloss:0.878767\n",
            "[517]\ttrain-mlogloss:0.878477\n",
            "[518]\ttrain-mlogloss:0.878176\n",
            "[519]\ttrain-mlogloss:0.877936\n",
            "[520]\ttrain-mlogloss:0.877681\n",
            "[521]\ttrain-mlogloss:0.877375\n",
            "[522]\ttrain-mlogloss:0.877001\n",
            "[523]\ttrain-mlogloss:0.876709\n",
            "[524]\ttrain-mlogloss:0.876444\n",
            "[525]\ttrain-mlogloss:0.876171\n",
            "[526]\ttrain-mlogloss:0.875862\n",
            "[527]\ttrain-mlogloss:0.875618\n",
            "[528]\ttrain-mlogloss:0.875313\n",
            "[529]\ttrain-mlogloss:0.875016\n",
            "[530]\ttrain-mlogloss:0.874812\n",
            "[531]\ttrain-mlogloss:0.874589\n",
            "[532]\ttrain-mlogloss:0.874259\n",
            "[533]\ttrain-mlogloss:0.873994\n",
            "[534]\ttrain-mlogloss:0.873729\n",
            "[535]\ttrain-mlogloss:0.873458\n",
            "[536]\ttrain-mlogloss:0.873181\n",
            "[537]\ttrain-mlogloss:0.872878\n",
            "[538]\ttrain-mlogloss:0.87268\n",
            "[539]\ttrain-mlogloss:0.872405\n",
            "[540]\ttrain-mlogloss:0.87204\n",
            "[541]\ttrain-mlogloss:0.871846\n",
            "[542]\ttrain-mlogloss:0.871566\n",
            "[543]\ttrain-mlogloss:0.871327\n",
            "[544]\ttrain-mlogloss:0.87107\n",
            "[545]\ttrain-mlogloss:0.870786\n",
            "[546]\ttrain-mlogloss:0.870521\n",
            "[547]\ttrain-mlogloss:0.870247\n",
            "[548]\ttrain-mlogloss:0.869925\n",
            "[549]\ttrain-mlogloss:0.869694\n",
            "[550]\ttrain-mlogloss:0.869392\n",
            "[551]\ttrain-mlogloss:0.869138\n",
            "[552]\ttrain-mlogloss:0.868903\n",
            "[553]\ttrain-mlogloss:0.868639\n",
            "[554]\ttrain-mlogloss:0.868343\n",
            "[555]\ttrain-mlogloss:0.867994\n",
            "[556]\ttrain-mlogloss:0.867674\n",
            "[557]\ttrain-mlogloss:0.86751\n",
            "[558]\ttrain-mlogloss:0.867228\n",
            "[559]\ttrain-mlogloss:0.866915\n",
            "[560]\ttrain-mlogloss:0.866581\n",
            "[561]\ttrain-mlogloss:0.866262\n",
            "[562]\ttrain-mlogloss:0.865979\n",
            "[563]\ttrain-mlogloss:0.86567\n",
            "[564]\ttrain-mlogloss:0.865451\n",
            "[565]\ttrain-mlogloss:0.865181\n",
            "[566]\ttrain-mlogloss:0.864906\n",
            "[567]\ttrain-mlogloss:0.864597\n",
            "[568]\ttrain-mlogloss:0.864295\n",
            "[569]\ttrain-mlogloss:0.864067\n",
            "[570]\ttrain-mlogloss:0.863845\n",
            "[571]\ttrain-mlogloss:0.863579\n",
            "[572]\ttrain-mlogloss:0.863232\n",
            "[573]\ttrain-mlogloss:0.862976\n",
            "[574]\ttrain-mlogloss:0.862687\n",
            "[575]\ttrain-mlogloss:0.862422\n",
            "[576]\ttrain-mlogloss:0.862169\n",
            "[577]\ttrain-mlogloss:0.861859\n",
            "[578]\ttrain-mlogloss:0.861573\n",
            "[579]\ttrain-mlogloss:0.861247\n",
            "[580]\ttrain-mlogloss:0.861063\n",
            "[581]\ttrain-mlogloss:0.86077\n",
            "[582]\ttrain-mlogloss:0.860503\n",
            "[583]\ttrain-mlogloss:0.860232\n",
            "[584]\ttrain-mlogloss:0.859962\n",
            "[585]\ttrain-mlogloss:0.859753\n",
            "[586]\ttrain-mlogloss:0.859455\n",
            "[587]\ttrain-mlogloss:0.859187\n",
            "[588]\ttrain-mlogloss:0.85897\n",
            "[589]\ttrain-mlogloss:0.858717\n",
            "[590]\ttrain-mlogloss:0.858469\n",
            "[591]\ttrain-mlogloss:0.858214\n",
            "[592]\ttrain-mlogloss:0.858009\n",
            "[593]\ttrain-mlogloss:0.857773\n",
            "[594]\ttrain-mlogloss:0.857494\n",
            "[595]\ttrain-mlogloss:0.85722\n",
            "[596]\ttrain-mlogloss:0.856994\n",
            "[597]\ttrain-mlogloss:0.856731\n",
            "[598]\ttrain-mlogloss:0.856527\n",
            "[599]\ttrain-mlogloss:0.856299\n",
            "[600]\ttrain-mlogloss:0.856068\n",
            "[601]\ttrain-mlogloss:0.8558\n",
            "[602]\ttrain-mlogloss:0.855566\n",
            "[603]\ttrain-mlogloss:0.855281\n",
            "[604]\ttrain-mlogloss:0.855054\n",
            "[605]\ttrain-mlogloss:0.854702\n",
            "[606]\ttrain-mlogloss:0.854376\n",
            "[607]\ttrain-mlogloss:0.854121\n",
            "[608]\ttrain-mlogloss:0.853756\n",
            "[609]\ttrain-mlogloss:0.853477\n",
            "[610]\ttrain-mlogloss:0.853245\n",
            "[611]\ttrain-mlogloss:0.852954\n",
            "[612]\ttrain-mlogloss:0.852724\n",
            "[613]\ttrain-mlogloss:0.852475\n",
            "[614]\ttrain-mlogloss:0.852221\n",
            "[615]\ttrain-mlogloss:0.851993\n",
            "[616]\ttrain-mlogloss:0.851718\n",
            "[617]\ttrain-mlogloss:0.851505\n",
            "[618]\ttrain-mlogloss:0.851309\n",
            "[619]\ttrain-mlogloss:0.851102\n",
            "[620]\ttrain-mlogloss:0.850747\n",
            "[621]\ttrain-mlogloss:0.850486\n",
            "[622]\ttrain-mlogloss:0.850216\n",
            "[623]\ttrain-mlogloss:0.850004\n",
            "[624]\ttrain-mlogloss:0.849775\n",
            "[625]\ttrain-mlogloss:0.84954\n",
            "[626]\ttrain-mlogloss:0.849275\n",
            "[627]\ttrain-mlogloss:0.849071\n",
            "[628]\ttrain-mlogloss:0.848891\n",
            "[629]\ttrain-mlogloss:0.84863\n",
            "[630]\ttrain-mlogloss:0.848367\n",
            "[631]\ttrain-mlogloss:0.848118\n",
            "[632]\ttrain-mlogloss:0.847858\n",
            "[633]\ttrain-mlogloss:0.847583\n",
            "[634]\ttrain-mlogloss:0.84729\n",
            "[635]\ttrain-mlogloss:0.847147\n",
            "[636]\ttrain-mlogloss:0.846884\n",
            "[637]\ttrain-mlogloss:0.846661\n",
            "[638]\ttrain-mlogloss:0.846469\n",
            "[639]\ttrain-mlogloss:0.846205\n",
            "[640]\ttrain-mlogloss:0.846016\n",
            "[641]\ttrain-mlogloss:0.845848\n",
            "[642]\ttrain-mlogloss:0.845657\n",
            "[643]\ttrain-mlogloss:0.845406\n",
            "[644]\ttrain-mlogloss:0.845192\n",
            "[645]\ttrain-mlogloss:0.844981\n",
            "[646]\ttrain-mlogloss:0.844829\n",
            "[647]\ttrain-mlogloss:0.844618\n",
            "[648]\ttrain-mlogloss:0.844369\n",
            "[649]\ttrain-mlogloss:0.844178\n",
            "[650]\ttrain-mlogloss:0.843982\n",
            "[651]\ttrain-mlogloss:0.843778\n",
            "[652]\ttrain-mlogloss:0.843515\n",
            "[653]\ttrain-mlogloss:0.843297\n",
            "[654]\ttrain-mlogloss:0.842967\n",
            "[655]\ttrain-mlogloss:0.842769\n",
            "[656]\ttrain-mlogloss:0.842524\n",
            "[657]\ttrain-mlogloss:0.842289\n",
            "[658]\ttrain-mlogloss:0.842034\n",
            "[659]\ttrain-mlogloss:0.841811\n",
            "[660]\ttrain-mlogloss:0.841518\n",
            "[661]\ttrain-mlogloss:0.84123\n",
            "[662]\ttrain-mlogloss:0.840962\n",
            "[663]\ttrain-mlogloss:0.840715\n",
            "[664]\ttrain-mlogloss:0.840457\n",
            "[665]\ttrain-mlogloss:0.840243\n",
            "[666]\ttrain-mlogloss:0.84\n",
            "[667]\ttrain-mlogloss:0.839748\n",
            "[668]\ttrain-mlogloss:0.839472\n",
            "[669]\ttrain-mlogloss:0.83915\n",
            "[670]\ttrain-mlogloss:0.838979\n",
            "[671]\ttrain-mlogloss:0.838815\n",
            "[672]\ttrain-mlogloss:0.838553\n",
            "[673]\ttrain-mlogloss:0.838299\n",
            "[674]\ttrain-mlogloss:0.838121\n",
            "[675]\ttrain-mlogloss:0.837822\n",
            "[676]\ttrain-mlogloss:0.837642\n",
            "[677]\ttrain-mlogloss:0.83733\n",
            "[678]\ttrain-mlogloss:0.837037\n",
            "[679]\ttrain-mlogloss:0.836784\n",
            "[680]\ttrain-mlogloss:0.836499\n",
            "[681]\ttrain-mlogloss:0.836341\n",
            "[682]\ttrain-mlogloss:0.836116\n",
            "[683]\ttrain-mlogloss:0.835913\n",
            "[684]\ttrain-mlogloss:0.835654\n",
            "[685]\ttrain-mlogloss:0.835426\n",
            "[686]\ttrain-mlogloss:0.835192\n",
            "[687]\ttrain-mlogloss:0.835025\n",
            "[688]\ttrain-mlogloss:0.834783\n",
            "[689]\ttrain-mlogloss:0.834597\n",
            "[690]\ttrain-mlogloss:0.834387\n",
            "[691]\ttrain-mlogloss:0.834143\n",
            "[692]\ttrain-mlogloss:0.833896\n",
            "[693]\ttrain-mlogloss:0.83371\n",
            "[694]\ttrain-mlogloss:0.833468\n",
            "[695]\ttrain-mlogloss:0.833275\n",
            "[696]\ttrain-mlogloss:0.833086\n",
            "[697]\ttrain-mlogloss:0.832872\n",
            "[698]\ttrain-mlogloss:0.832701\n",
            "[699]\ttrain-mlogloss:0.832445\n",
            "[700]\ttrain-mlogloss:0.832296\n",
            "[701]\ttrain-mlogloss:0.832048\n",
            "[702]\ttrain-mlogloss:0.831843\n",
            "[703]\ttrain-mlogloss:0.831531\n",
            "[704]\ttrain-mlogloss:0.831325\n",
            "[705]\ttrain-mlogloss:0.831134\n",
            "[706]\ttrain-mlogloss:0.830962\n",
            "[707]\ttrain-mlogloss:0.830713\n",
            "[708]\ttrain-mlogloss:0.830465\n",
            "[709]\ttrain-mlogloss:0.830219\n",
            "[710]\ttrain-mlogloss:0.830069\n",
            "[711]\ttrain-mlogloss:0.829911\n",
            "[712]\ttrain-mlogloss:0.829655\n",
            "[713]\ttrain-mlogloss:0.829408\n",
            "[714]\ttrain-mlogloss:0.829199\n",
            "[715]\ttrain-mlogloss:0.82902\n",
            "[716]\ttrain-mlogloss:0.828778\n",
            "[717]\ttrain-mlogloss:0.828551\n",
            "[718]\ttrain-mlogloss:0.828404\n",
            "[719]\ttrain-mlogloss:0.828188\n",
            "[720]\ttrain-mlogloss:0.827983\n",
            "[721]\ttrain-mlogloss:0.827769\n",
            "[722]\ttrain-mlogloss:0.827506\n",
            "[723]\ttrain-mlogloss:0.827304\n",
            "[724]\ttrain-mlogloss:0.827123\n",
            "[725]\ttrain-mlogloss:0.826927\n",
            "[726]\ttrain-mlogloss:0.826724\n",
            "[727]\ttrain-mlogloss:0.826463\n",
            "[728]\ttrain-mlogloss:0.826259\n",
            "[729]\ttrain-mlogloss:0.82601\n",
            "[730]\ttrain-mlogloss:0.825767\n",
            "[731]\ttrain-mlogloss:0.825477\n",
            "[732]\ttrain-mlogloss:0.825277\n",
            "[733]\ttrain-mlogloss:0.82509\n",
            "[734]\ttrain-mlogloss:0.824856\n",
            "[735]\ttrain-mlogloss:0.824646\n",
            "[736]\ttrain-mlogloss:0.824476\n",
            "[737]\ttrain-mlogloss:0.824303\n",
            "[738]\ttrain-mlogloss:0.82405\n",
            "[739]\ttrain-mlogloss:0.82377\n",
            "[740]\ttrain-mlogloss:0.82352\n",
            "[741]\ttrain-mlogloss:0.823294\n",
            "[742]\ttrain-mlogloss:0.823009\n",
            "[743]\ttrain-mlogloss:0.822796\n",
            "[744]\ttrain-mlogloss:0.822531\n",
            "[745]\ttrain-mlogloss:0.822336\n",
            "[746]\ttrain-mlogloss:0.822063\n",
            "[747]\ttrain-mlogloss:0.82187\n",
            "[748]\ttrain-mlogloss:0.821693\n",
            "[749]\ttrain-mlogloss:0.821465\n",
            "[750]\ttrain-mlogloss:0.821265\n",
            "[751]\ttrain-mlogloss:0.821076\n",
            "[752]\ttrain-mlogloss:0.820846\n",
            "[753]\ttrain-mlogloss:0.820612\n",
            "[754]\ttrain-mlogloss:0.820422\n",
            "[755]\ttrain-mlogloss:0.820147\n",
            "[756]\ttrain-mlogloss:0.819897\n",
            "[757]\ttrain-mlogloss:0.81969\n",
            "[758]\ttrain-mlogloss:0.819515\n",
            "[759]\ttrain-mlogloss:0.819287\n",
            "[760]\ttrain-mlogloss:0.819064\n",
            "[761]\ttrain-mlogloss:0.818914\n",
            "[762]\ttrain-mlogloss:0.818699\n",
            "[763]\ttrain-mlogloss:0.818393\n",
            "[764]\ttrain-mlogloss:0.818261\n",
            "[765]\ttrain-mlogloss:0.818013\n",
            "[766]\ttrain-mlogloss:0.817766\n",
            "[767]\ttrain-mlogloss:0.817523\n",
            "[768]\ttrain-mlogloss:0.81731\n",
            "[769]\ttrain-mlogloss:0.817157\n",
            "[770]\ttrain-mlogloss:0.816948\n",
            "[771]\ttrain-mlogloss:0.816711\n",
            "[772]\ttrain-mlogloss:0.816534\n",
            "[773]\ttrain-mlogloss:0.816296\n",
            "[774]\ttrain-mlogloss:0.816104\n",
            "[775]\ttrain-mlogloss:0.81591\n",
            "[776]\ttrain-mlogloss:0.815707\n",
            "[777]\ttrain-mlogloss:0.815456\n",
            "[778]\ttrain-mlogloss:0.815269\n",
            "[779]\ttrain-mlogloss:0.815039\n",
            "[780]\ttrain-mlogloss:0.814833\n",
            "[781]\ttrain-mlogloss:0.814635\n",
            "[782]\ttrain-mlogloss:0.814443\n",
            "[783]\ttrain-mlogloss:0.814167\n",
            "[784]\ttrain-mlogloss:0.813936\n",
            "[785]\ttrain-mlogloss:0.81375\n",
            "[786]\ttrain-mlogloss:0.813556\n",
            "[787]\ttrain-mlogloss:0.813378\n",
            "[788]\ttrain-mlogloss:0.813195\n",
            "[789]\ttrain-mlogloss:0.81296\n",
            "[790]\ttrain-mlogloss:0.812766\n",
            "[791]\ttrain-mlogloss:0.812615\n",
            "[792]\ttrain-mlogloss:0.812372\n",
            "[793]\ttrain-mlogloss:0.812164\n",
            "[794]\ttrain-mlogloss:0.811951\n",
            "[795]\ttrain-mlogloss:0.811723\n",
            "[796]\ttrain-mlogloss:0.811557\n",
            "[797]\ttrain-mlogloss:0.81134\n",
            "[798]\ttrain-mlogloss:0.811126\n",
            "[799]\ttrain-mlogloss:0.810927\n",
            "[800]\ttrain-mlogloss:0.810732\n",
            "[801]\ttrain-mlogloss:0.810548\n",
            "[802]\ttrain-mlogloss:0.810391\n",
            "[803]\ttrain-mlogloss:0.810214\n",
            "[804]\ttrain-mlogloss:0.810012\n",
            "[805]\ttrain-mlogloss:0.809843\n",
            "[806]\ttrain-mlogloss:0.809682\n",
            "[807]\ttrain-mlogloss:0.80951\n",
            "[808]\ttrain-mlogloss:0.809226\n",
            "[809]\ttrain-mlogloss:0.809009\n",
            "[810]\ttrain-mlogloss:0.808803\n",
            "[811]\ttrain-mlogloss:0.808592\n",
            "[812]\ttrain-mlogloss:0.808348\n",
            "[813]\ttrain-mlogloss:0.80815\n",
            "[814]\ttrain-mlogloss:0.807874\n",
            "[815]\ttrain-mlogloss:0.807667\n",
            "[816]\ttrain-mlogloss:0.807482\n",
            "[817]\ttrain-mlogloss:0.807253\n",
            "[818]\ttrain-mlogloss:0.807038\n",
            "[819]\ttrain-mlogloss:0.806881\n",
            "[820]\ttrain-mlogloss:0.806702\n",
            "[821]\ttrain-mlogloss:0.80655\n",
            "[822]\ttrain-mlogloss:0.806342\n",
            "[823]\ttrain-mlogloss:0.806094\n",
            "[824]\ttrain-mlogloss:0.805868\n",
            "[825]\ttrain-mlogloss:0.805719\n",
            "[826]\ttrain-mlogloss:0.805511\n",
            "[827]\ttrain-mlogloss:0.805329\n",
            "[828]\ttrain-mlogloss:0.805099\n",
            "[829]\ttrain-mlogloss:0.804919\n",
            "[830]\ttrain-mlogloss:0.804734\n",
            "[831]\ttrain-mlogloss:0.804533\n",
            "[832]\ttrain-mlogloss:0.804365\n",
            "[833]\ttrain-mlogloss:0.804146\n",
            "[834]\ttrain-mlogloss:0.803998\n",
            "[835]\ttrain-mlogloss:0.803841\n",
            "[836]\ttrain-mlogloss:0.803579\n",
            "[837]\ttrain-mlogloss:0.803417\n",
            "[838]\ttrain-mlogloss:0.803219\n",
            "[839]\ttrain-mlogloss:0.803034\n",
            "[840]\ttrain-mlogloss:0.802844\n",
            "[841]\ttrain-mlogloss:0.802667\n",
            "[842]\ttrain-mlogloss:0.802415\n",
            "[843]\ttrain-mlogloss:0.802225\n",
            "[844]\ttrain-mlogloss:0.802011\n",
            "[845]\ttrain-mlogloss:0.801821\n",
            "[846]\ttrain-mlogloss:0.801609\n",
            "[847]\ttrain-mlogloss:0.801452\n",
            "[848]\ttrain-mlogloss:0.801225\n",
            "[849]\ttrain-mlogloss:0.801076\n",
            "[850]\ttrain-mlogloss:0.800902\n",
            "[851]\ttrain-mlogloss:0.800626\n",
            "[852]\ttrain-mlogloss:0.800448\n",
            "[853]\ttrain-mlogloss:0.800294\n",
            "[854]\ttrain-mlogloss:0.800059\n",
            "[855]\ttrain-mlogloss:0.79989\n",
            "[856]\ttrain-mlogloss:0.799679\n",
            "[857]\ttrain-mlogloss:0.799469\n",
            "[858]\ttrain-mlogloss:0.7993\n",
            "[859]\ttrain-mlogloss:0.799075\n",
            "[860]\ttrain-mlogloss:0.798831\n",
            "[861]\ttrain-mlogloss:0.798594\n",
            "[862]\ttrain-mlogloss:0.798328\n",
            "[863]\ttrain-mlogloss:0.798115\n",
            "[864]\ttrain-mlogloss:0.797851\n",
            "[865]\ttrain-mlogloss:0.797611\n",
            "[866]\ttrain-mlogloss:0.797356\n",
            "[867]\ttrain-mlogloss:0.797106\n",
            "[868]\ttrain-mlogloss:0.796869\n",
            "[869]\ttrain-mlogloss:0.796688\n",
            "[870]\ttrain-mlogloss:0.796555\n",
            "[871]\ttrain-mlogloss:0.796346\n",
            "[872]\ttrain-mlogloss:0.796216\n",
            "[873]\ttrain-mlogloss:0.796044\n",
            "[874]\ttrain-mlogloss:0.795869\n",
            "[875]\ttrain-mlogloss:0.79568\n",
            "[876]\ttrain-mlogloss:0.795493\n",
            "[877]\ttrain-mlogloss:0.795295\n",
            "[878]\ttrain-mlogloss:0.795085\n",
            "[879]\ttrain-mlogloss:0.794903\n",
            "[880]\ttrain-mlogloss:0.794743\n",
            "[881]\ttrain-mlogloss:0.794501\n",
            "[882]\ttrain-mlogloss:0.794359\n",
            "[883]\ttrain-mlogloss:0.794236\n",
            "[884]\ttrain-mlogloss:0.794031\n",
            "[885]\ttrain-mlogloss:0.793846\n",
            "[886]\ttrain-mlogloss:0.793721\n",
            "[887]\ttrain-mlogloss:0.793579\n",
            "[888]\ttrain-mlogloss:0.793447\n",
            "[889]\ttrain-mlogloss:0.79328\n",
            "[890]\ttrain-mlogloss:0.793131\n",
            "[891]\ttrain-mlogloss:0.793008\n",
            "[892]\ttrain-mlogloss:0.792811\n",
            "[893]\ttrain-mlogloss:0.792674\n",
            "[894]\ttrain-mlogloss:0.792513\n",
            "[895]\ttrain-mlogloss:0.79233\n",
            "[896]\ttrain-mlogloss:0.792147\n",
            "[897]\ttrain-mlogloss:0.791959\n",
            "[898]\ttrain-mlogloss:0.791748\n",
            "[899]\ttrain-mlogloss:0.791545\n",
            "[900]\ttrain-mlogloss:0.791372\n",
            "[901]\ttrain-mlogloss:0.79116\n",
            "[902]\ttrain-mlogloss:0.790977\n",
            "[903]\ttrain-mlogloss:0.790816\n",
            "[904]\ttrain-mlogloss:0.790614\n",
            "[905]\ttrain-mlogloss:0.790449\n",
            "[906]\ttrain-mlogloss:0.79029\n",
            "[907]\ttrain-mlogloss:0.790061\n",
            "[908]\ttrain-mlogloss:0.789863\n",
            "[909]\ttrain-mlogloss:0.789682\n",
            "[910]\ttrain-mlogloss:0.789485\n",
            "[911]\ttrain-mlogloss:0.789307\n",
            "[912]\ttrain-mlogloss:0.78913\n",
            "Feature importance:\n",
            "('renta', 82746)\n",
            "('age', 65355)\n",
            "('antiguedad', 64816)\n",
            "('antiguedad_prev', 48758)\n",
            "('age_prev', 47858)\n",
            "('fecha_alta_month', 42813)\n",
            "('nomprov', 38880)\n",
            "('renta_prev', 37337)\n",
            "('fecha_alta_year', 28974)\n",
            "('canal_entrada', 25904)\n",
            "('nomprov_prev', 22980)\n",
            "('canal_entrada_prev', 16626)\n",
            "('fecha_alta_month_prev', 16089)\n",
            "('sexo', 10820)\n",
            "('fecha_alta_year_prev', 10091)\n",
            "('ind_recibo_ult1_prev', 9619)\n",
            "('ind_ecue_fin_ult1_prev', 7923)\n",
            "('ind_cco_fin_ult1_prev', 7347)\n",
            "('ind_cno_fin_ult1_prev', 7301)\n",
            "('segmento', 6445)\n",
            "('segmento_prev', 5903)\n",
            "('ind_reca_fin_ult1_prev', 5808)\n",
            "('ind_tjcr_fin_ult1_prev', 5374)\n",
            "('ind_ctop_fin_ult1_prev', 4497)\n",
            "('sexo_prev', 4146)\n",
            "('ind_dela_fin_ult1_prev', 4030)\n",
            "('tiprel_1mes', 3612)\n",
            "('ind_valo_fin_ult1_prev', 3587)\n",
            "('ind_ctpp_fin_ult1_prev', 3397)\n",
            "('ind_nom_pens_ult1_prev', 3315)\n",
            "('tiprel_1mes_prev', 2974)\n",
            "('ind_nomina_ult1_prev', 2876)\n",
            "('ind_fond_fin_ult1_prev', 2395)\n",
            "('ind_actividad_cliente', 2248)\n",
            "('ind_actividad_cliente_prev', 2096)\n",
            "('indext', 2019)\n",
            "('ind_ctma_fin_ult1_prev', 1937)\n",
            "('ind_plan_fin_ult1_prev', 1644)\n",
            "('ind_nuevo', 1565)\n",
            "('ind_hip_fin_ult1_prev', 1361)\n",
            "('indext_prev', 1177)\n",
            "('ind_nuevo_prev', 1051)\n",
            "('pais_residencia', 892)\n",
            "('indrel_1mes_prev', 806)\n",
            "('ind_deco_fin_ult1_prev', 795)\n",
            "('ind_viv_fin_ult1_prev', 780)\n",
            "('indrel_1mes', 779)\n",
            "('pais_residencia_prev', 750)\n",
            "('ind_deme_fin_ult1_prev', 743)\n",
            "('ind_empleado', 484)\n",
            "('ind_empleado_prev', 446)\n",
            "('ind_pres_fin_ult1_prev', 431)\n",
            "('indrel', 347)\n",
            "('ind_cder_fin_ult1_prev', 301)\n",
            "('ind_ctju_fin_ult1_prev', 264)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "('ult_fec_cli_1t_month', 144)\n",
            "('indresi', 66)\n",
            "('indfall', 54)\n",
            "('indresi_prev', 37)\n",
            "('indfall_prev', 37)\n",
            "('conyuemp_prev', 34)\n",
            "('conyuemp', 30)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JugKcQJ7lgqJ",
        "colab_type": "code",
        "outputId": "005eaf92-fba7-457d-9fbe-dccd6ad464bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "#제출 파일을 생성\n",
        "submit_file = open(\"/content/drive/My Drive/CoLab/explorer_of_machine_learning/ch02_santander/modle/xgb.baseline.2020-02-05\", 'w')\n",
        "submit_file.write('ncodpers, added_products\\n')\n",
        "for ncodper, pred in zip(ncodpers_tst, preds_tst):\n",
        "  y_prods = [(y, p, ip) for y, p, ip in zip(pred, prods, range(len(prods)))]\n",
        "  y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
        "  submit_file.write('{},{}\\n'.format(int(ncodper), ' '.join(y_prods)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-496e683b0e40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0my_prods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0my_prods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0msubmit_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{},{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncodper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, tuple found"
          ]
        }
      ]
    }
  ]
}