{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOtYY2CHJ39lFmjESbX0JMC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimcaprio/lifeofkaggle/blob/master/lifeofkaggle/Baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wHJ6sFD2mls",
        "colab_type": "text"
      },
      "source": [
        "# Baseline 모델\n",
        "## tabular 데이터를 다루는 캐클의 파이프라인\n",
        "### 1. 데이터 전처리 -> 2. 피처엔지니어링 -> 3. 머신러닝 모델학습 -> 4. 테스트 데이터 예측 및 캐글 업로드\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwVGkqiC3Gb5",
        "colab_type": "text"
      },
      "source": [
        "1.   데이터 전처리\n",
        "\n",
        "*   제품 변수의 결측값을 0으로 대체. 제품 보유 여부에 대한 정보가 없으면, 해당 제품을 보유하지 않고 있지 않다고 가정.\n",
        "*   훈련 데이터와 테스트 데이터를 통합. 훈련 데이터와 테스트 데이터는 날짜변수(fecha_dato)로 쉽게 구분 가능. 동일한 24개의 고객 변수를 공유하고 있으며, 테스트 데이터에 없는 24개의 제품 변수는 0으로 채움.\n",
        "* 번주형, 수치형 데이터를 전처리. 변수형 데이터는 .factorize()통해 label encoding을 수행. 데이터 타입이 object로 표현되는 수치형 데이터에서는 .unique()를 통해 특이값들을 대체하거나 제거하고, 정수형 데이터로 변환.\n",
        "*   추후 모델 학습에 사용할 변수 이름을 features 리스트에 미리 담는다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuO1MEyA69ok",
        "colab_type": "code",
        "outputId": "685056a5-d8f0-48c7-aeac-88b874555ecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lVUCwgC7TtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/home/leo/workspace/kaggle/,kaggle/\" # put path for wherever you put it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkCA9mkb2l_7",
        "colab_type": "code",
        "outputId": "525fc712-758e-4daf-f49b-3b7ed63c2075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "\n",
        "np.random.seed(2018)\n",
        "\n",
        "# 데이터를 불러온다\n",
        "trn = pd.read_csv('/content/drive/My Drive/CoLab/explorer_of_machine_learning/ch02_santander/data/train_ver2.csv')\n",
        "tst = pd.read_csv('/content/drive/My Drive/CoLab/explorer_of_machine_learning/ch02_santander/data/test_ver2.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76du4a0Aqohx",
        "colab_type": "code",
        "outputId": "372af4cc-078b-4e4a-c88a-11c37c84b457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "## 데이터 전처리\n",
        "# 제품 변수를 별도로 저장\n",
        "prods = trn.columns[24:].tolist()\n",
        "\n",
        "# 제품 변수 결측값을 미리 0으로 대체\n",
        "trn[prods] = trn[prods].fillna(0.0).astype(np.int8)\n",
        "\n",
        "# 24개 제품 중 하나도 보유하지 않는 고객 데이터를 제거\n",
        "no_product = trn[prods].sum(axis=1) == 0\n",
        "# print(\"trn.len : \", len(trn))\n",
        "# print(\"no_product.len : \", len(no_product))\n",
        "\n",
        "print(\"no_prodcut.sum : \", no_product.sum())\n",
        "\n",
        "# print(\"before trn cols : \", trn.columns[:])\n",
        "trn = trn[~no_product]\n",
        "# print(\"after trn cols : \", trn.columns[:])\n",
        "\n",
        "# 훈련 데이터와 테스트 데이터를 통합. 테스트 데이터에 없는 제품 변수는 0으로 대체\n",
        "for col in trn.columns[24:]:\n",
        "  tst[col] = 0\n",
        "df = pd.concat([trn, tst], axis=0)\n",
        "\n",
        "# 학습에 사용할 변수를 담는 list\n",
        "features = []\n",
        "\n",
        "# 범주형 변수를 .factorize() 함수를 통해 label encoding 함(one-hot) -- 날짜 데이터는 제외 시켜야...\n",
        "# 그래서 모든 object type 변수를 fatorize 해서는 안됨... categorical_cols = [col for col in trn.columns[:24] if trn[col].dtype in ['O']]\n",
        "categorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp','canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\n",
        "\n",
        "for col in categorical_cols:\n",
        "  df[col], _ = df[col].factorize(na_sentinel=-99)\n",
        "features += categorical_cols\n",
        "\n",
        "# 수치형 변수의 특이값과 결측값을 -99로 대체하고, 정수형으로 변환\n",
        "df['age'].replace(' NA', -99, inplace=True)\n",
        "df['age'] = df['age'].astype(np.int8)\n",
        "\n",
        "\n",
        "df['antiguedad'].replace('     NA', -99, inplace=True)\n",
        "df['antiguedad'] = df['antiguedad'].astype(np.int8)\n",
        "\n",
        "df['renta'].replace('         NA', -99, inplace=True)\n",
        "df['renta'].fillna(-99, inplace=True)\n",
        "df['renta'] = df['renta'].astype(float).astype(np.int8)\n",
        "\n",
        "df['indrel_1mes'].replace('P', 5, inplace=True)\n",
        "df['indrel_1mes'].fillna(-99, inplace=True)\n",
        "df['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)\n",
        "\n",
        "#학습에 사용할 수치형 변수를 features에 추가\n",
        "#features +=  [col for col in trn.columns[:24] if trn[col].dtype in ['int64', 'float64']]\n",
        "features += ['age', 'antiguedad', 'renta', 'ind_nuevo', 'indrel', 'indrel_1mes', 'ind_actividad_cliente']\n",
        "print(features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no_prodcut.sum :  0\n",
            "['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento', 'age', 'antiguedad', 'renta', 'ind_nuevo', 'indrel', 'indrel_1mes', 'ind_actividad_cliente']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MeYTErP888",
        "colab_type": "text"
      },
      "source": [
        "2.   피처 엔지니어링\n",
        "*   모델 학습에 사용할 파생변수를 생성."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruovoQQ1eU5U",
        "colab_type": "code",
        "outputId": "5622968b-6585-4ed7-c6a9-3991dfb1fcf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "df['fecha_alta'].head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2015-01-12\n",
              "1    2012-08-10\n",
              "2    2012-08-10\n",
              "3    2012-08-10\n",
              "4    2012-08-10\n",
              "Name: fecha_alta, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjZdfeboP8UX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (피처 엔지니어링) 두 날짜 변수에서 연도와 월 정보를 추출\n",
        "df['fecha_alta_month'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
        "df['fecha_alta_year'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
        "features += ['fecha_alta_month', 'fecha_alta_year']\n",
        "\n",
        "df['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
        "df['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
        "features += ['ult_fec_cli_1t_month', 'ult_cli_1t_year']\n",
        "\n",
        "# 그 외 변수의 결측값은 모두 -99로 변환\n",
        "df.fillna(-99, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QR5zFM_XjM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (피처 엔지니어링) lag-1 데이터를 생성\n",
        "# # 날짜를 숫자로 변환하는 함수. 2015-01-28 은 1, 2016-06-28은 18로 변환\n",
        "def date_to_int(str_date):\n",
        "  Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]\n",
        "  int_date = (int(Y) - 2015) * 12 + int(M)\n",
        "  return int_date\n",
        "\n",
        "# 날짜를 숫자로 변환하여 int_date에 저장\n",
        "df['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)\n",
        "\n",
        "# 데이터를 복사하고, int_date 날짜에 1을 더하여 lag를 생성. 변수명에 _prev를 추가\n",
        "# 이렇게하면, row 데이터 기준 int_date의 현재월이 현재월+1로 바뀜.\n",
        "df_lag = df.copy()\n",
        "df_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns]\n",
        "df_lag['int_date'] += 1\n",
        "\n",
        "# 원본 데이터와 lag 데이터를 ncodper와 int_date 기준으로 합침. lag 데이터의 int_date는 1이 밀려 있기 때문에, 저번 달의 제품 정보가 삽입.\n",
        "# 예를 들어 df의 int_date 10은 df_lag의 int_date 10과 merge됨. df_lag의 int_date 10은 df의 int_date 9와 같은 row 데이터.\n",
        "## 따라서, df에 int_date로 left로 조인되는 df_lag 데이터는 1개월 이전 데이터가 되는 것임.\n",
        "df_trn = df.merge(df_lag, on=['ncodpers', 'int_date'], how='left')\n",
        "\n",
        "# 메모리 효율을 위해 불필요한 변수르 메모리에서 제거.\n",
        "#del df, df_lag\n",
        "\n",
        "# 저번달의 제품 정보가 존재하지 않을 경우를 대비하여 0으로 대체\n",
        "for prod in prods:\n",
        "  prev = prod + '_prev'\n",
        "  df_trn[prev].fillna(0, inplace=True)\n",
        "df_trn.fillna(-99, inplace=True)\n",
        "\n",
        "# lag-1 변수를 추가\n",
        "features += [feature + '_prev' for feature in features]\n",
        "features += [prod + '_prev' for prod in prods]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttt1nxKUZXlT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "3.   교차검증\n",
        "\n",
        "\n",
        "*   테스트 데이터 제외하고 주어진 train 데이터를 활용하여 교차검증 데이터로 활용\n",
        "*   시계열 데이터의 경우, 테스트 데이터 중 마지막 날짜 또는 년, 월 을 교차검증 데이터로 분리하여 활용하는 것이 일반적. <- 미래를 예측하는 모델이기 때문\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQKAez1jaApF",
        "colab_type": "code",
        "outputId": "16538072-da90-43d8-e8b3-ed8df33954e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "## 모델 학습\n",
        "# 학습을 위하여 데이터를 훈련, 테스트용으로 분리\n",
        "# 학습에는 2016-01-28 ~ 2016-04-28 epdlxjaks tkdyd, 검증에는 2016-05-28 epdlxjfmf tkdyd.\n",
        "use_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
        "trn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\n",
        "tst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\n",
        "#del df_trn\n",
        "\n",
        "print(\"tst\", tst)\n",
        "\n",
        "# 훈련 데이터에서 신규 구매 건수만 추출\n",
        "# 변수를 아래와 같이 선언한 이유는, xgboost 모델 파라미터가 다음과 같이 정의\n",
        "# xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n",
        "X = []\n",
        "Y = []\n",
        "for i, prod in enumerate(prods):\n",
        "  prev = prod + '_prev'\n",
        "  prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]  # <- 신규 구매가 일어나 row만 선택\n",
        "  prY = np.zeros(prX.shape[0], dtype=np.int8) + i # <- 신규 구매가 일어나 column의 index를 prY에 담음.\n",
        "  X.append(prX) #신규 구매가 일어난 row만 추가.\n",
        "  Y.append(prY) #신규 구매가 일어나 컬럼의 index를 추가. 결과적으로 prev 와 현재 데이터의 feature 차이로 부터 변경되는(신규 구매되는) column의 index를 label데이터로 활용.\n",
        "XY = pd.concat(X)\n",
        "Y = np.hstack(Y)\n",
        "XY['y'] = Y\n",
        "\n",
        "## 결과적으로 신규 구매가 일어난 row들만 학습 대상이 됨. \n",
        "## 신규 구매가 일어나지 않은 row는 라벨 데이터 변경이 없기 때문에, feature들의 변경 내용이 의미가 없다????\n",
        "\n",
        "#훈련, 검증 데이터로 분리\n",
        "vld_date = '2016-05-28'\n",
        "XY_trn = XY[XY['fecha_dato'] != vld_date]\n",
        "XY_vld = XY[XY['fecha_dato'] == vld_date]\n",
        "\n",
        "print(\"XY_trn\", XY_trn)\n",
        "print(\"XY_vld\", XY_vld)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-61b5d44370ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0muse_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'2016-01-28'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2016-02-28'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2016-03-28'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2016-04-28'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2016-05-28'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_trn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_trn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fecha_dato'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_dates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_trn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_trn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fecha_dato'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'2016-06-28'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#del df_trn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_trn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUToWvAJwBni",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   xgboost 모델을 사용 하여 학습 모델 만들기\n",
        "2.   주요 파라미터\n",
        "*   max_depth : 트리 모델의 최대 깊이를 의미. 값이 높을 수록 더 복잡한 트리를 만들지만, 과적합의 원인이 될수 있음.\n",
        "*   eta : 딥러닝에서의 learning rate 의미. 0과 1사이의 값을 가지며, 값이 너무 높으면 학습이 잘 안되고, 낮으면 학습에 시간이 오래 걸림.\n",
        "*   colsample_bytree : 트리를 생성할 때 룬련 데이터에서 변수를 샘플링해주는 비율. 마찬가지로 과적합을 막아주는 방안. (딥러닝의 drop out 개념?) 보통 0.6 ~ 0.9\n",
        "*   colsample_bylevel : 트리의 레벨 별로 훈련 데이터의 변수를 샘플링해주는 비율. 보통 0.6 ~ 0.9\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XWYHmwEwBBl",
        "colab_type": "code",
        "outputId": "9661e49b-69a1-4ff9-b3de-d01b8b25493c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## XGBoost 모델 parameter를 설정\n",
        "param = {\n",
        "  'booster': 'gbtree',\n",
        "  'max_depth' : 8,\n",
        "  'nthread' : 4,\n",
        "  'num_class' : len(prods),\n",
        "  'objective' : 'multi:softprob',\n",
        "  'silent' : 1,\n",
        "  'eval_metric' : 'mlogloss',\n",
        "  'eta' : 0.1,\n",
        "  'min_child_weight' : 10,\n",
        "  'colsample_bytree' : 0.8,\n",
        "  'colsample_bylevel' : 0.9,\n",
        "  'seed' : 2018,\n",
        "}\n",
        "\n",
        "# 훈련, 검증 데이터를 XGBoost 형태로 변환\n",
        "X_trn = XY_trn.as_matrix(columns=features)\n",
        "Y_trn = XY_trn.as_matrix(columns=['y'])\n",
        "dtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n",
        "\n",
        "X_vld = XY_vld.as_matrix(columns=features)\n",
        "Y_vld = XY_vld.as_matrix(columns=['y'])\n",
        "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
        "\n",
        "print(Y_vld)\n",
        "\n",
        "# XBBoost 모델을 훈련 데이터로 학습\n",
        "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
        "model = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)\n",
        "\n",
        "import pickle\n",
        "pickle.dump(model, open('/content/drive/My Drive/CoLab/explorer_of_machine_learning/ch02_santander/modle/xgb.baseline.pkl', 'wb'))\n",
        "best_ntree_limit = model.best_ntree_limit"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 0]\n",
            " [ 2]\n",
            " [ 2]\n",
            " ...\n",
            " [23]\n",
            " [23]\n",
            " [23]]\n",
            "[0]\ttrain-mlogloss:2.6752\teval-mlogloss:2.68357\n",
            "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until eval-mlogloss hasn't improved in 20 rounds.\n",
            "[1]\ttrain-mlogloss:2.43959\teval-mlogloss:2.4521\n",
            "[2]\ttrain-mlogloss:2.26071\teval-mlogloss:2.27549\n",
            "[3]\ttrain-mlogloss:2.12697\teval-mlogloss:2.14326\n",
            "[4]\ttrain-mlogloss:2.01445\teval-mlogloss:2.03139\n",
            "[5]\ttrain-mlogloss:1.92002\teval-mlogloss:1.93765\n",
            "[6]\ttrain-mlogloss:1.84373\teval-mlogloss:1.86197\n",
            "[7]\ttrain-mlogloss:1.77481\teval-mlogloss:1.79374\n",
            "[8]\ttrain-mlogloss:1.71598\teval-mlogloss:1.73584\n",
            "[9]\ttrain-mlogloss:1.66264\teval-mlogloss:1.68283\n",
            "[10]\ttrain-mlogloss:1.61532\teval-mlogloss:1.63602\n",
            "[11]\ttrain-mlogloss:1.57299\teval-mlogloss:1.59415\n",
            "[12]\ttrain-mlogloss:1.53602\teval-mlogloss:1.55785\n",
            "[13]\ttrain-mlogloss:1.50103\teval-mlogloss:1.52306\n",
            "[14]\ttrain-mlogloss:1.46888\teval-mlogloss:1.49102\n",
            "[15]\ttrain-mlogloss:1.44007\teval-mlogloss:1.46248\n",
            "[16]\ttrain-mlogloss:1.41394\teval-mlogloss:1.43632\n",
            "[17]\ttrain-mlogloss:1.39043\teval-mlogloss:1.41319\n",
            "[18]\ttrain-mlogloss:1.36815\teval-mlogloss:1.39104\n",
            "[19]\ttrain-mlogloss:1.34773\teval-mlogloss:1.37044\n",
            "[20]\ttrain-mlogloss:1.32872\teval-mlogloss:1.35157\n",
            "[21]\ttrain-mlogloss:1.31158\teval-mlogloss:1.33487\n",
            "[22]\ttrain-mlogloss:1.29593\teval-mlogloss:1.31953\n",
            "[23]\ttrain-mlogloss:1.28171\teval-mlogloss:1.30565\n",
            "[24]\ttrain-mlogloss:1.26847\teval-mlogloss:1.29285\n",
            "[25]\ttrain-mlogloss:1.25602\teval-mlogloss:1.28095\n",
            "[26]\ttrain-mlogloss:1.24436\teval-mlogloss:1.26952\n",
            "[27]\ttrain-mlogloss:1.2333\teval-mlogloss:1.25857\n",
            "[28]\ttrain-mlogloss:1.223\teval-mlogloss:1.24837\n",
            "[29]\ttrain-mlogloss:1.21386\teval-mlogloss:1.23967\n",
            "[30]\ttrain-mlogloss:1.205\teval-mlogloss:1.23131\n",
            "[31]\ttrain-mlogloss:1.19658\teval-mlogloss:1.22304\n",
            "[32]\ttrain-mlogloss:1.18865\teval-mlogloss:1.21531\n",
            "[33]\ttrain-mlogloss:1.18135\teval-mlogloss:1.20845\n",
            "[34]\ttrain-mlogloss:1.17431\teval-mlogloss:1.20194\n",
            "[35]\ttrain-mlogloss:1.16782\teval-mlogloss:1.19579\n",
            "[36]\ttrain-mlogloss:1.16179\teval-mlogloss:1.19015\n",
            "[37]\ttrain-mlogloss:1.15591\teval-mlogloss:1.18468\n",
            "[38]\ttrain-mlogloss:1.15047\teval-mlogloss:1.17958\n",
            "[39]\ttrain-mlogloss:1.14531\teval-mlogloss:1.17482\n",
            "[40]\ttrain-mlogloss:1.14045\teval-mlogloss:1.17039\n",
            "[41]\ttrain-mlogloss:1.13581\teval-mlogloss:1.16609\n",
            "[42]\ttrain-mlogloss:1.13144\teval-mlogloss:1.1621\n",
            "[43]\ttrain-mlogloss:1.12728\teval-mlogloss:1.15831\n",
            "[44]\ttrain-mlogloss:1.12327\teval-mlogloss:1.15472\n",
            "[45]\ttrain-mlogloss:1.11965\teval-mlogloss:1.15134\n",
            "[46]\ttrain-mlogloss:1.11597\teval-mlogloss:1.14812\n",
            "[47]\ttrain-mlogloss:1.11283\teval-mlogloss:1.14555\n",
            "[48]\ttrain-mlogloss:1.10974\teval-mlogloss:1.14285\n",
            "[49]\ttrain-mlogloss:1.10683\teval-mlogloss:1.14032\n",
            "[50]\ttrain-mlogloss:1.10393\teval-mlogloss:1.13775\n",
            "[51]\ttrain-mlogloss:1.10106\teval-mlogloss:1.13536\n",
            "[52]\ttrain-mlogloss:1.0985\teval-mlogloss:1.13326\n",
            "[53]\ttrain-mlogloss:1.09593\teval-mlogloss:1.13107\n",
            "[54]\ttrain-mlogloss:1.09349\teval-mlogloss:1.12895\n",
            "[55]\ttrain-mlogloss:1.09107\teval-mlogloss:1.12708\n",
            "[56]\ttrain-mlogloss:1.08876\teval-mlogloss:1.12535\n",
            "[57]\ttrain-mlogloss:1.08668\teval-mlogloss:1.12363\n",
            "[58]\ttrain-mlogloss:1.08461\teval-mlogloss:1.12191\n",
            "[59]\ttrain-mlogloss:1.08263\teval-mlogloss:1.12035\n",
            "[60]\ttrain-mlogloss:1.08083\teval-mlogloss:1.11895\n",
            "[61]\ttrain-mlogloss:1.0789\teval-mlogloss:1.11757\n",
            "[62]\ttrain-mlogloss:1.07709\teval-mlogloss:1.11626\n",
            "[63]\ttrain-mlogloss:1.0754\teval-mlogloss:1.11497\n",
            "[64]\ttrain-mlogloss:1.07388\teval-mlogloss:1.11385\n",
            "[65]\ttrain-mlogloss:1.07231\teval-mlogloss:1.11269\n",
            "[66]\ttrain-mlogloss:1.07088\teval-mlogloss:1.11171\n",
            "[67]\ttrain-mlogloss:1.06947\teval-mlogloss:1.11073\n",
            "[68]\ttrain-mlogloss:1.06806\teval-mlogloss:1.10987\n",
            "[69]\ttrain-mlogloss:1.06677\teval-mlogloss:1.10891\n",
            "[70]\ttrain-mlogloss:1.06545\teval-mlogloss:1.10805\n",
            "[71]\ttrain-mlogloss:1.06423\teval-mlogloss:1.10719\n",
            "[72]\ttrain-mlogloss:1.06291\teval-mlogloss:1.10641\n",
            "[73]\ttrain-mlogloss:1.0617\teval-mlogloss:1.10565\n",
            "[74]\ttrain-mlogloss:1.06053\teval-mlogloss:1.10482\n",
            "[75]\ttrain-mlogloss:1.05931\teval-mlogloss:1.10413\n",
            "[76]\ttrain-mlogloss:1.05826\teval-mlogloss:1.1035\n",
            "[77]\ttrain-mlogloss:1.05711\teval-mlogloss:1.10282\n",
            "[78]\ttrain-mlogloss:1.05611\teval-mlogloss:1.10224\n",
            "[79]\ttrain-mlogloss:1.05512\teval-mlogloss:1.10164\n",
            "[80]\ttrain-mlogloss:1.05413\teval-mlogloss:1.10104\n",
            "[81]\ttrain-mlogloss:1.05325\teval-mlogloss:1.10051\n",
            "[82]\ttrain-mlogloss:1.05227\teval-mlogloss:1.09994\n",
            "[83]\ttrain-mlogloss:1.05135\teval-mlogloss:1.09945\n",
            "[84]\ttrain-mlogloss:1.05038\teval-mlogloss:1.09901\n",
            "[85]\ttrain-mlogloss:1.04941\teval-mlogloss:1.09856\n",
            "[86]\ttrain-mlogloss:1.04852\teval-mlogloss:1.09813\n",
            "[87]\ttrain-mlogloss:1.04774\teval-mlogloss:1.09772\n",
            "[88]\ttrain-mlogloss:1.04686\teval-mlogloss:1.09737\n",
            "[89]\ttrain-mlogloss:1.04613\teval-mlogloss:1.09701\n",
            "[90]\ttrain-mlogloss:1.04543\teval-mlogloss:1.09664\n",
            "[91]\ttrain-mlogloss:1.04468\teval-mlogloss:1.09635\n",
            "[92]\ttrain-mlogloss:1.04378\teval-mlogloss:1.096\n",
            "[93]\ttrain-mlogloss:1.04306\teval-mlogloss:1.09568\n",
            "[94]\ttrain-mlogloss:1.0423\teval-mlogloss:1.09537\n",
            "[95]\ttrain-mlogloss:1.04158\teval-mlogloss:1.09506\n",
            "[96]\ttrain-mlogloss:1.04091\teval-mlogloss:1.09482\n",
            "[97]\ttrain-mlogloss:1.04016\teval-mlogloss:1.09451\n",
            "[98]\ttrain-mlogloss:1.03946\teval-mlogloss:1.09426\n",
            "[99]\ttrain-mlogloss:1.03861\teval-mlogloss:1.09398\n",
            "[100]\ttrain-mlogloss:1.03791\teval-mlogloss:1.09376\n",
            "[101]\ttrain-mlogloss:1.03733\teval-mlogloss:1.09353\n",
            "[102]\ttrain-mlogloss:1.03664\teval-mlogloss:1.09331\n",
            "[103]\ttrain-mlogloss:1.03598\teval-mlogloss:1.09307\n",
            "[104]\ttrain-mlogloss:1.03519\teval-mlogloss:1.09288\n",
            "[105]\ttrain-mlogloss:1.0346\teval-mlogloss:1.09264\n",
            "[106]\ttrain-mlogloss:1.03385\teval-mlogloss:1.0925\n",
            "[107]\ttrain-mlogloss:1.0332\teval-mlogloss:1.09233\n",
            "[108]\ttrain-mlogloss:1.03258\teval-mlogloss:1.09218\n",
            "[109]\ttrain-mlogloss:1.03199\teval-mlogloss:1.09198\n",
            "[110]\ttrain-mlogloss:1.0315\teval-mlogloss:1.09183\n",
            "[111]\ttrain-mlogloss:1.03089\teval-mlogloss:1.09168\n",
            "[112]\ttrain-mlogloss:1.0303\teval-mlogloss:1.09154\n",
            "[113]\ttrain-mlogloss:1.0297\teval-mlogloss:1.0914\n",
            "[114]\ttrain-mlogloss:1.02905\teval-mlogloss:1.09122\n",
            "[115]\ttrain-mlogloss:1.02837\teval-mlogloss:1.09108\n",
            "[116]\ttrain-mlogloss:1.02776\teval-mlogloss:1.0909\n",
            "[117]\ttrain-mlogloss:1.02723\teval-mlogloss:1.0908\n",
            "[118]\ttrain-mlogloss:1.02665\teval-mlogloss:1.09063\n",
            "[119]\ttrain-mlogloss:1.02611\teval-mlogloss:1.09052\n",
            "[120]\ttrain-mlogloss:1.02558\teval-mlogloss:1.09045\n",
            "[121]\ttrain-mlogloss:1.02493\teval-mlogloss:1.09028\n",
            "[122]\ttrain-mlogloss:1.02412\teval-mlogloss:1.09021\n",
            "[123]\ttrain-mlogloss:1.02349\teval-mlogloss:1.09009\n",
            "[124]\ttrain-mlogloss:1.02282\teval-mlogloss:1.08999\n",
            "[125]\ttrain-mlogloss:1.02221\teval-mlogloss:1.08989\n",
            "[126]\ttrain-mlogloss:1.02165\teval-mlogloss:1.08984\n",
            "[127]\ttrain-mlogloss:1.02094\teval-mlogloss:1.08978\n",
            "[128]\ttrain-mlogloss:1.0205\teval-mlogloss:1.0897\n",
            "[129]\ttrain-mlogloss:1.01978\teval-mlogloss:1.08959\n",
            "[130]\ttrain-mlogloss:1.01913\teval-mlogloss:1.08951\n",
            "[131]\ttrain-mlogloss:1.01849\teval-mlogloss:1.08939\n",
            "[132]\ttrain-mlogloss:1.01797\teval-mlogloss:1.08937\n",
            "[133]\ttrain-mlogloss:1.01719\teval-mlogloss:1.08925\n",
            "[134]\ttrain-mlogloss:1.01655\teval-mlogloss:1.08918\n",
            "[135]\ttrain-mlogloss:1.01599\teval-mlogloss:1.08911\n",
            "[136]\ttrain-mlogloss:1.0153\teval-mlogloss:1.08905\n",
            "[137]\ttrain-mlogloss:1.01457\teval-mlogloss:1.08894\n",
            "[138]\ttrain-mlogloss:1.01402\teval-mlogloss:1.08883\n",
            "[139]\ttrain-mlogloss:1.01329\teval-mlogloss:1.08871\n",
            "[140]\ttrain-mlogloss:1.01261\teval-mlogloss:1.08867\n",
            "[141]\ttrain-mlogloss:1.01208\teval-mlogloss:1.08862\n",
            "[142]\ttrain-mlogloss:1.0115\teval-mlogloss:1.08857\n",
            "[143]\ttrain-mlogloss:1.01088\teval-mlogloss:1.08846\n",
            "[144]\ttrain-mlogloss:1.01029\teval-mlogloss:1.08843\n",
            "[145]\ttrain-mlogloss:1.00952\teval-mlogloss:1.08834\n",
            "[146]\ttrain-mlogloss:1.00893\teval-mlogloss:1.08828\n",
            "[147]\ttrain-mlogloss:1.00836\teval-mlogloss:1.08826\n",
            "[148]\ttrain-mlogloss:1.00794\teval-mlogloss:1.08821\n",
            "[149]\ttrain-mlogloss:1.00741\teval-mlogloss:1.08817\n",
            "[150]\ttrain-mlogloss:1.00661\teval-mlogloss:1.08806\n",
            "[151]\ttrain-mlogloss:1.00595\teval-mlogloss:1.08802\n",
            "[152]\ttrain-mlogloss:1.00535\teval-mlogloss:1.08803\n",
            "[153]\ttrain-mlogloss:1.00478\teval-mlogloss:1.08798\n",
            "[154]\ttrain-mlogloss:1.00421\teval-mlogloss:1.08797\n",
            "[155]\ttrain-mlogloss:1.00376\teval-mlogloss:1.08794\n",
            "[156]\ttrain-mlogloss:1.00317\teval-mlogloss:1.08794\n",
            "[157]\ttrain-mlogloss:1.00263\teval-mlogloss:1.08792\n",
            "[158]\ttrain-mlogloss:1.00198\teval-mlogloss:1.08785\n",
            "[159]\ttrain-mlogloss:1.00134\teval-mlogloss:1.08783\n",
            "[160]\ttrain-mlogloss:1.0007\teval-mlogloss:1.08777\n",
            "[161]\ttrain-mlogloss:1.00005\teval-mlogloss:1.0877\n",
            "[162]\ttrain-mlogloss:0.999449\teval-mlogloss:1.08764\n",
            "[163]\ttrain-mlogloss:0.998865\teval-mlogloss:1.08762\n",
            "[164]\ttrain-mlogloss:0.998347\teval-mlogloss:1.08759\n",
            "[165]\ttrain-mlogloss:0.997677\teval-mlogloss:1.08746\n",
            "[166]\ttrain-mlogloss:0.997089\teval-mlogloss:1.08735\n",
            "[167]\ttrain-mlogloss:0.99645\teval-mlogloss:1.08734\n",
            "[168]\ttrain-mlogloss:0.995882\teval-mlogloss:1.08727\n",
            "[169]\ttrain-mlogloss:0.995226\teval-mlogloss:1.08722\n",
            "[170]\ttrain-mlogloss:0.994838\teval-mlogloss:1.08721\n",
            "[171]\ttrain-mlogloss:0.994259\teval-mlogloss:1.08716\n",
            "[172]\ttrain-mlogloss:0.993769\teval-mlogloss:1.08715\n",
            "[173]\ttrain-mlogloss:0.99322\teval-mlogloss:1.08711\n",
            "[174]\ttrain-mlogloss:0.992602\teval-mlogloss:1.08707\n",
            "[175]\ttrain-mlogloss:0.992103\teval-mlogloss:1.08707\n",
            "[176]\ttrain-mlogloss:0.991425\teval-mlogloss:1.08699\n",
            "[177]\ttrain-mlogloss:0.990733\teval-mlogloss:1.08701\n",
            "[178]\ttrain-mlogloss:0.990186\teval-mlogloss:1.08697\n",
            "[179]\ttrain-mlogloss:0.989489\teval-mlogloss:1.08693\n",
            "[180]\ttrain-mlogloss:0.988925\teval-mlogloss:1.08695\n",
            "[181]\ttrain-mlogloss:0.988333\teval-mlogloss:1.08693\n",
            "[182]\ttrain-mlogloss:0.987874\teval-mlogloss:1.08688\n",
            "[183]\ttrain-mlogloss:0.987383\teval-mlogloss:1.08685\n",
            "[184]\ttrain-mlogloss:0.986964\teval-mlogloss:1.0869\n",
            "[185]\ttrain-mlogloss:0.986323\teval-mlogloss:1.08686\n",
            "[186]\ttrain-mlogloss:0.985762\teval-mlogloss:1.08685\n",
            "[187]\ttrain-mlogloss:0.98529\teval-mlogloss:1.08682\n",
            "[188]\ttrain-mlogloss:0.984683\teval-mlogloss:1.08684\n",
            "[189]\ttrain-mlogloss:0.984116\teval-mlogloss:1.08686\n",
            "[190]\ttrain-mlogloss:0.983554\teval-mlogloss:1.08683\n",
            "[191]\ttrain-mlogloss:0.983034\teval-mlogloss:1.08681\n",
            "[192]\ttrain-mlogloss:0.982566\teval-mlogloss:1.08682\n",
            "[193]\ttrain-mlogloss:0.982116\teval-mlogloss:1.08682\n",
            "[194]\ttrain-mlogloss:0.981599\teval-mlogloss:1.08682\n",
            "[195]\ttrain-mlogloss:0.981079\teval-mlogloss:1.08678\n",
            "[196]\ttrain-mlogloss:0.980575\teval-mlogloss:1.0868\n",
            "[197]\ttrain-mlogloss:0.980127\teval-mlogloss:1.08677\n",
            "[198]\ttrain-mlogloss:0.979603\teval-mlogloss:1.08677\n",
            "[199]\ttrain-mlogloss:0.979131\teval-mlogloss:1.08679\n",
            "[200]\ttrain-mlogloss:0.97847\teval-mlogloss:1.08673\n",
            "[201]\ttrain-mlogloss:0.977985\teval-mlogloss:1.08676\n",
            "[202]\ttrain-mlogloss:0.977542\teval-mlogloss:1.08675\n",
            "[203]\ttrain-mlogloss:0.97693\teval-mlogloss:1.08672\n",
            "[204]\ttrain-mlogloss:0.976447\teval-mlogloss:1.08671\n",
            "[205]\ttrain-mlogloss:0.97596\teval-mlogloss:1.08671\n",
            "[206]\ttrain-mlogloss:0.975475\teval-mlogloss:1.08668\n",
            "[207]\ttrain-mlogloss:0.974989\teval-mlogloss:1.08666\n",
            "[208]\ttrain-mlogloss:0.974428\teval-mlogloss:1.0867\n",
            "[209]\ttrain-mlogloss:0.973848\teval-mlogloss:1.08675\n",
            "[210]\ttrain-mlogloss:0.973334\teval-mlogloss:1.08672\n",
            "[211]\ttrain-mlogloss:0.972784\teval-mlogloss:1.08677\n",
            "[212]\ttrain-mlogloss:0.972305\teval-mlogloss:1.0868\n",
            "[213]\ttrain-mlogloss:0.971889\teval-mlogloss:1.08679\n",
            "[214]\ttrain-mlogloss:0.971462\teval-mlogloss:1.08681\n",
            "[215]\ttrain-mlogloss:0.971002\teval-mlogloss:1.08682\n",
            "[216]\ttrain-mlogloss:0.970444\teval-mlogloss:1.08678\n",
            "[217]\ttrain-mlogloss:0.969902\teval-mlogloss:1.08676\n",
            "[218]\ttrain-mlogloss:0.969364\teval-mlogloss:1.08677\n",
            "[219]\ttrain-mlogloss:0.96885\teval-mlogloss:1.08674\n",
            "[220]\ttrain-mlogloss:0.968355\teval-mlogloss:1.08677\n",
            "[221]\ttrain-mlogloss:0.967739\teval-mlogloss:1.08676\n",
            "[222]\ttrain-mlogloss:0.967129\teval-mlogloss:1.08679\n",
            "[223]\ttrain-mlogloss:0.966644\teval-mlogloss:1.08682\n",
            "[224]\ttrain-mlogloss:0.966233\teval-mlogloss:1.08686\n",
            "[225]\ttrain-mlogloss:0.965702\teval-mlogloss:1.08683\n",
            "[226]\ttrain-mlogloss:0.965185\teval-mlogloss:1.0868\n",
            "[227]\ttrain-mlogloss:0.964619\teval-mlogloss:1.08683\n",
            "Stopping. Best iteration:\n",
            "[207]\ttrain-mlogloss:0.974989\teval-mlogloss:1.08666\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG4v1Wv9AdDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apk(actual, predicted, k=7, default=0.0):\n",
        "    if len(predicted) > k:\n",
        "        predicted = predicted[:k]\n",
        "    #MAP@7 이므로, 최대 7개만 사용\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i , p in enumerate(predicted):\n",
        "        #점수를 부여하는 조건은 다음과 같음 :\n",
        "        # 예측 값이 정답에 있고 ('p in actual')\n",
        "        # 예측 값이 중복이 아니면('p not in predicted[:i]')\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i+1.0)\n",
        "\n",
        "        #정답 값이 공백일 경우, 무조건 0.0을 반환\n",
        "    if not actual:\n",
        "      return default\n",
        "    # print(\"score / min(len(actual), k) :: \", score / min(len(actual), k))\n",
        "    #정답의 개수(len(actual))로 averate precision을 구한다\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "#list of list인 정답 값(actual)과 예측값(predicted)에서 고객별 Average Precision을 구하고, np.mean()을 통해 평균을 계산\n",
        "def mapk(actual, predicted, k=7, default=0.0):\n",
        "    # rst = [apk(a,p,k,default) for a,p in zip(actual, predicted)]\n",
        "    # print(\"rst ::: \", rst[:1000])\n",
        "    return np.mean([apk(a,p,k,default) for a,p in zip(actual, predicted)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd57-Yl_7Ae2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "5.   평가\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35tunzI47AA5",
        "colab_type": "code",
        "outputId": "44347cdb-3a72-4736-befa-410eba840b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "#MAP@7 평가 척도를 위한 준비작업.\n",
        "# 고객 식별 버놓를 추출한다.\n",
        "vld = trn[trn['fecha_dato'] == vld_date]\n",
        "ncodpers_vld = vld.as_matrix(columns=['ncodpers'])\n",
        "\n",
        "#print(\"ncodpers_vld : \", ncodpers_vld)\n",
        "#print(\"vld : \", vld)\n",
        "\n",
        "# 검증 데이터에서 신규 구매를 구한다\n",
        "for prod in prods:\n",
        "  prev = prod + '_prev'\n",
        "  padd = prod + '_add'\n",
        "\n",
        "  vld[prev] = vld[prev].astype(float).astype(np.int8)\n",
        "  vld[prod] = vld[prod].astype(float).astype(np.int8)\n",
        "\n",
        "  vld[padd] = vld[prod] - vld[prev]\n",
        "\n",
        "add_vld = vld.as_matrix(columns=[prod + '_add' for prod in prods])\n",
        "#ncodpers_vld 크기의 list를 생성\n",
        "add_vld_list = [list() for i in range(len(ncodpers_vld))]\n",
        "\n",
        "# 고객별 신규 구매 정답 값을 add_vld_list에 저장하고, 총 count를 count_vld에 저장한다.\n",
        "count_vld = 0\n",
        "for ncodper in range(len(ncodpers_vld)):\n",
        "    for prod in range(len(prods)):\n",
        "      if add_vld[ncodper, prod] > 0:\n",
        "        # print(\"ncodper : \", ncodper, \"  || prod : \", prod)\n",
        "        add_vld_list[ncodper].append(prod)\n",
        "        count_vld += 1\n",
        "        \n",
        "# print(\"count_vld : \", count_vld)\n",
        "# print(\"add_vld_list[0:10] : \", add_vld_list[0:1000])\n",
        "\n",
        "#import map7 as mapk\n",
        "# 고객 데이터에서 얻을 수 있는 MAP@7 최고점을 미리 구함.(0.042663)\n",
        "print(mapk(add_vld_list, add_vld_list, 7, 0.0))\n",
        "\n",
        "# 검증 데이터에 대한 예측값을 구한다\n",
        "X_vld = vld.as_matrix(columns=features)\n",
        "Y_vld = vld.as_matrix(columns=['y'])\n",
        "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
        "preds_vld = model.predict(dvld, ntree_limit=best_ntree_limit)\n",
        "\n",
        "# 저번 달에 보유한 제품은 신규 구매가 불가하기 때문에, 확률값에서 미리 1을 빼줌\n",
        "preds_vld = preds_vld - vld.as_matrix(columns=[prod + '_prev' for prod in prods])\n",
        "\n",
        "# 검증 데이터 예측 상위 7개를 추출\n",
        "result_vld = []\n",
        "for ncodper, pred in zip(ncodpers_vld, preds_vld):\n",
        "  y_prods = [(y,p,ip) for y, p, ip, in zip(pred, prods, range(len(prods)))]\n",
        "  y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
        "  result_vld.append([ip for y,p,ip in y_prods])\n",
        "\n",
        "# 검증 데이터에서의 MAP@7 점수를 구한다.\n",
        "print(mapk(add_vld_list, result_vld, 7, 0.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.04266379915553903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.03646478055889802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkKDPASSvHXw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "5.   테스트 데이터 예측 및 캐글 업데이트\n",
        "\n",
        "\n",
        "*   XGBoost의 get_fscore() 함수를 통해서 학습한 모델의 변수 중요도를 출력 가능\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkrtp-4XvNvp",
        "colab_type": "code",
        "outputId": "bd069a48-aca3-4f60-fbe6-c9ce45a7b7b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# XGBoost 모델을 전체 훈련 데이터로 재학습한다!\n",
        "X_all = XY.as_matrix(columns=features)\n",
        "Y_all = XY.as_matrix(columns=['y'])\n",
        "dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
        "watch_list =[(dall, 'train')]\n",
        "\n",
        "# 트리 개수를 늘어난 데이터 양만큼 비례해서 증가\n",
        "best_ntree_limit = int(round(int(best_ntree_limit * (len(XY_trn) + len(XY_vld)))/(len(XY_trn))))\n",
        "# print(type(best_ntree_limit.astype(float8).astype(int8)))\n",
        "#XGBoost 모델 재 학습\n",
        "model = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list)\n",
        "\n",
        "#변수 중요도를 출력해본다. 예상하던 변수가 상위로 올라와 있는가?\n",
        "print(\"Feature importance:\")\n",
        "for kv in sorted([(k,v) for k, v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
        "  print(kv)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\ttrain-mlogloss:2.67555\n",
            "[1]\ttrain-mlogloss:2.43966\n",
            "[2]\ttrain-mlogloss:2.26045\n",
            "[3]\ttrain-mlogloss:2.12681\n",
            "[4]\ttrain-mlogloss:2.01412\n",
            "[5]\ttrain-mlogloss:1.91965\n",
            "[6]\ttrain-mlogloss:1.84318\n",
            "[7]\ttrain-mlogloss:1.77418\n",
            "[8]\ttrain-mlogloss:1.71523\n",
            "[9]\ttrain-mlogloss:1.66179\n",
            "[10]\ttrain-mlogloss:1.61435\n",
            "[11]\ttrain-mlogloss:1.57202\n",
            "[12]\ttrain-mlogloss:1.53506\n",
            "[13]\ttrain-mlogloss:1.50008\n",
            "[14]\ttrain-mlogloss:1.46791\n",
            "[15]\ttrain-mlogloss:1.43891\n",
            "[16]\ttrain-mlogloss:1.4129\n",
            "[17]\ttrain-mlogloss:1.38938\n",
            "[18]\ttrain-mlogloss:1.36713\n",
            "[19]\ttrain-mlogloss:1.34675\n",
            "[20]\ttrain-mlogloss:1.32772\n",
            "[21]\ttrain-mlogloss:1.3107\n",
            "[22]\ttrain-mlogloss:1.2952\n",
            "[23]\ttrain-mlogloss:1.28089\n",
            "[24]\ttrain-mlogloss:1.26778\n",
            "[25]\ttrain-mlogloss:1.25541\n",
            "[26]\ttrain-mlogloss:1.24375\n",
            "[27]\ttrain-mlogloss:1.23268\n",
            "[28]\ttrain-mlogloss:1.22236\n",
            "[29]\ttrain-mlogloss:1.21331\n",
            "[30]\ttrain-mlogloss:1.2045\n",
            "[31]\ttrain-mlogloss:1.1961\n",
            "[32]\ttrain-mlogloss:1.18831\n",
            "[33]\ttrain-mlogloss:1.18101\n",
            "[34]\ttrain-mlogloss:1.17414\n",
            "[35]\ttrain-mlogloss:1.16768\n",
            "[36]\ttrain-mlogloss:1.16175\n",
            "[37]\ttrain-mlogloss:1.15599\n",
            "[38]\ttrain-mlogloss:1.15058\n",
            "[39]\ttrain-mlogloss:1.14546\n",
            "[40]\ttrain-mlogloss:1.14061\n",
            "[41]\ttrain-mlogloss:1.13595\n",
            "[42]\ttrain-mlogloss:1.13175\n",
            "[43]\ttrain-mlogloss:1.12764\n",
            "[44]\ttrain-mlogloss:1.12362\n",
            "[45]\ttrain-mlogloss:1.12004\n",
            "[46]\ttrain-mlogloss:1.11638\n",
            "[47]\ttrain-mlogloss:1.11334\n",
            "[48]\ttrain-mlogloss:1.11022\n",
            "[49]\ttrain-mlogloss:1.10739\n",
            "[50]\ttrain-mlogloss:1.10452\n",
            "[51]\ttrain-mlogloss:1.10177\n",
            "[52]\ttrain-mlogloss:1.09932\n",
            "[53]\ttrain-mlogloss:1.09677\n",
            "[54]\ttrain-mlogloss:1.09434\n",
            "[55]\ttrain-mlogloss:1.09199\n",
            "[56]\ttrain-mlogloss:1.08988\n",
            "[57]\ttrain-mlogloss:1.08779\n",
            "[58]\ttrain-mlogloss:1.08583\n",
            "[59]\ttrain-mlogloss:1.08388\n",
            "[60]\ttrain-mlogloss:1.08204\n",
            "[61]\ttrain-mlogloss:1.08025\n",
            "[62]\ttrain-mlogloss:1.07856\n",
            "[63]\ttrain-mlogloss:1.07701\n",
            "[64]\ttrain-mlogloss:1.07551\n",
            "[65]\ttrain-mlogloss:1.07389\n",
            "[66]\ttrain-mlogloss:1.07245\n",
            "[67]\ttrain-mlogloss:1.07106\n",
            "[68]\ttrain-mlogloss:1.06976\n",
            "[69]\ttrain-mlogloss:1.06844\n",
            "[70]\ttrain-mlogloss:1.0672\n",
            "[71]\ttrain-mlogloss:1.06591\n",
            "[72]\ttrain-mlogloss:1.06467\n",
            "[73]\ttrain-mlogloss:1.06357\n",
            "[74]\ttrain-mlogloss:1.06242\n",
            "[75]\ttrain-mlogloss:1.06124\n",
            "[76]\ttrain-mlogloss:1.06019\n",
            "[77]\ttrain-mlogloss:1.05908\n",
            "[78]\ttrain-mlogloss:1.05813\n",
            "[79]\ttrain-mlogloss:1.05723\n",
            "[80]\ttrain-mlogloss:1.05624\n",
            "[81]\ttrain-mlogloss:1.05524\n",
            "[82]\ttrain-mlogloss:1.05433\n",
            "[83]\ttrain-mlogloss:1.05346\n",
            "[84]\ttrain-mlogloss:1.0526\n",
            "[85]\ttrain-mlogloss:1.05175\n",
            "[86]\ttrain-mlogloss:1.05086\n",
            "[87]\ttrain-mlogloss:1.05008\n",
            "[88]\ttrain-mlogloss:1.0494\n",
            "[89]\ttrain-mlogloss:1.04863\n",
            "[90]\ttrain-mlogloss:1.04794\n",
            "[91]\ttrain-mlogloss:1.04711\n",
            "[92]\ttrain-mlogloss:1.0464\n",
            "[93]\ttrain-mlogloss:1.0458\n",
            "[94]\ttrain-mlogloss:1.04501\n",
            "[95]\ttrain-mlogloss:1.04434\n",
            "[96]\ttrain-mlogloss:1.04377\n",
            "[97]\ttrain-mlogloss:1.04313\n",
            "[98]\ttrain-mlogloss:1.04252\n",
            "[99]\ttrain-mlogloss:1.04189\n",
            "[100]\ttrain-mlogloss:1.04132\n",
            "[101]\ttrain-mlogloss:1.04076\n",
            "[102]\ttrain-mlogloss:1.04013\n",
            "[103]\ttrain-mlogloss:1.0396\n",
            "[104]\ttrain-mlogloss:1.0388\n",
            "[105]\ttrain-mlogloss:1.03815\n",
            "[106]\ttrain-mlogloss:1.0375\n",
            "[107]\ttrain-mlogloss:1.03688\n",
            "[108]\ttrain-mlogloss:1.03638\n",
            "[109]\ttrain-mlogloss:1.03589\n",
            "[110]\ttrain-mlogloss:1.03538\n",
            "[111]\ttrain-mlogloss:1.0349\n",
            "[112]\ttrain-mlogloss:1.03434\n",
            "[113]\ttrain-mlogloss:1.03387\n",
            "[114]\ttrain-mlogloss:1.03342\n",
            "[115]\ttrain-mlogloss:1.03297\n",
            "[116]\ttrain-mlogloss:1.03234\n",
            "[117]\ttrain-mlogloss:1.03175\n",
            "[118]\ttrain-mlogloss:1.03119\n",
            "[119]\ttrain-mlogloss:1.03069\n",
            "[120]\ttrain-mlogloss:1.03011\n",
            "[121]\ttrain-mlogloss:1.02948\n",
            "[122]\ttrain-mlogloss:1.02908\n",
            "[123]\ttrain-mlogloss:1.02864\n",
            "[124]\ttrain-mlogloss:1.02812\n",
            "[125]\ttrain-mlogloss:1.02759\n",
            "[126]\ttrain-mlogloss:1.02722\n",
            "[127]\ttrain-mlogloss:1.02663\n",
            "[128]\ttrain-mlogloss:1.02617\n",
            "[129]\ttrain-mlogloss:1.02572\n",
            "[130]\ttrain-mlogloss:1.02513\n",
            "[131]\ttrain-mlogloss:1.02448\n",
            "[132]\ttrain-mlogloss:1.02388\n",
            "[133]\ttrain-mlogloss:1.02336\n",
            "[134]\ttrain-mlogloss:1.02282\n",
            "[135]\ttrain-mlogloss:1.02213\n",
            "[136]\ttrain-mlogloss:1.02161\n",
            "[137]\ttrain-mlogloss:1.02111\n",
            "[138]\ttrain-mlogloss:1.02061\n",
            "[139]\ttrain-mlogloss:1.02004\n",
            "[140]\ttrain-mlogloss:1.01944\n",
            "[141]\ttrain-mlogloss:1.01898\n",
            "[142]\ttrain-mlogloss:1.01842\n",
            "[143]\ttrain-mlogloss:1.01797\n",
            "[144]\ttrain-mlogloss:1.01748\n",
            "[145]\ttrain-mlogloss:1.01697\n",
            "[146]\ttrain-mlogloss:1.01643\n",
            "[147]\ttrain-mlogloss:1.01595\n",
            "[148]\ttrain-mlogloss:1.01549\n",
            "[149]\ttrain-mlogloss:1.0149\n",
            "[150]\ttrain-mlogloss:1.0145\n",
            "[151]\ttrain-mlogloss:1.0138\n",
            "[152]\ttrain-mlogloss:1.01328\n",
            "[153]\ttrain-mlogloss:1.0128\n",
            "[154]\ttrain-mlogloss:1.01237\n",
            "[155]\ttrain-mlogloss:1.01182\n",
            "[156]\ttrain-mlogloss:1.01134\n",
            "[157]\ttrain-mlogloss:1.01076\n",
            "[158]\ttrain-mlogloss:1.01027\n",
            "[159]\ttrain-mlogloss:1.00979\n",
            "[160]\ttrain-mlogloss:1.00927\n",
            "[161]\ttrain-mlogloss:1.00882\n",
            "[162]\ttrain-mlogloss:1.00827\n",
            "[163]\ttrain-mlogloss:1.00782\n",
            "[164]\ttrain-mlogloss:1.00734\n",
            "[165]\ttrain-mlogloss:1.00684\n",
            "[166]\ttrain-mlogloss:1.00623\n",
            "[167]\ttrain-mlogloss:1.00577\n",
            "[168]\ttrain-mlogloss:1.00526\n",
            "[169]\ttrain-mlogloss:1.00483\n",
            "[170]\ttrain-mlogloss:1.00435\n",
            "[171]\ttrain-mlogloss:1.00374\n",
            "[172]\ttrain-mlogloss:1.00322\n",
            "[173]\ttrain-mlogloss:1.00279\n",
            "[174]\ttrain-mlogloss:1.0022\n",
            "[175]\ttrain-mlogloss:1.00162\n",
            "[176]\ttrain-mlogloss:1.0011\n",
            "[177]\ttrain-mlogloss:1.00063\n",
            "[178]\ttrain-mlogloss:1.00008\n",
            "[179]\ttrain-mlogloss:0.999532\n",
            "[180]\ttrain-mlogloss:0.999093\n",
            "[181]\ttrain-mlogloss:0.998435\n",
            "[182]\ttrain-mlogloss:0.998065\n",
            "[183]\ttrain-mlogloss:0.997553\n",
            "[184]\ttrain-mlogloss:0.997049\n",
            "[185]\ttrain-mlogloss:0.996605\n",
            "[186]\ttrain-mlogloss:0.996154\n",
            "[187]\ttrain-mlogloss:0.995594\n",
            "[188]\ttrain-mlogloss:0.995097\n",
            "[189]\ttrain-mlogloss:0.994512\n",
            "[190]\ttrain-mlogloss:0.993969\n",
            "[191]\ttrain-mlogloss:0.993629\n",
            "[192]\ttrain-mlogloss:0.993092\n",
            "[193]\ttrain-mlogloss:0.992707\n",
            "[194]\ttrain-mlogloss:0.992272\n",
            "[195]\ttrain-mlogloss:0.991841\n",
            "[196]\ttrain-mlogloss:0.991334\n",
            "[197]\ttrain-mlogloss:0.990841\n",
            "[198]\ttrain-mlogloss:0.990337\n",
            "[199]\ttrain-mlogloss:0.989975\n",
            "[200]\ttrain-mlogloss:0.989351\n",
            "[201]\ttrain-mlogloss:0.988954\n",
            "[202]\ttrain-mlogloss:0.988518\n",
            "[203]\ttrain-mlogloss:0.988023\n",
            "[204]\ttrain-mlogloss:0.987521\n",
            "[205]\ttrain-mlogloss:0.987119\n",
            "[206]\ttrain-mlogloss:0.98656\n",
            "[207]\ttrain-mlogloss:0.986059\n",
            "[208]\ttrain-mlogloss:0.985741\n",
            "[209]\ttrain-mlogloss:0.985299\n",
            "[210]\ttrain-mlogloss:0.984863\n",
            "[211]\ttrain-mlogloss:0.984326\n",
            "[212]\ttrain-mlogloss:0.983747\n",
            "[213]\ttrain-mlogloss:0.983324\n",
            "[214]\ttrain-mlogloss:0.982819\n",
            "[215]\ttrain-mlogloss:0.982375\n",
            "[216]\ttrain-mlogloss:0.98194\n",
            "[217]\ttrain-mlogloss:0.981528\n",
            "[218]\ttrain-mlogloss:0.981144\n",
            "[219]\ttrain-mlogloss:0.980699\n",
            "[220]\ttrain-mlogloss:0.980183\n",
            "[221]\ttrain-mlogloss:0.979741\n",
            "[222]\ttrain-mlogloss:0.979264\n",
            "[223]\ttrain-mlogloss:0.978812\n",
            "[224]\ttrain-mlogloss:0.978434\n",
            "[225]\ttrain-mlogloss:0.978003\n",
            "[226]\ttrain-mlogloss:0.977574\n",
            "[227]\ttrain-mlogloss:0.977116\n",
            "[228]\ttrain-mlogloss:0.976697\n",
            "[229]\ttrain-mlogloss:0.976185\n",
            "[230]\ttrain-mlogloss:0.975836\n",
            "[231]\ttrain-mlogloss:0.975403\n",
            "[232]\ttrain-mlogloss:0.974976\n",
            "[233]\ttrain-mlogloss:0.974475\n",
            "[234]\ttrain-mlogloss:0.97405\n",
            "[235]\ttrain-mlogloss:0.973635\n",
            "[236]\ttrain-mlogloss:0.973061\n",
            "[237]\ttrain-mlogloss:0.972595\n",
            "[238]\ttrain-mlogloss:0.972142\n",
            "[239]\ttrain-mlogloss:0.971716\n",
            "[240]\ttrain-mlogloss:0.971272\n",
            "[241]\ttrain-mlogloss:0.970902\n",
            "[242]\ttrain-mlogloss:0.970478\n",
            "[243]\ttrain-mlogloss:0.970133\n",
            "[244]\ttrain-mlogloss:0.969644\n",
            "[245]\ttrain-mlogloss:0.969249\n",
            "[246]\ttrain-mlogloss:0.968904\n",
            "[247]\ttrain-mlogloss:0.968518\n",
            "[248]\ttrain-mlogloss:0.968046\n",
            "[249]\ttrain-mlogloss:0.967735\n",
            "[250]\ttrain-mlogloss:0.967276\n",
            "[251]\ttrain-mlogloss:0.966925\n",
            "[252]\ttrain-mlogloss:0.966459\n",
            "[253]\ttrain-mlogloss:0.966022\n",
            "[254]\ttrain-mlogloss:0.965712\n",
            "[255]\ttrain-mlogloss:0.965256\n",
            "[256]\ttrain-mlogloss:0.964742\n",
            "Feature importance:\n",
            "('renta', 21981)\n",
            "('age', 20449)\n",
            "('antiguedad', 19120)\n",
            "('age_prev', 13768)\n",
            "('antiguedad_prev', 13370)\n",
            "('fecha_alta_month', 12627)\n",
            "('nomprov', 11919)\n",
            "('fecha_alta_year', 10034)\n",
            "('renta_prev', 8963)\n",
            "('canal_entrada', 8060)\n",
            "('nomprov_prev', 6526)\n",
            "('canal_entrada_prev', 4912)\n",
            "('fecha_alta_month_prev', 4498)\n",
            "('sexo', 3497)\n",
            "('ind_recibo_ult1_prev', 3434)\n",
            "('fecha_alta_year_prev', 3331)\n",
            "('ind_ecue_fin_ult1_prev', 3186)\n",
            "('ind_cco_fin_ult1_prev', 3080)\n",
            "('ind_cno_fin_ult1_prev', 2926)\n",
            "('segmento', 2353)\n",
            "('ind_tjcr_fin_ult1_prev', 2164)\n",
            "('ind_reca_fin_ult1_prev', 2145)\n",
            "('segmento_prev', 2038)\n",
            "('ind_nom_pens_ult1_prev', 1651)\n",
            "('ind_valo_fin_ult1_prev', 1634)\n",
            "('tiprel_1mes', 1607)\n",
            "('ind_ctop_fin_ult1_prev', 1552)\n",
            "('ind_nomina_ult1_prev', 1542)\n",
            "('ind_dela_fin_ult1_prev', 1532)\n",
            "('ind_actividad_cliente', 1327)\n",
            "('sexo_prev', 1293)\n",
            "('ind_ctpp_fin_ult1_prev', 1282)\n",
            "('tiprel_1mes_prev', 1179)\n",
            "('ind_fond_fin_ult1_prev', 1023)\n",
            "('ind_ctma_fin_ult1_prev', 930)\n",
            "('ind_actividad_cliente_prev', 894)\n",
            "('indext', 796)\n",
            "('ind_nuevo', 756)\n",
            "('ind_plan_fin_ult1_prev', 667)\n",
            "('ind_hip_fin_ult1_prev', 548)\n",
            "('indrel_1mes', 432)\n",
            "('ind_nuevo_prev', 403)\n",
            "('indext_prev', 401)\n",
            "('ind_deco_fin_ult1_prev', 367)\n",
            "('indrel_1mes_prev', 209)\n",
            "('pais_residencia', 203)\n",
            "('ind_viv_fin_ult1_prev', 192)\n",
            "('ind_empleado', 177)\n",
            "('indrel', 170)\n",
            "('ind_empleado_prev', 157)\n",
            "('pais_residencia_prev', 146)\n",
            "('ind_deme_fin_ult1_prev', 114)\n",
            "('ind_ctju_fin_ult1_prev', 109)\n",
            "('ind_pres_fin_ult1_prev', 90)\n",
            "('ult_fec_cli_1t_month', 75)\n",
            "('ind_cder_fin_ult1_prev', 41)\n",
            "('indfall', 37)\n",
            "('indfall_prev', 24)\n",
            "('conyuemp_prev', 23)\n",
            "('indresi_prev', 8)\n",
            "('indresi', 8)\n",
            "('conyuemp', 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHBTLj9LcNyG",
        "colab_type": "code",
        "outputId": "e1f6bff4-d1d1-4c81-b16f-6f131e234280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# 캐글 제출을 위하여 테스트 데이터에 대한 예측값을 구함\n",
        "X_tst = tst.as_matrix(columns=features)\n",
        "dtst = xgb.DMatrix(X_tst, feature_names=features)\n",
        "preds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)\n",
        "ncodpers_tst = tst.as_matrix(columns=['ncodpers'])\n",
        "pred_tst = preds_tst - tst.as_matrix(columns=[prod + '_prev' for prod in prods])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JugKcQJ7lgqJ",
        "colab_type": "code",
        "outputId": "8aa78a6e-8bcc-4a05-aa5b-a81e768b32e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "#제출 파일을 생성\n",
        "submit_file = open(\"/content/drive/My Drive/CoLab/explorer_of_machine_learning/ch02_santander/modle/xgb.baseline.2020-02-05\", 'w')\n",
        "submit_file.write('ncodpers, added_products\\n')\n",
        "for ncodper, pred in zip(ncodpers_tst, preds_tst):\n",
        "  y_prods = [(y, p, ip) for y, p, ip in zip(pred, prods, range(len(prods)))]\n",
        "  y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
        "  y_prods = [p for y,p,ip in y_prods]\n",
        "  submit_file.write( '{},{}\\n'.format(int(ncodper), ' '.join(y_prods)) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_prods : [(8.973117e-06, 'ind_ahor_fin_ult1', 0), (8.973117e-06, 'ind_aval_fin_ult1', 1), (9.3397306e-05, 'ind_cco_fin_ult1', 2), (0.0035226487, 'ind_cder_fin_ult1', 3), (0.012752264, 'ind_cno_fin_ult1', 4), (1.5288391e-05, 'ind_ctju_fin_ult1', 5), (0.000602728, 'ind_ctma_fin_ult1', 6), (0.0031030187, 'ind_ctop_fin_ult1', 7), (0.0006515521, 'ind_ctpp_fin_ult1', 8), (8.973117e-06, 'ind_deco_fin_ult1', 9), (1.6252887e-06, 'ind_deme_fin_ult1', 10), (0.004638684, 'ind_dela_fin_ult1', 11), (0.14719014, 'ind_ecue_fin_ult1', 12), (0.009785439, 'ind_fond_fin_ult1', 13), (0.0001880369, 'ind_hip_fin_ult1', 14), (0.0009610458, 'ind_plan_fin_ult1', 15), (0.00070706324, 'ind_pres_fin_ult1', 16), (0.015471831, 'ind_reca_fin_ult1', 17), (0.00038648915, 'ind_tjcr_fin_ult1', 18), (0.0003936922, 'ind_valo_fin_ult1', 19), (0.00020060931, 'ind_viv_fin_ult1', 20), (0.024650436, 'ind_nomina_ult1', 21), (0.02886898, 'ind_nom_pens_ult1', 22), (0.74578816, 'ind_recibo_ult1', 23)]\n",
            "ncodper :  [15889]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-fb0c459ed074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ncodper : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncodper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0my_prods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0msubmit_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'{},{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncodper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, tuple found"
          ]
        }
      ]
    }
  ]
}